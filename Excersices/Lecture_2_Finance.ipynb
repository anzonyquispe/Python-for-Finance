{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed1fd2d",
   "metadata": {},
   "source": [
    "# 3. <a id='intro'>Pandas</a>\n",
    "\n",
    "This practice notebook  is **guided by the original Lecture 2** structure. All exercises use **real financial / economic data** from Peru and the US.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029387d",
   "metadata": {},
   "source": [
    "## 3.1. <a id='def'>Definition</a>\n",
    "\n",
    "Pandas is a Python library for working with tabular data (Series and DataFrames), including importing, cleaning, reshaping, and merging datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "id": "72df12c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use postponed evaluation of type annotations (helps with `str | None` on older Python versions).\n",
    "from __future__ import annotations\n",
    "\n",
    "# Path: cross-platform file/folder paths.\n",
    "from pathlib import Path\n",
    "# hashlib: create stable hashes for cache filenames.\n",
    "import hashlib\n",
    "# re: regular expressions for validating/parsing date strings.\n",
    "import re\n",
    "# warnings: control warning messages.\n",
    "import warnings\n",
    "\n",
    "# numpy: numeric operations + NaN handling.\n",
    "import numpy as np\n",
    "# pandas: tables (Series/DataFrame) + parsing dates + IO (parquet).\n",
    "import pandas as pd\n",
    "\n",
    "# Hide warnings in notebook output (keeps cells clean; you can remove this while debugging).\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define a local folder to store cached downloads.\n",
    "CACHE_DIR = Path(\".cache\")\n",
    "# Create the cache folder if it doesn't exist.\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Map Spanish 3-letter month abbreviations to English ones (needed for parsing BCRP date labels).\n",
    "_ES_TO_EN_MONTH = {\n",
    "    \"Ene\": \"Jan\", \"Feb\": \"Feb\", \"Mar\": \"Mar\", \"Abr\": \"Apr\", \"May\": \"May\", \"Jun\": \"Jun\",\n",
    "    \"Jul\": \"Jul\", \"Ago\": \"Aug\", \"Set\": \"Sep\", \"Sep\": \"Sep\", \"Oct\": \"Oct\", \"Nov\": \"Nov\", \"Dic\": \"Dec\"\n",
    "}\n",
    "\n",
    "def _hash_key(*parts: str) -> str:\n",
    "    # Create a SHA-256 hash object.\n",
    "    h = hashlib.sha256()\n",
    "    # Update the hash with each part (as UTF-8 bytes), plus a separator.\n",
    "    for p in parts:\n",
    "        h.update(str(p).encode(\"utf-8\"))\n",
    "        h.update(b\"|\")\n",
    "    # Return a short hash prefix to use in filenames (still very unlikely to collide).\n",
    "    return h.hexdigest()[:24]\n",
    "\n",
    "def _normalize_period(code: str, period: str | None) -> str | None:\n",
    "    # If no period provided, return None.\n",
    "    if period is None:\n",
    "        return None\n",
    "    # Convert to string and trim spaces.\n",
    "    period = str(period).strip()\n",
    "    # Use the last 2 characters of the BCRP code to infer frequency (PD daily, PM monthly, PA annual).\n",
    "    freq = code[-2:].upper() if len(code) >= 2 else \"\"\n",
    "\n",
    "    if freq == \"PD\":  # daily frequency\n",
    "        # If user passes \"YYYY-M\" or \"YYYY-MM\", convert to \"YYYY-MM-01\" (first day of month).\n",
    "        if re.fullmatch(r\"\\d{4}-\\d{1,2}\", period):\n",
    "            y, m = period.split(\"-\")\n",
    "            return f\"{int(y):04d}-{int(m):02d}-01\"\n",
    "        # If user passes just \"YYYY\", convert to \"YYYY-01-01\".\n",
    "        if re.fullmatch(r\"\\d{4}\", period):\n",
    "            return f\"{int(period):04d}-01-01\"\n",
    "        # Otherwise keep the period as-is (e.g., already \"YYYY-MM-DD\").\n",
    "        return period\n",
    "\n",
    "    if freq == \"PM\":  # monthly frequency\n",
    "        # If user passes \"YYYY-MM-DD\", convert to \"YYYY-M\" (month index).\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{1,2})-(\\d{1,2})\", period)\n",
    "        if m:\n",
    "            y, mo, _ = m.groups()\n",
    "            return f\"{int(y):04d}-{int(mo)}\"\n",
    "        # If user passes \"YYYY-MM\", convert to \"YYYY-M\".\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{1,2})\", period)\n",
    "        if m:\n",
    "            y, mo = m.groups()\n",
    "            return f\"{int(y):04d}-{int(mo)}\"\n",
    "        # If user passes \"YYYY\", default to \"YYYY-1\" (January).\n",
    "        if re.fullmatch(r\"\\d{4}\", period):\n",
    "            return f\"{int(period):04d}-1\"\n",
    "        # Otherwise keep the period as-is.\n",
    "        return period\n",
    "\n",
    "    if freq == \"PA\":  # annual frequency\n",
    "        # Extract the year \"YYYY\" if present at the start.\n",
    "        m = re.match(r\"(\\d{4})\", period)\n",
    "        return m.group(1) if m else period\n",
    "\n",
    "    # If frequency is unknown, return the original period string.\n",
    "    return period\n",
    "\n",
    "def _parse_bcrp_period_name(name: str) -> pd.Timestamp:\n",
    "    # Convert to string and trim.\n",
    "    s = str(name).strip()\n",
    "\n",
    "    # --- Case 1: ISO-like strings: \"YYYY\", \"YYYY-MM\", \"YYYY-MM-DD\" ---\n",
    "    try:\n",
    "        # Validate ISO-like patterns with regex.\n",
    "        if re.fullmatch(r\"\\d{4}(-\\d{1,2}){0,2}\", s):\n",
    "            # Convert to datetime; raise on failure.\n",
    "            return pd.to_datetime(s, errors=\"raise\")\n",
    "    except Exception:\n",
    "        # If it fails, continue to other formats.\n",
    "        pass\n",
    "\n",
    "    # --- Case 2: Monthly label like \"Mar.2020\" (often used by BCRP monthly series) ---\n",
    "    m = re.fullmatch(r\"([A-Za-zÁÉÍÓÚÑñ]{3})\\.(\\d{4})\", s)\n",
    "    if m:\n",
    "        # Extract Spanish month abbreviation and year.\n",
    "        mon_es, y = m.groups()\n",
    "        # Convert Spanish month to English month abbreviation if possible.\n",
    "        mon = _ES_TO_EN_MONTH.get(mon_es[:3], mon_es[:3])\n",
    "        # Parse using the specified format \"%b.%Y\".\n",
    "        return pd.to_datetime(f\"{mon}.{y}\", format=\"%b.%Y\", errors=\"coerce\")\n",
    "\n",
    "    # --- Case 3: Daily label like \"18Nov25\" or \"02Ene97\" (DDMonYY) ---\n",
    "    m = re.fullmatch(r\"(\\d{2})([A-Za-zÁÉÍÓÚÑñ]{3})(\\d{2})\", s)\n",
    "    if m:\n",
    "        # Extract day, Spanish month abbreviation, 2-digit year.\n",
    "        d, mon_es, yy = m.groups()\n",
    "        # Convert Spanish month to English month abbreviation if possible.\n",
    "        mon = _ES_TO_EN_MONTH.get(mon_es[:3], mon_es[:3])\n",
    "        # Convert 2-digit year to 4-digit year (00–69 => 2000–2069, else 1900–1999).\n",
    "        year = 2000 + int(yy) if int(yy) <= 69 else 1900 + int(yy)\n",
    "        # Parse using \"%d%b%Y\" (e.g., \"18Nov2025\").\n",
    "        return pd.to_datetime(f\"{d}{mon}{year}\", format=\"%d%b%Y\", errors=\"coerce\")\n",
    "\n",
    "    # --- Fallback: let pandas try its best; invalid parses become NaT ---\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def bcrp_get(series_codes, start: str | None = None, end: str | None = None, lang: str = \"esp\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch BCRPData series (JSON API) into a DataFrame.\n",
    "\n",
    "    Returns columns: [\"date\", <code1>, <code2>, ...]\n",
    "    \"\"\"\n",
    "    # Try importing requests (needed for HTTP calls). If missing, return empty DataFrame.\n",
    "    try:\n",
    "        import requests\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Accept one code or multiple codes.\n",
    "    if isinstance(series_codes, (list, tuple)):\n",
    "        # Clean each code string.\n",
    "        codes_list = [str(c).strip() for c in series_codes]\n",
    "        # BCRP API allows multiple codes joined by '-'.\n",
    "        codes = \"-\".join(codes_list)\n",
    "        # Use the first code to infer frequency for date normalization.\n",
    "        freq_code = codes_list[0]\n",
    "    else:\n",
    "        # Single code (string).\n",
    "        codes = str(series_codes).strip()\n",
    "        # Split anyway so we keep a list for consistent column naming.\n",
    "        codes_list = codes.split(\"-\")\n",
    "        # Use the first code to infer frequency.\n",
    "        freq_code = codes_list[0]\n",
    "\n",
    "    # Normalize start/end based on frequency (daily/monthly/annual).\n",
    "    start_n = _normalize_period(freq_code, start)\n",
    "    end_n = _normalize_period(freq_code, end)\n",
    "\n",
    "    # Build a deterministic cache key and cache filename.\n",
    "    key = _hash_key(\"bcrp\", codes, start_n or \"\", end_n or \"\", lang)\n",
    "    cache_path = CACHE_DIR / f\"bcrp_{key}.parquet\"\n",
    "    # If cached file exists, load it and return immediately.\n",
    "    if cache_path.exists():\n",
    "        return pd.read_parquet(cache_path)\n",
    "\n",
    "    # Base endpoint for the BCRP series API.\n",
    "    base_url = \"https://estadisticas.bcrp.gob.pe/estadisticas/series/api\"\n",
    "    # Start building URL parts.\n",
    "    parts = [base_url, codes, \"json\"]\n",
    "    # Add start/end only if both are provided.\n",
    "    if start_n and end_n:\n",
    "        parts += [start_n, end_n]\n",
    "    # Add language parameter (e.g., \"esp\").\n",
    "    if lang:\n",
    "        parts += [lang]\n",
    "    # Join into final URL string.\n",
    "    url = \"/\".join(parts)\n",
    "\n",
    "    # Make the HTTP request (30s timeout).\n",
    "    r = requests.get(url, timeout=30)\n",
    "    # Raise an exception if HTTP status is not 200.\n",
    "    r.raise_for_status()\n",
    "    # Parse JSON response body.\n",
    "    obj = r.json()\n",
    "\n",
    "    # Get the list of periods (each period has a label and values).\n",
    "    periods = obj.get(\"periods\", [])\n",
    "    rows = []\n",
    "    # Convert the JSON structure into rows for a DataFrame.\n",
    "    for p in periods:\n",
    "        # Period label (date-like string).\n",
    "        name = p.get(\"name\")\n",
    "        # Values are ordered to match the requested codes.\n",
    "        vals = p.get(\"values\", [])\n",
    "        # If API returns a single string, wrap it into a list for consistency.\n",
    "        if isinstance(vals, str):\n",
    "            vals = [vals]\n",
    "        # Skip malformed entries.\n",
    "        if name is None or not isinstance(vals, list):\n",
    "            continue\n",
    "        # Pad/truncate values to match number of codes.\n",
    "        vals = (vals + [None] * len(codes_list))[:len(codes_list)]\n",
    "        # Append row: [date_label, value1, value2, ...]\n",
    "        rows.append([name] + vals)\n",
    "\n",
    "    # Create a DataFrame with \"date\" + one column per code.\n",
    "    df = pd.DataFrame(rows, columns=[\"date\"] + codes_list)\n",
    "    # If no rows, return an empty DataFrame with the right columns.\n",
    "    if df.shape[0] == 0:\n",
    "        return pd.DataFrame(columns=[\"date\"] + codes_list)\n",
    "\n",
    "    # Parse the \"date\" strings into actual timestamps.\n",
    "    df[\"date\"] = df[\"date\"].apply(_parse_bcrp_period_name)\n",
    "    # Convert each code column to numeric.\n",
    "    for c in codes_list:\n",
    "        # Replace known \"no data\" markers with NaN.\n",
    "        df[c] = df[c].replace({\"n.d.\": np.nan, \"nd\": np.nan, \"N.D.\": np.nan})\n",
    "        # Coerce to numeric (invalid -> NaN).\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows where date failed to parse; sort by date; reset index.\n",
    "    df = df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "    # Save to parquet cache for faster reruns.\n",
    "    df.to_parquet(cache_path)\n",
    "    # Return the cleaned data.\n",
    "    return df\n",
    "\n",
    "def bcrp_get_cached_or_empty(series_codes, start: str, end: str) -> pd.DataFrame:\n",
    "    # Safe wrapper: if network/API fails, return an empty DataFrame with expected columns.\n",
    "    try:\n",
    "        return bcrp_get(series_codes, start=start, end=end)\n",
    "    except Exception:\n",
    "        # Ensure we return the correct columns even when failing.\n",
    "        if isinstance(series_codes, (list, tuple)):\n",
    "            codes_list = [str(c).strip() for c in series_codes]\n",
    "        else:\n",
    "            codes_list = [str(series_codes).strip()]\n",
    "        return pd.DataFrame(columns=[\"date\"] + codes_list)\n",
    "\n",
    "def yf_download_close_volume(tickers, start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download Close and Volume (real market data) using yfinance.\n",
    "    Returns a DataFrame with columns: [\"date\",\"ticker\",\"close\",\"volume\"] in long format.\n",
    "    If download fails, returns an empty DataFrame with those columns.\n",
    "    \"\"\"\n",
    "    # Try importing yfinance. If missing, return empty DataFrame with expected columns.\n",
    "    try:\n",
    "        import yfinance as yf\n",
    "    except Exception:\n",
    "        return pd.DataFrame(columns=[\"date\",\"ticker\",\"close\",\"volume\"])\n",
    "\n",
    "    # Allow passing a single ticker or a list/tuple of tickers.\n",
    "    cols = tickers if isinstance(tickers, (list, tuple)) else [tickers]\n",
    "    # Create a deterministic cache key.\n",
    "    key = _hash_key(\"yf_long\", \",\".join(cols), start, end)\n",
    "    # Cache filename for this request.\n",
    "    cache_path = CACHE_DIR / f\"yf_long_{key}.parquet\"\n",
    "    # If cached file exists, load it.\n",
    "    if cache_path.exists():\n",
    "        return pd.read_parquet(cache_path)\n",
    "\n",
    "    try:\n",
    "        # Download OHLCV data; auto_adjust=True returns adjusted prices.\n",
    "        data = yf.download(cols, start=start, end=end, auto_adjust=True, progress=False)\n",
    "        # If nothing returned, return empty DataFrame with expected columns.\n",
    "        if data.empty:\n",
    "            return pd.DataFrame(columns=[\"date\",\"ticker\",\"close\",\"volume\"])\n",
    "        # If multiple tickers, yfinance returns MultiIndex columns: (\"Close\", ticker), etc.\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            close = data[\"Close\"].copy()\n",
    "            vol = data[\"Volume\"].copy()\n",
    "        else:\n",
    "            # Single ticker: rename to keep ticker as column label.\n",
    "            close = data[[\"Close\"]].rename(columns={\"Close\": cols[0]})\n",
    "            vol = data[[\"Volume\"]].rename(columns={\"Volume\": cols[0]})\n",
    "        # Name the index so it becomes a column after reset_index().\n",
    "        close.index.name = \"date\"\n",
    "        vol.index.name = \"date\"\n",
    "        # Convert wide -> long: columns become rows with a \"ticker\" column.\n",
    "        long_close = close.reset_index().melt(id_vars=\"date\", var_name=\"ticker\", value_name=\"close\")\n",
    "        long_vol = vol.reset_index().melt(id_vars=\"date\", var_name=\"ticker\", value_name=\"volume\")\n",
    "        # Merge close and volume long tables on (date, ticker).\n",
    "        out = long_close.merge(long_vol, on=[\"date\",\"ticker\"], how=\"inner\").dropna(subset=[\"close\"])\n",
    "        # Cache to parquet.\n",
    "        out.to_parquet(cache_path)\n",
    "        # Return the final long-format DataFrame.\n",
    "        return out\n",
    "    except Exception:\n",
    "        # If anything fails, return an empty DataFrame with expected columns.\n",
    "        return pd.DataFrame(columns=[\"date\",\"ticker\",\"close\",\"volume\"])\n",
    "\n",
    "def safe_head(df: pd.DataFrame, n: int = 5) -> pd.DataFrame:\n",
    "    # If df is a DataFrame, return df.head(n); otherwise return an empty DataFrame.\n",
    "    return df.head(n) if isinstance(df, pd.DataFrame) else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3e812",
   "metadata": {},
   "source": [
    "## 3.2. <a id='series'>Pandas Series</a>\n",
    "\n",
    "We will use:\n",
    "- **BCRPData API**: daily PEN/USD exchange rate (buy/sell)\n",
    "- **Yahoo Finance** via `yfinance`: close/volume for US tickers\n",
    "\n",
    "Data sources:\n",
    "- BCRP API help: https://estadisticas.bcrp.gob.pe/estadisticas/series/ayuda/api\n",
    "- yfinance: https://ranaroussi.github.io/yfinance/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "id": "40b68879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((584, 3),\n",
       " (4970, 4),\n",
       "         date  PEN_USD_buy  PEN_USD_sell\n",
       " 0 2022-02-01     3.871333      3.877667\n",
       " 1 2022-02-02     3.852000      3.857000\n",
       " 2 2022-02-03     3.858500      3.860833\n",
       " 3 2022-02-04     3.863000      3.867833\n",
       " 4 2022-02-07     3.838500      3.845833,\n",
       "         date ticker      close    volume\n",
       " 0 2022-01-03    EEM  44.624969  27572700\n",
       " 1 2022-01-04    EEM  44.470772  24579500\n",
       " 2 2022-01-05    EEM  43.745163  46425100\n",
       " 3 2022-01-06    EEM  43.944714  34288700\n",
       " 4 2022-01-07    EEM  44.343792  32640900)"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START = \"2022-01-01\"\n",
    "END = \"2025-12-18\"\n",
    "\n",
    "# BCRP: daily USD/PEN buy & sell\n",
    "fx = bcrp_get_cached_or_empty([\"PD04637PD\",\"PD04638PD\"], start=START, end=END).rename(\n",
    "    columns={\"PD04637PD\":\"PEN_USD_buy\", \"PD04638PD\":\"PEN_USD_sell\"}\n",
    ")\n",
    "\n",
    "# Yahoo Finance: long-format table (date, ticker, close, volume)\n",
    "tickers = [\"SPY\", \"QQQ\", \"TLT\", \"GLD\", \"EEM\"]\n",
    "us_mkt = yf_download_close_volume(tickers, start=START, end=END)\n",
    "\n",
    "fx.shape, us_mkt.shape, safe_head(fx), safe_head(us_mkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0427a811",
   "metadata": {},
   "source": [
    "### 3.2.1. <a id='3.2.1'>From `lists` to `Series`</a>\n",
    "\n",
    "**Exercise:** create a Series from a Python list using FX mid-rate.\n",
    "\n",
    "1. Create `PEN_USD_mid = (buy + sell)/2`.\n",
    "2. Take the **last 15 values** as a Python list.\n",
    "3. Build a `pd.Series` with those values (index can be 0..14).\n",
    "4. Name the Series `PEN_USD_mid_last15`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "id": "c867c5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEN_USD_mid_last15 (15,)\n",
      "0    3.370750\n",
      "1    3.359500\n",
      "2    3.368214\n",
      "3    3.364071\n",
      "4    3.370393\n",
      "Name: PEN_USD_mid_last15, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 3.2.1 — From list to Series (FX mid-rate)\n",
    "\n",
    "# 1) Mid-rate: average of buy and sell (PEN per USD).\n",
    "if {\"PEN_USD_buy\", \"PEN_USD_sell\"}.issubset(fx.columns) and fx.shape[0] > 0:\n",
    "    PEN_USD_mid = (fx[\"PEN_USD_buy\"] + fx[\"PEN_USD_sell\"]) / 2\n",
    "else:\n",
    "    PEN_USD_mid = pd.Series(dtype=float)\n",
    "\n",
    "# 2) Last 15 values as a plain Python list (drop missing first).\n",
    "mid_last15_list = PEN_USD_mid.dropna().tail(15).tolist()\n",
    "\n",
    "# 3) Build a Series from the list; default integer index 0..14 is fine.\n",
    "PEN_USD_mid_last15 = pd.Series(mid_last15_list, name=\"PEN_USD_mid_last15\")\n",
    "\n",
    "# Optional self-check\n",
    "print(PEN_USD_mid_last15.name, PEN_USD_mid_last15.shape)\n",
    "print(PEN_USD_mid_last15.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100847d",
   "metadata": {},
   "source": [
    "### 3.2.2. <a id='3.2.2'>From `NumPy array` to `Series`</a>\n",
    "\n",
    "**Exercise:** create a Series from a NumPy array using US market close prices.\n",
    "\n",
    "1. Filter `us_mkt` for ticker `SPY`.\n",
    "2. Extract the `close` column as a NumPy array.\n",
    "3. Build a `pd.Series` with:\n",
    "   - data = the NumPy array\n",
    "   - index = the corresponding dates\n",
    "4. Compute `mean`, `min`, `max` using Series methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "id": "60f394a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "2022-01-03    453.210388\n",
      "2022-01-04    453.058594\n",
      "2022-01-05    444.358948\n",
      "2022-01-06    443.941528\n",
      "2022-01-07    442.186310\n",
      "Name: SPY_close, dtype: float64\n",
      "{'mean': 487.0435106058716, 'min': 342.1902160644531, 'max': 689.1699829101562}\n"
     ]
    }
   ],
   "source": [
    "# 3.2.2 — From NumPy array to Series (SPY close)\n",
    "\n",
    "# 1) Filter us_mkt to keep only rows for ticker \"SPY\".\n",
    "spy_df = us_mkt.loc[us_mkt[\"ticker\"].eq(\"SPY\")].copy() if isinstance(us_mkt, pd.DataFrame) else pd.DataFrame()\n",
    "\n",
    "if not spy_df.empty:\n",
    "    # 2) Make sure 'date' is datetime and sorted, since we'll use it as the Series index.\n",
    "    spy_df[\"date\"] = pd.to_datetime(spy_df[\"date\"], errors=\"coerce\")\n",
    "    spy_df = spy_df.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "\n",
    "    # 3) Extract the 'close' column as a NumPy array.\n",
    "    spy_close_np = spy_df[\"close\"].to_numpy()\n",
    "\n",
    "    # 4) Build a pandas Series using:\n",
    "    #    - data = NumPy array of closes\n",
    "    #    - index = the corresponding dates\n",
    "    SPY_close_series = pd.Series(\n",
    "        data=spy_close_np,\n",
    "        index=spy_df[\"date\"],      # keeping this as a DatetimeIndex is nice (no need for .to_numpy())\n",
    "        name=\"SPY_close\"\n",
    "    )\n",
    "\n",
    "    # 5) Compute mean, min, max using Series methods.\n",
    "    summary_stats = {\n",
    "        \"mean\": float(SPY_close_series.mean()),\n",
    "        \"min\": float(SPY_close_series.min()),\n",
    "        \"max\": float(SPY_close_series.max()),\n",
    "    }\n",
    "else:\n",
    "    SPY_close_series = pd.Series(dtype=float)\n",
    "    summary_stats = {\"mean\": np.nan, \"min\": np.nan, \"max\": np.nan}\n",
    "\n",
    "# Optional self-check\n",
    "print(SPY_close_series.head())\n",
    "print(summary_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec93acb",
   "metadata": {},
   "source": [
    "### 3.2.3. <a id='3.2.3'>From `Dictionary` to `Series`</a>\n",
    "\n",
    "**Exercise:** build a dict and convert to a Series.\n",
    "\n",
    "1. Using `us_mkt`, compute the **last available close** for each ticker in `tickers`.\n",
    "2. Store results in a dict: `{ticker: last_close}`.\n",
    "3. Convert to a Series and sort descending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "76d001ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY    671.400024\n",
      "QQQ    600.409973\n",
      "GLD    399.290009\n",
      "TLT     87.800003\n",
      "EEM     52.599998\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 3.2.3 — From dictionary to Series (last close by ticker)\n",
    "\n",
    "# Goal:\n",
    "# 1) For each ticker in `tickers`, find the last available closing price in `us_mkt`.\n",
    "# 2) Store results in a Python dict: {ticker: last_close}.\n",
    "# 3) Convert that dict to a pandas Series and sort it in descending order.\n",
    "\n",
    "last_close_by_ticker = {}\n",
    "\n",
    "if isinstance(us_mkt, pd.DataFrame) and not us_mkt.empty:\n",
    "    tmp = us_mkt.copy()\n",
    "\n",
    "    # Make sure dates are proper datetimes so sorting by time works correctly.\n",
    "    tmp[\"date\"] = pd.to_datetime(tmp[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows where date is missing, then sort by ticker and date\n",
    "    # so the \"last row\" for each ticker is the most recent observation.\n",
    "    tmp = tmp.dropna(subset=[\"date\"]).sort_values([\"ticker\", \"date\"])\n",
    "\n",
    "    # Loop over the tickers we care about and pick the last non-missing close.\n",
    "    for t in tickers:\n",
    "        sub = tmp.loc[tmp[\"ticker\"].eq(t), [\"date\", \"close\"]].dropna(subset=[\"close\"])\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        # Because sub is sorted by date, iloc[-1] is the most recent close.\n",
    "        last_close_by_ticker[t] = float(sub[\"close\"].iloc[-1])\n",
    "\n",
    "# Convert dict -> Series (index=ticker, values=last_close) and sort highest to lowest.\n",
    "last_close_series = pd.Series(last_close_by_ticker, dtype=float).sort_values(ascending=False)\n",
    "\n",
    "# Optional self-check\n",
    "print(last_close_series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1c3de",
   "metadata": {},
   "source": [
    "### 3.2.4. <a id='3.2.4'>`Series` vs `NumPy`</a>\n",
    "\n",
    "1. Create two Series:\n",
    "   - `fx_mid`: FX mid-rate indexed by date\n",
    "   - `spy_close`: SPY close indexed by date\n",
    "2. Create a DataFrame by combining them (pandas aligns on dates).\n",
    "3. Separately, create two NumPy arrays of the same length by truncating to the same number of rows.\n",
    "4. Explain in markdown why pandas alignment is safer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "id": "7e84680a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned (head) — pandas alignment by date (NaNs are expected if calendars differ):\n",
      "            fx_mid   spy_close\n",
      "date                          \n",
      "2022-01-03     NaN  453.210388\n",
      "2022-01-04     NaN  453.058594\n",
      "2022-01-05     NaN  444.358948\n",
      "2022-01-06     NaN  443.941528\n",
      "2022-01-07     NaN  442.186310 \n",
      "\n",
      "Aligned (common dates) head — only dates where both exist:\n",
      "              fx_mid   spy_close\n",
      "date                            \n",
      "2022-02-01  3.874500  429.720306\n",
      "2022-02-02  3.854500  433.894562\n",
      "2022-02-03  3.859667  423.695923\n",
      "2022-02-04  3.865417  425.688232\n",
      "2022-02-07  3.842167  424.322052 \n",
      "\n",
      "NumPy shapes (forced equal length by truncation): (584,) (584,)\n",
      "First 5 NumPy 'paired' dates (often NOT the same date):\n",
      "0 2022-02-01T00:00:00.000000000 2022-01-03T00:00:00.000000000\n",
      "1 2022-02-02T00:00:00.000000000 2022-01-04T00:00:00.000000000\n",
      "2 2022-02-03T00:00:00.000000000 2022-01-05T00:00:00.000000000\n",
      "3 2022-02-04T00:00:00.000000000 2022-01-06T00:00:00.000000000\n",
      "4 2022-02-07T00:00:00.000000000 2022-01-07T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "# 3.2.4 — Series vs NumPy (alignment vs truncation)\n",
    "\n",
    "# Goal:\n",
    "# 1) Create two pandas Series indexed by date:\n",
    "#    - fx_mid: FX mid-rate (PEN per USD) indexed by FX dates\n",
    "#    - spy_close: SPY close indexed by trading dates\n",
    "# 2) Combine them into a DataFrame. Pandas aligns by date labels (index).\n",
    "# 3) Create two NumPy arrays of the same length by truncation (no date alignment).\n",
    "# 4) Show why pandas alignment is safer.\n",
    "\n",
    "# -------------------------\n",
    "# 1) Build Series with a date index\n",
    "# -------------------------\n",
    "\n",
    "# FX mid-rate Series (indexed by FX dates)\n",
    "if {\"PEN_USD_buy\", \"PEN_USD_sell\", \"date\"}.issubset(fx.columns) and not fx.empty:\n",
    "    fx_tmp = fx[[\"date\", \"PEN_USD_buy\", \"PEN_USD_sell\"]].copy()\n",
    "    fx_tmp[\"date\"] = pd.to_datetime(fx_tmp[\"date\"], errors=\"coerce\")\n",
    "    fx_tmp = fx_tmp.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "\n",
    "    fx_mid = (fx_tmp[\"PEN_USD_buy\"] + fx_tmp[\"PEN_USD_sell\"]) / 2\n",
    "    fx_mid.index = fx_tmp[\"date\"]          # date labels live in the index\n",
    "    fx_mid.name = \"fx_mid\"\n",
    "else:\n",
    "    fx_mid = pd.Series(dtype=float, name=\"fx_mid\")\n",
    "\n",
    "# SPY close Series (indexed by trading dates)\n",
    "if isinstance(us_mkt, pd.DataFrame) and not us_mkt.empty:\n",
    "    spy_tmp = us_mkt.loc[us_mkt[\"ticker\"].eq(\"SPY\"), [\"date\", \"close\"]].copy()\n",
    "    spy_tmp[\"date\"] = pd.to_datetime(spy_tmp[\"date\"], errors=\"coerce\")\n",
    "    spy_tmp = spy_tmp.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "\n",
    "    spy_close = pd.Series(spy_tmp[\"close\"].to_numpy(), index=spy_tmp[\"date\"], name=\"spy_close\")\n",
    "else:\n",
    "    spy_close = pd.Series(dtype=float, name=\"spy_close\")\n",
    "\n",
    "# -------------------------\n",
    "# 2) Pandas: safe alignment by date\n",
    "# -------------------------\n",
    "# Pandas matches values by the date index (labels), not by row position.\n",
    "# If a date exists in SPY but not in FX (or vice versa), you will see NaN.\n",
    "aligned_df = pd.concat([fx_mid, spy_close], axis=1)\n",
    "\n",
    "# Optional: keep only dates where BOTH series are available (intersection)\n",
    "aligned_common = aligned_df.dropna()\n",
    "\n",
    "# -------------------------\n",
    "# 3) NumPy: unsafe truncation by position\n",
    "# -------------------------\n",
    "# NumPy arrays have no date labels. Truncation forces the same length,\n",
    "# but it pairs values by position (row 0 with row 0), even if the dates differ.\n",
    "fx_dates = fx_mid.index.to_numpy()\n",
    "spy_dates = spy_close.index.to_numpy()\n",
    "\n",
    "fx_vals = fx_mid.to_numpy()\n",
    "spy_vals = spy_close.to_numpy()\n",
    "\n",
    "n = min(len(fx_vals), len(spy_vals))\n",
    "fx_np = fx_vals[:n]\n",
    "spy_np = spy_vals[:n]\n",
    "fx_np_dates = fx_dates[:n]\n",
    "spy_np_dates = spy_dates[:n]\n",
    "\n",
    "# -------------------------\n",
    "# Demonstration\n",
    "# -------------------------\n",
    "print(\"Aligned (head) — pandas alignment by date (NaNs are expected if calendars differ):\")\n",
    "print(aligned_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Aligned (common dates) head — only dates where both exist:\")\n",
    "print(aligned_common.head(), \"\\n\")\n",
    "\n",
    "print(\"NumPy shapes (forced equal length by truncation):\", fx_np.shape, spy_np.shape)\n",
    "print(\"First 5 NumPy 'paired' dates (often NOT the same date):\")\n",
    "for i in range(min(5, n)):\n",
    "    print(i, fx_np_dates[i], spy_np_dates[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02fa6f",
   "metadata": {},
   "source": [
    "### 3.2.5. <a id='3.2.5'>Indexing</a>\n",
    "\n",
    "**Exercise:** practice `.loc` and `.iloc`.\n",
    "\n",
    "1. From `last_close_series`, use `.iloc` to take the top 3 tickers.\n",
    "2. Use `.loc` to select the value for `SPY`.\n",
    "3. If `SPY` is not present, explain why (in markdown).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "54c656de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY    671.400024\n",
      "QQQ    600.409973\n",
      "GLD    399.290009\n",
      "dtype: float64\n",
      "SPY value: 671.4000244140625\n"
     ]
    }
   ],
   "source": [
    "# 3.2.5 — Indexing with .iloc and .loc\n",
    "\n",
    "# last_close_series is a pandas Series whose:\n",
    "#   - index = ticker symbols (e.g., \"SPY\", \"QQQ\", ...)\n",
    "#   - values = last available close price for each ticker\n",
    "# It is already sorted in descending order (highest close first).\n",
    "\n",
    "# 1) Use .iloc (position-based indexing) to take the first 3 entries.\n",
    "#    Since the Series is sorted descending, these are the \"top 3\" tickers by last close.\n",
    "top3 = last_close_series.iloc[:3]\n",
    "\n",
    "# 2) Use .loc (label-based indexing) to fetch the value for the ticker \"SPY\".\n",
    "#    We first check membership to avoid a KeyError if \"SPY\" is not present.\n",
    "if \"SPY\" in last_close_series.index:\n",
    "    spy_value = float(last_close_series.loc[\"SPY\"])\n",
    "else:\n",
    "    spy_value = np.nan\n",
    "\n",
    "# self-check\n",
    "print(top3)\n",
    "print(\"SPY value:\", spy_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8d503",
   "metadata": {},
   "source": [
    "## 3.3. <a id='3.3'>DataFrame</a>\n",
    "\n",
    "We now practice DataFrame creation and common methods using the same datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799d65a",
   "metadata": {},
   "source": [
    "### 3.3.1. <a id='3.3.1'>DataFrame Generation</a>\n",
    "\n",
    "#### From `lists` and `dict` to `DataFrame`\n",
    "\n",
    "**Exercie:** create a DataFrame of ticker metadata.\n",
    "\n",
    "1. Make a list of tickers.\n",
    "2. Make a list of last closes (same order).\n",
    "3. Make a dict for an extra column, e.g. `{ticker: 'US'}`.\n",
    "4. Build a DataFrame with columns: `ticker`, `last_close`, `market`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "e716c835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker  last_close market\n",
      "0    SPY  671.400024     US\n",
      "1    QQQ  600.409973     US\n",
      "2    GLD  399.290009     US\n",
      "3    TLT   87.800003     US\n",
      "4    EEM   52.599998     US\n"
     ]
    }
   ],
   "source": [
    "# 3.3.1 — DataFrame from lists + dict (ticker metadata)\n",
    "\n",
    "# Step 1) Create a list of tickers (taken from the index of last_close_series).\n",
    "tickers_list = list(last_close_series.index)\n",
    "\n",
    "# Step 2) Create a list of last closes in the same order as tickers_list.\n",
    "#         last_close_series.values follows the same order as the index.\n",
    "last_close_list = [float(v) for v in last_close_series.values]\n",
    "\n",
    "# Step 3) Create a dictionary for an extra metadata column.\n",
    "#         Example: tag every ticker as belonging to the \"US\" market.\n",
    "market_dict = {t: \"US\" for t in tickers_list}\n",
    "\n",
    "# Step 4) Build the DataFrame with the required columns.\n",
    "#         - \"ticker\" and \"last_close\" come from the two lists\n",
    "#         - \"market\" is created by looking up each ticker in market_dict\n",
    "ticker_df = pd.DataFrame(\n",
    "    {\n",
    "        \"ticker\": tickers_list,\n",
    "        \"last_close\": last_close_list,\n",
    "        \"market\": [market_dict[t] for t in tickers_list],\n",
    "    }\n",
    ")\n",
    "print(ticker_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed4e31",
   "metadata": {},
   "source": [
    "#### From `lists` and `NumPy` to `DataFrame`\n",
    "\n",
    "**Exercie:** build a DataFrame from NumPy arrays.\n",
    "\n",
    "1. Take the `close` column for `SPY` and `QQQ` from `us_mkt`.\n",
    "2. Convert each to a NumPy array.\n",
    "3. Build a DataFrame with 2 columns: `SPY_close`, `QQQ_close`.\n",
    "4. Add a column with row index (0..n-1) named `t`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "id": "15813335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SPY_close   QQQ_close  t\n",
      "0  453.210388  392.184082  0\n",
      "1  453.058594  387.097229  1\n",
      "2  444.358948  375.205200  2\n",
      "3  443.941528  374.941589  3\n",
      "4  442.186310  370.879913  4\n"
     ]
    }
   ],
   "source": [
    "# 3.3.1 — DataFrame from NumPy arrays (SPY vs QQQ closes)\n",
    "\n",
    "# Step 1) Filter `us_mkt` to get the Close prices for SPY and QQQ.\n",
    "#         We keep only the \"close\" column, drop missing values, and convert to NumPy arrays.\n",
    "spy_close = us_mkt.loc[us_mkt[\"ticker\"].eq(\"SPY\"), \"close\"].dropna().to_numpy()\n",
    "qqq_close = us_mkt.loc[us_mkt[\"ticker\"].eq(\"QQQ\"), \"close\"].dropna().to_numpy()\n",
    "\n",
    "# Step 2) NumPy arrays do not align by date labels.\n",
    "#         To make them the same length, truncate both to the length of the shorter array.\n",
    "n = min(len(spy_close), len(qqq_close))\n",
    "spy_close = spy_close[:n]\n",
    "qqq_close = qqq_close[:n]\n",
    "\n",
    "# Step 3) Build a DataFrame using the two NumPy arrays as columns.\n",
    "prices_np_df = pd.DataFrame({\n",
    "    \"SPY_close\": spy_close,\n",
    "    \"QQQ_close\": qqq_close,\n",
    "})\n",
    "\n",
    "# Step 4) Add a simple row counter column t = 0..n-1 (useful as a time index in this  example).\n",
    "prices_np_df[\"t\"] = np.arange(n)\n",
    "print(prices_np_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd1ec2",
   "metadata": {},
   "source": [
    "### 3.3.2. <a id='3.3.2'>Indexing</a>\n",
    "\n",
    "**Exercie:** `.loc` and `.iloc` on DataFrames.\n",
    "\n",
    "1. Use `.iloc` to take first 5 rows of `us_mkt`.\n",
    "2. Use `.loc` with a boolean condition to keep only rows where `ticker == 'SPY'`.\n",
    "3. Select only the columns `date`, `ticker`, `close`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "3f4adde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date ticker       close\n",
      "2982 2022-01-03    SPY  453.210388\n",
      "2983 2022-01-04    SPY  453.058594\n",
      "2984 2022-01-05    SPY  444.358948\n",
      "2985 2022-01-06    SPY  443.941528\n",
      "2986 2022-01-07    SPY  442.186310\n"
     ]
    }
   ],
   "source": [
    "# 3.3.2 — Indexing with .iloc and .loc\n",
    "\n",
    "# 1) .iloc selects rows by POSITION (row numbers 0,1,2,3,4).\n",
    "#    Here we take the first 5 rows of us_mkt.\n",
    "first5 = us_mkt.iloc[:5].copy() if isinstance(us_mkt, pd.DataFrame) else pd.DataFrame()\n",
    "\n",
    "# 2) .loc selects rows by LABEL / CONDITION.\n",
    "#    Here we filter to keep only rows where ticker == \"SPY\"\n",
    "# 3) and at the same time we select only the columns: date, ticker, close.\n",
    "only_spy = (\n",
    "    us_mkt.loc[us_mkt[\"ticker\"].eq(\"SPY\"), [\"date\", \"ticker\", \"close\"]].copy()\n",
    "    if isinstance(us_mkt, pd.DataFrame)\n",
    "    else pd.DataFrame()\n",
    ")\n",
    "\n",
    "# Optional self-check: show the first rows of the filtered result\n",
    "if isinstance(only_spy, pd.DataFrame) and only_spy.shape[0] > 0:\n",
    "    print(only_spy.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3a27c",
   "metadata": {},
   "source": [
    "### 3.3.3. <a id='3.3.3'>General Methods</a>\n",
    "\n",
    "**Exercie:** basic methods: `.shape`, `.columns`, `.info`, `.describe`, `.sort_values`.\n",
    "\n",
    "1. Show `us_mkt.shape` and `us_mkt.columns`.\n",
    "2. Use `.describe()` on `close` and `volume`.\n",
    "3. Sort `us_mkt` by `volume` descending and keep top 10 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "9da61ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4970, 4)\n",
      "Columns: ['date', 'ticker', 'close', 'volume'] \n",
      "\n",
      "Describe (close, volume):\n",
      "             close        volume\n",
      "count  4970.000000  4.970000e+03\n",
      "mean    250.238890  4.042105e+07\n",
      "std     187.757433  2.956145e+07\n",
      "min      31.036425  1.436500e+06\n",
      "25%      85.874727  1.882345e+07\n",
      "50%     187.864998  3.452785e+07\n",
      "75%     399.523529  5.666980e+07\n",
      "max     689.169983  2.566114e+08 \n",
      "\n",
      "Top 10 by volume:\n",
      "           date ticker       close     volume\n",
      "3799 2025-04-07    SPY  501.502930  256611400\n",
      "2996 2022-01-24    SPY  417.282562  251783900\n",
      "3801 2025-04-09    SPY  545.490601  241867300\n",
      "3798 2025-04-04    SPY  502.397797  217965100\n",
      "3018 2022-02-24    SPY  406.334412  213942900\n",
      "2995 2022-01-21    SPY  415.517944  202271200\n",
      "2002 2022-01-24    QQQ  344.947845  198685800\n",
      "3279 2023-03-10    SPY  372.058044  189253000\n",
      "2998 2022-01-26    SPY  411.153931  186391100\n",
      "3282 2023-03-15    SPY  375.307159  172996900\n"
     ]
    }
   ],
   "source": [
    "# 3.3.3 — General methods\n",
    "\n",
    "# We first check that `us_mkt` is a non-empty pandas DataFrame.\n",
    "if isinstance(us_mkt, pd.DataFrame) and not us_mkt.empty:\n",
    "\n",
    "    # 1) Basic structure: shape = (number of rows, number of columns)\n",
    "    #    and the list of column names.\n",
    "    print(\"Shape:\", us_mkt.shape)\n",
    "    print(\"Columns:\", list(us_mkt.columns), \"\\n\")\n",
    "\n",
    "    # 2) Descriptive statistics for the numeric columns \"close\" and \"volume\".\n",
    "    #    .describe() reports count, mean, std, min, quartiles, and max.\n",
    "    desc = us_mkt[[\"close\", \"volume\"]].describe()\n",
    "    print(\"Describe (close, volume):\")\n",
    "    print(desc, \"\\n\")\n",
    "\n",
    "    # 3) Sort the DataFrame by \"volume\" in descending order (largest first)\n",
    "    #    and keep only the top 10 rows with the highest volume.\n",
    "    top10_volume = us_mkt.sort_values(\"volume\", ascending=False).head(10)\n",
    "    print(\"Top 10 by volume:\")\n",
    "    print(top10_volume)\n",
    "\n",
    "else:\n",
    "    # If `us_mkt` is empty or not a DataFrame, create empty placeholders.\n",
    "    desc = pd.DataFrame()\n",
    "    top10_volume = pd.DataFrame()\n",
    "    print(\"us_mkt is empty or not a DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5dd1b9",
   "metadata": {},
   "source": [
    "### 3.3.4. <a id='3.3.4'>Importing Data</a>\n",
    "\n",
    "**Exercie:** `to_csv` + `read_csv` using real data.\n",
    "\n",
    "1. Save a subset of `us_mkt` to `data/us_mkt_sample.csv` (e.g., 500 rows).\n",
    "2. Read it back using `pd.read_csv`.\n",
    "3. Rename columns to snake_case.\n",
    "4. Check dtypes and missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "658402d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtypes:\n",
      "date       object\n",
      "ticker     object\n",
      "close     float64\n",
      "volume      int64\n",
      "dtype: object \n",
      "\n",
      "Missing values per column:\n",
      "date      0\n",
      "ticker    0\n",
      "close     0\n",
      "volume    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 3.3.4 — Importing data: to_csv + read_csv\n",
    "# We first check that `us_mkt` is a non-empty pandas DataFrame.\n",
    "if isinstance(us_mkt, pd.DataFrame) and not us_mkt.empty:\n",
    "\n",
    "    # Make sure the output folder (\"data\") exists; create it if it doesn't.\n",
    "    Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Take a small sample (first 500 rows) from `us_mkt`\n",
    "    #    and save it as a CSV file (without writing the index column).\n",
    "    us_sample = us_mkt.head(500).copy()\n",
    "    us_sample.to_csv(\"data/us_mkt_sample.csv\", index=False)\n",
    "\n",
    "    # 2) Read the CSV file back into a new DataFrame.\n",
    "    us_from_csv = pd.read_csv(\"data/us_mkt_sample.csv\")\n",
    "\n",
    "    # 3) Rename columns to snake_case:\n",
    "    #    - remove leading/trailing spaces\n",
    "    #    - convert to lowercase\n",
    "    #    - replace spaces with underscores\n",
    "    us_from_csv.columns = [c.strip().lower().replace(\" \", \"_\") for c in us_from_csv.columns]\n",
    "\n",
    "    # 4) Check data types and missing values after reading the CSV.\n",
    "    #    This helps verify that numeric columns stayed numeric and nothing was lost.\n",
    "    print(\"Dtypes:\")\n",
    "    print(us_from_csv.dtypes, \"\\n\")\n",
    "\n",
    "    print(\"Missing values per column:\")\n",
    "    print(us_from_csv.isna().sum())\n",
    "\n",
    "else:\n",
    "    # If `us_mkt` is empty or not a DataFrame, return empty placeholders.\n",
    "    us_sample = pd.DataFrame()\n",
    "    us_from_csv = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b051852",
   "metadata": {},
   "source": [
    "### 3.3.5. <a id='3.3.5'>Filtering data</a>\n",
    "\n",
    "**Exercise:** filtering with conditions.\n",
    "\n",
    "1. Filter `us_mkt` for rows where `close` is above the 90th percentile **within each ticker**.\n",
    "2. Filter rows with `volume` missing (if any) and count them.\n",
    "3. Create a filtered DataFrame for tickers `['SPY','GLD']` only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "aa4f33e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-close rows shape: (500, 4)\n",
      "Missing volume rows: 0\n",
      "SPY+GLD subset shape: (1988, 4)\n",
      "          date ticker       close    volume\n",
      "994 2022-01-03    GLD  168.330002   9014400\n",
      "995 2022-01-04    GLD  169.570007   6965600\n",
      "996 2022-01-05    GLD  169.059998   8715600\n",
      "997 2022-01-06    GLD  166.990005  10902700\n",
      "998 2022-01-07    GLD  167.750000   8191900\n"
     ]
    }
   ],
   "source": [
    "# 3.3.5 — Filtering data (well explained)\n",
    "\n",
    "if isinstance(us_mkt, pd.DataFrame) and not us_mkt.empty:\n",
    "    tmp = us_mkt.copy()\n",
    "\n",
    "    # 1) Close above the 90th percentile *within each ticker*\n",
    "    #    - groupby(\"ticker\") splits the data by ticker\n",
    "    #    - transform(...) returns a threshold value for every row (same length as tmp)\n",
    "    #    - then we keep rows where close > that ticker-specific threshold\n",
    "    p90 = tmp.groupby(\"ticker\")[\"close\"].transform(lambda s: s.quantile(0.90))\n",
    "    high_close = tmp.loc[tmp[\"close\"] > p90].copy()\n",
    "\n",
    "    # 2) Rows with missing volume (if any) + count them\n",
    "    missing_volume = tmp.loc[tmp[\"volume\"].isna()].copy()\n",
    "    missing_volume_count = int(missing_volume.shape[0])\n",
    "\n",
    "    # 3) Keep only SPY and GLD rows\n",
    "    spy_gld = tmp.loc[tmp[\"ticker\"].isin([\"SPY\", \"GLD\"])].copy()\n",
    "\n",
    "else:\n",
    "    # If us_mkt is empty or not a DataFrame, create empty placeholders\n",
    "    tmp = pd.DataFrame()\n",
    "    high_close = pd.DataFrame()\n",
    "    missing_volume = pd.DataFrame()\n",
    "    missing_volume_count = 0\n",
    "    spy_gld = pd.DataFrame()\n",
    "\n",
    "# Optional quick checks\n",
    "print(\"High-close rows shape:\", high_close.shape)\n",
    "print(\"Missing volume rows:\", missing_volume_count)\n",
    "print(\"SPY+GLD subset shape:\", spy_gld.shape)\n",
    "print(spy_gld.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6b93b",
   "metadata": {},
   "source": [
    "### 3.3.6. <a id='3.3.6'>Dealing with nulls</a>\n",
    "\n",
    "**Exercise:** introduce NaNs and handle them.\n",
    "\n",
    "1. Copy `us_mkt` to `us_mkt_nan`.\n",
    "2. Set 1% of the `close` values to NaN (fixed random seed).\n",
    "3. Create two cleaned versions:\n",
    "   - dropped NaNs\n",
    "   - filled NaNs with the ticker-specific median close\n",
    "4. Compare shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "id": "f7cbc4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (4970, 4) With NaNs: (4970, 4)\n",
      "Drop: (4921, 4) Fill: (4970, 4)\n"
     ]
    }
   ],
   "source": [
    "# 3.3.6 — Dealing with nulls (introduce NaNs then clean)\n",
    "\n",
    "# Step 1) Make a copy of the original dataset so we do not modify `us_mkt`.\n",
    "us_mkt_nan = us_mkt.copy()\n",
    "\n",
    "if isinstance(us_mkt_nan, pd.DataFrame) and us_mkt_nan.shape[0] > 0:\n",
    "\n",
    "    # Step 2) Randomly set 1% of the 'close' values to NaN (missing).\n",
    "    # - We use a fixed random seed (123) so results are reproducible.\n",
    "    rng = np.random.default_rng(123)\n",
    "    n = us_mkt_nan.shape[0]                 # total number of rows\n",
    "    k = max(1, int(0.01 * n))               # number of rows to corrupt (1% of n, at least 1)\n",
    "\n",
    "    # Choose k random row positions (0..n-1) without replacement\n",
    "    idx = rng.choice(n, size=k, replace=False)\n",
    "\n",
    "    # Convert those row positions into actual DataFrame index labels, then set close = NaN\n",
    "    us_mkt_nan.loc[us_mkt_nan.index[idx], \"close\"] = np.nan\n",
    "\n",
    "    # Step 3a) Clean version 1: drop rows where 'close' is missing.\n",
    "    # This removes observations and therefore reduces the number of rows.\n",
    "    us_drop = us_mkt_nan.dropna(subset=[\"close\"]).copy()\n",
    "\n",
    "    # Step 3b) Clean version 2: fill missing 'close' values using the ticker-specific median.\n",
    "    # - groupby(\"ticker\") computes the median close within each ticker\n",
    "    # - transform(\"median\") returns a median value for every row (aligned to the original rows)\n",
    "    med = us_mkt_nan.groupby(\"ticker\")[\"close\"].transform(\"median\")\n",
    "\n",
    "    # Fill NaN closes with the corresponding ticker median (keeps the same number of rows)\n",
    "    us_fill = us_mkt_nan.copy()\n",
    "    us_fill[\"close\"] = us_fill[\"close\"].fillna(med)\n",
    "\n",
    "else:\n",
    "    # If input is empty or not a DataFrame, create empty placeholders.\n",
    "    us_drop = pd.DataFrame()\n",
    "    us_fill = pd.DataFrame()\n",
    "\n",
    "# Step 4) Compare shapes to see the effect of dropping vs filling.\n",
    "print(\"Original:\", us_mkt.shape, \"With NaNs:\", us_mkt_nan.shape)\n",
    "print(\"Drop:\", us_drop.shape, \"Fill:\", us_fill.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c49c5",
   "metadata": {},
   "source": [
    "### 3.3.7. <a id='3.3.7'>Duplicates</a>\n",
    "\n",
    "**Exercise:** create duplicates and remove them.\n",
    "\n",
    "1. Create `dup_df` by stacking the last 5 rows of `us_mkt` twice.\n",
    "2. Use `.duplicated()` to detect duplicates.\n",
    "3. Use `.drop_duplicates()` to remove duplicates.\n",
    "4. Verify row counts before/after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "id": "50e8b1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dup_df rows: 10\n",
      "Number of duplicated rows (True in dup_mask): 5\n",
      "dedup_df rows: 5\n"
     ]
    }
   ],
   "source": [
    "# 3.3.7 — Duplicates (create duplicates, detect them, and remove them)\n",
    "\n",
    "if isinstance(us_mkt, pd.DataFrame) and not us_mkt.empty:\n",
    "\n",
    "    # Step 1) Take the last 5 rows of us_mkt.\n",
    "    # These 5 rows will be used to create an example dataset with duplicates.\n",
    "    last5 = us_mkt.tail(5).copy()\n",
    "\n",
    "    # Stack (concatenate) the last 5 rows twice.\n",
    "    # Result: dup_df has 10 rows, where rows 0–4 are repeated again in rows 5–9.\n",
    "    dup_df = pd.concat([last5, last5], ignore_index=True)\n",
    "\n",
    "    # Step 2) Detect duplicates.\n",
    "    # .duplicated() returns a boolean Series:\n",
    "    # - False for the first time a row appears\n",
    "    # - True for later repeated copies of the same row\n",
    "    dup_mask = dup_df.duplicated()\n",
    "\n",
    "    # Step 3) Remove duplicates.\n",
    "    # .drop_duplicates() keeps the first occurrence and removes repeated copies.\n",
    "    dedup_df = dup_df.drop_duplicates().copy()\n",
    "\n",
    "else:\n",
    "    # If us_mkt is empty or not a DataFrame, create empty placeholders.\n",
    "    dup_df = pd.DataFrame()\n",
    "    dup_mask = pd.Series(dtype=bool)\n",
    "    dedup_df = pd.DataFrame()\n",
    "\n",
    "# Step 4) Verify row counts before and after removing duplicates\n",
    "print(\"dup_df rows:\", dup_df.shape[0])\n",
    "print(\"Number of duplicated rows (True in dup_mask):\", int(dup_mask.sum()) if len(dup_mask) else 0)\n",
    "print(\"dedup_df rows:\", dedup_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba91b8dd",
   "metadata": {},
   "source": [
    "### 3.3.8. <a id='3.3.8'>Groupby</a>\n",
    "\n",
    "**Exercise:** groupby + aggregation.\n",
    "\n",
    "1. Group `us_mkt` by `ticker` and compute:\n",
    "   - mean close\n",
    "   - median close\n",
    "   - max volume\n",
    "2. Rename the resulting columns clearly.\n",
    "3. Sort by mean close descending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "id": "51da27e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker  mean_close  median_close  max_volume\n",
      "0    SPY  487.043511    462.101456   256611400\n",
      "1    QQQ  411.806856    400.730148   198685800\n",
      "2    GLD  220.130422    187.864998    62025000\n",
      "3    TLT   91.751313     88.894325   131353500\n",
      "4    EEM   40.462350     39.107405   134225700\n"
     ]
    }
   ],
   "source": [
    "# 3.3.8 — Groupby + aggregation (ticker-level summary)\n",
    "\n",
    "if isinstance(us_mkt, pd.DataFrame) and not us_mkt.empty:\n",
    "\n",
    "    # 1) Group the data by ticker and compute summary statistics:\n",
    "    #    - mean_close: average closing price for each ticker\n",
    "    #    - median_close: median closing price for each ticker\n",
    "    #    - max_volume: maximum daily trading volume observed for each ticker\n",
    "    ticker_summary = (\n",
    "        us_mkt.groupby(\"ticker\")\n",
    "        .agg(\n",
    "            mean_close=(\"close\", \"mean\"),\n",
    "            median_close=(\"close\", \"median\"),\n",
    "            max_volume=(\"volume\", \"max\"),\n",
    "        )\n",
    "        # 2) Move the group label (ticker) back from the index into a regular column\n",
    "        .reset_index()\n",
    "        # 3) Sort tickers by mean close (highest average price first)\n",
    "        .sort_values(\"mean_close\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "else:\n",
    "    ticker_summary = pd.DataFrame()\n",
    "print(ticker_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196dffb",
   "metadata": {},
   "source": [
    "### 3.3.9. <a id='3.3.9'>Reform</a>\n",
    "\n",
    "##### From Wide to Long\n",
    "\n",
    "**Exercise:** Merge a wide table.\n",
    "\n",
    "1. Create a small, wide DataFrame with one row containing the last closing dates for each ticker.\n",
    "\n",
    "2. Merge it into the long format with the columns: `ticker`, `last_close`.\n",
    "\n",
    "#### From Long to Wide\n",
    "\n",
    "**Exercise:** \n",
    "\n",
    "3. Using `us_mkt`, create a pivot table with:\n",
    "- index = `date`\n",
    "- columns = `ticker`\n",
    "- values ​​= `close`\n",
    "4. Keep only the first 50 dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "id": "bda69690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIDE (last closes):\n",
      "                   SPY         QQQ         GLD        TLT        EEM\n",
      "last_close  671.400024  600.409973  399.290009  87.800003  52.599998 \n",
      "\n",
      "LONG (ticker, last_close):\n",
      "  ticker  last_close\n",
      "0    SPY  671.400024\n",
      "1    QQQ  600.409973\n",
      "2    GLD  399.290009\n",
      "3    TLT   87.800003\n",
      "4    EEM   52.599998 \n",
      "\n",
      "WIDE (pivot: date x ticker, first 50 dates):\n",
      "ticker       date        EEM         GLD         QQQ         SPY         TLT\n",
      "0      2022-01-03  44.624969  168.330002  392.184082  453.210388  125.782967\n",
      "1      2022-01-04  44.470772  169.570007  387.097229  453.058594  125.259987\n",
      "2      2022-01-05  43.745163  169.059998  375.205200  444.358948  124.580070\n",
      "3      2022-01-06  43.944714  166.990005  374.941589  443.941528  124.902588\n",
      "4      2022-01-07  44.343792  167.750000  370.879913  442.186310  124.004715\n"
     ]
    }
   ],
   "source": [
    "# 3.3.9 — Reshape: wide ↔ long \n",
    "\n",
    "# Part A) From WIDE to LONG\n",
    "# 1) Create a small WIDE DataFrame (one row) with the last close for each ticker.\n",
    "#    This means: columns = tickers, one row = last_close values.\n",
    "if isinstance(last_close_series, pd.Series) and len(last_close_series) > 0:\n",
    "    wide_last = last_close_series.to_frame().T          # wide: 1 row, columns=tickers\n",
    "    wide_last.index = [\"last_close\"]                    # optional row label\n",
    "else:\n",
    "    wide_last = pd.DataFrame()\n",
    "\n",
    "# 2) Convert that WIDE table into LONG format with columns: ticker, last_close.\n",
    "#    melt() turns column names (tickers) into a \"ticker\" column and values into \"last_close\".\n",
    "if not wide_last.empty:\n",
    "    long_last = (\n",
    "        wide_last.reset_index(drop=True)                # drop the row label (not needed)\n",
    "        .melt(var_name=\"ticker\", value_name=\"last_close\")\n",
    "        .dropna(subset=[\"last_close\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    long_last = pd.DataFrame(columns=[\"ticker\", \"last_close\"])\n",
    "\n",
    "\n",
    "# Part B) From LONG to WIDE\n",
    "# 3) Using us_mkt (long format), create a pivot table with:\n",
    "#    - index   = date\n",
    "#    - columns = ticker\n",
    "#    - values  = close\n",
    "#    If there are duplicates for the same (date, ticker), we take the last one (aggfunc=\"last\").\n",
    "if isinstance(us_mkt, pd.DataFrame) and not us_mkt.empty:\n",
    "    tmp = us_mkt.copy()\n",
    "    tmp[\"date\"] = pd.to_datetime(tmp[\"date\"], errors=\"coerce\")\n",
    "    tmp = tmp.dropna(subset=[\"date\"])\n",
    "\n",
    "    wide_close = (\n",
    "        tmp.pivot_table(index=\"date\", columns=\"ticker\", values=\"close\", aggfunc=\"last\")\n",
    "        .sort_index()                                   # make sure dates are in chronological order\n",
    "    )\n",
    "\n",
    "    # 4) Keep only the first 50 dates (rows).\n",
    "    wide_close = wide_close.iloc[:50].reset_index()\n",
    "else:\n",
    "    wide_close = pd.DataFrame()\n",
    "print(\"WIDE (last closes):\")\n",
    "print(wide_last, \"\\n\")\n",
    "\n",
    "print(\"LONG (ticker, last_close):\")\n",
    "print(long_last.head(), \"\\n\")\n",
    "\n",
    "print(\"WIDE (pivot: date x ticker, first 50 dates):\")\n",
    "print(wide_close.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2170e6e",
   "metadata": {},
   "source": [
    "### 3.3.10. <a id='3.3.10'>Merge</a>\n",
    "\n",
    "**Exercise:** merge Peru macro data (BCRP) with US market data (Yahoo).\n",
    "\n",
    "1. Fetch BCRP monthly policy rate: `PD12301MD`.\n",
    "2. Create a monthly table from US market data by extracting `year` and `month` from the `date` column.\n",
    "   Hint: you can use `pd.to_datetime` **only here**.\n",
    "3. Compute the monthly average close for SPY.\n",
    "4. Merge policy rate with monthly SPY average using `merge`.\n",
    "5. Save to `outputs/lecture2_policy_spy_monthly.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "id": "fbb87fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs/lecture2_policy_spy_monthly.csv | shape: (28, 4)\n"
     ]
    }
   ],
   "source": [
    "# 3.3.10 — Merge (monthly policy rate with monthly SPY average)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# 0) Ensure output folder exists\n",
    "Path(\"outputs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Fetch BCRP policy rate series and rename column\n",
    "# Note: bcrp_get(...) already parses the \"date\" column into datetime.\n",
    "policy = (\n",
    "    bcrp_get_cached_or_empty(\"PD12301MD\", start=START, end=END)\n",
    "    .rename(columns={\"PD12301MD\": \"policy_rate\"})\n",
    ")\n",
    "\n",
    "# 2) Build a monthly table from US market data (SPY)\n",
    "# We use pd.to_datetime here (as suggested) to extract year/month from the date column.\n",
    "if isinstance(us_mkt, pd.DataFrame) and not us_mkt.empty:\n",
    "    tmp = us_mkt.copy()\n",
    "    tmp[\"date\"] = pd.to_datetime(tmp[\"date\"], errors=\"coerce\")\n",
    "    tmp = tmp.dropna(subset=[\"date\"])\n",
    "\n",
    "    tmp[\"year\"] = tmp[\"date\"].dt.year\n",
    "    tmp[\"month\"] = tmp[\"date\"].dt.month\n",
    "\n",
    "    # 3) Compute monthly average close for SPY\n",
    "    spy_monthly = (\n",
    "        tmp.loc[tmp[\"ticker\"].eq(\"SPY\")]\n",
    "        .groupby([\"year\", \"month\"], as_index=False)[\"close\"]\n",
    "        .mean()\n",
    "        .rename(columns={\"close\": \"spy_close_avg\"})\n",
    "    )\n",
    "else:\n",
    "    spy_monthly = pd.DataFrame(columns=[\"year\", \"month\", \"spy_close_avg\"])\n",
    "\n",
    "# 1b/3b) Convert policy series into a monthly table (average within each month)\n",
    "# Since policy[\"date\"] should already be datetime, we can extract year/month directly.\n",
    "if isinstance(policy, pd.DataFrame) and not policy.empty and \"date\" in policy.columns:\n",
    "    pol = policy.dropna(subset=[\"date\"]).copy()\n",
    "    pol[\"year\"] = pol[\"date\"].dt.year\n",
    "    pol[\"month\"] = pol[\"date\"].dt.month\n",
    "\n",
    "    policy_monthly = (\n",
    "        pol.groupby([\"year\", \"month\"], as_index=False)[\"policy_rate\"]\n",
    "        .mean()\n",
    "    )\n",
    "else:\n",
    "    policy_monthly = pd.DataFrame(columns=[\"year\", \"month\", \"policy_rate\"])\n",
    "\n",
    "# 4) Merge on (year, month)\n",
    "if not spy_monthly.empty and not policy_monthly.empty:\n",
    "    merged_monthly = (\n",
    "        spy_monthly.merge(policy_monthly, on=[\"year\", \"month\"], how=\"inner\")\n",
    "        .sort_values([\"year\", \"month\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    merged_monthly = pd.DataFrame(columns=[\"year\", \"month\", \"spy_close_avg\", \"policy_rate\"])\n",
    "\n",
    "# 5) Save output\n",
    "merged_monthly.to_csv(\"outputs/lecture2_policy_spy_monthly.csv\", index=False)\n",
    "print(\"Saved:\", \"outputs/lecture2_policy_spy_monthly.csv\", \"| shape:\", merged_monthly.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16669b3",
   "metadata": {},
   "source": [
    "## 3.4. <a id='3.4'>References</a>\n",
    "\n",
    "- Pandas Series: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html\n",
    "- Pandas DataFrame: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\n",
    "- Pandas melt: https://pandas.pydata.org/docs/reference/api/pandas.melt.html\n",
    "- Pandas pivot_table: https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html\n",
    "- Pandas merge: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html\n",
    "- BCRP API help: https://estadisticas.bcrp.gob.pe/estadisticas/series/ayuda/api\n",
    "- yfinance: https://ranaroussi.github.io/yfinance/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ra_task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
