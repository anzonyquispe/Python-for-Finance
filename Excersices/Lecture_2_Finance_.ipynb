{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa57c87",
   "metadata": {},
   "source": [
    "# Lecture 2 Finance Practice\n",
    "## Data finance (Peru / US)\n",
    "- **BCRPData API (Peru)** (official): https://estadisticas.bcrp.gob.pe/estadisticas/series/ayuda/api\n",
    "- **Yahoo Finance** via `yfinance` (US market data): https://ranaroussi.github.io/yfinance/\n",
    "- **INEI open data (Peru)** (ENAPREF sample CSV): https://www.datosabiertos.gob.pe/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "24418d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fc3466c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = \"2022-01-01\"\n",
    "END = \"2025-12-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e5c6cb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_17484\\1773827851.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt_iso = pd.to_datetime(s, errors=\"coerce\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    date_raw  PENUSD_buy  PENUSD_sell       date\n",
       " 0  01.Feb.22    3.871333     3.877667 2022-02-01\n",
       " 1  02.Feb.22    3.852000     3.857000 2022-02-02\n",
       " 2  03.Feb.22    3.858500     3.860833 2022-02-03\n",
       " 3  04.Feb.22    3.863000     3.867833 2022-02-04\n",
       " 4  07.Feb.22    3.838500     3.845833 2022-02-07,\n",
       "       date_raw  PENUSD_buy  PENUSD_sell       date\n",
       " 579  24.Nov.25    3.382857     3.385286 2025-11-24\n",
       " 580  25.Nov.25    3.375143     3.377214 2025-11-25\n",
       " 581  26.Nov.25    3.365000     3.366857 2025-11-26\n",
       " 582  27.Nov.25    3.364143     3.366429 2025-11-27\n",
       " 583  28.Nov.25    3.360714     3.362500 2025-11-28,\n",
       " (584, 4))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- BCRP: daily PEN/USD buy & sell (official API) ---\n",
    "# Codes:\n",
    "# - PD04637PD: USD/PEN (buy)\n",
    "# - PD04638PD: USD/PEN (sell)\n",
    "\n",
    "import requests\n",
    "\n",
    "bcrp_url = f\"https://estadisticas.bcrp.gob.pe/estadisticas/series/api/PD04637PD-PD04638PD/json/{START}/{END}/esp\"\n",
    "try:\n",
    "    r = requests.get(bcrp_url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    bcrp_obj = r.json()\n",
    "except Exception as e:\n",
    "    bcrp_obj = {\"periods\": []}\n",
    "    print(\"BCRP request failed:\", type(e).__name__, str(e))\n",
    "\n",
    "periods = bcrp_obj.get(\"periods\", [])\n",
    "rows = []\n",
    "for p in periods:\n",
    "    name = p.get(\"name\")\n",
    "    vals = p.get(\"values\", [])\n",
    "    if isinstance(vals, str):\n",
    "        vals = [vals]\n",
    "    if name is None or not isinstance(vals, list) or len(vals) < 2:\n",
    "        continue\n",
    "    rows.append([name, vals[0], vals[1]])\n",
    "\n",
    "fx = pd.DataFrame(rows, columns=[\"date_raw\", \"PENUSD_buy\", \"PENUSD_sell\"])\n",
    "\n",
    "# Convert numeric\n",
    "if fx.shape[0] > 0:\n",
    "    fx[\"PENUSD_buy\"] = pd.to_numeric(fx[\"PENUSD_buy\"].replace({\"n.d.\": np.nan, \"nd\": np.nan, \"N.D.\": np.nan}), errors=\"coerce\")\n",
    "    fx[\"PENUSD_sell\"] = pd.to_numeric(fx[\"PENUSD_sell\"].replace({\"n.d.\": np.nan, \"nd\": np.nan, \"N.D.\": np.nan}), errors=\"coerce\")\n",
    "\n",
    "# Parse dates (BCRP labels can be ISO-like or ddMonYY or Mon.YYYY)\n",
    "if fx.shape[0] > 0:\n",
    "    s = fx[\"date_raw\"].astype(str).str.strip()\n",
    "\n",
    "    # ISO parse\n",
    "    dt_iso = pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "    # Monthly like Mar.2020\n",
    "    mask_monthly = s.str.match(r\"^[A-Za-zÁÉÍÓÚÑñ]{3}\\.[0-9]{4}$\", na=False)\n",
    "    mon_map = {\n",
    "        \"Ene\":\"Jan\",\"Feb\":\"Feb\",\"Mar\":\"Mar\",\"Abr\":\"Apr\",\"May\":\"May\",\"Jun\":\"Jun\",\n",
    "        \"Jul\":\"Jul\",\"Ago\":\"Aug\",\"Set\":\"Sep\",\"Sep\":\"Sep\",\"Oct\":\"Oct\",\"Nov\":\"Nov\",\"Dic\":\"Dec\"\n",
    "    }\n",
    "    mon_es = s.where(mask_monthly).str.slice(0,3)\n",
    "    year4 = s.where(mask_monthly).str.slice(4,8)\n",
    "    mon_en = mon_es.map(mon_map)\n",
    "    dt_monthly = pd.to_datetime(mon_en + \".\" + year4, format=\"%b.%Y\", errors=\"coerce\")\n",
    "\n",
    "    # Daily like 18Dic25\n",
    "    mask_daily = s.str.match(r\"^[0-9]{2}[A-Za-zÁÉÍÓÚÑñ]{3}[0-9]{2}$\", na=False)\n",
    "    day = s.where(mask_daily).str.slice(0,2)\n",
    "    mon_es2 = s.where(mask_daily).str.slice(2,5)\n",
    "    yy = pd.to_numeric(s.where(mask_daily).str.slice(5,7), errors=\"coerce\")\n",
    "    mon_en2 = mon_es2.map(mon_map)\n",
    "    year = np.where(yy <= 69, 2000 + yy, 1900 + yy)\n",
    "    year_s = pd.Series(year).astype(\"Int64\").astype(str)\n",
    "    dt_daily = pd.to_datetime(day + mon_en2 + year_s, format=\"%d%b%Y\", errors=\"coerce\")\n",
    "\n",
    "    dt = dt_iso.copy()\n",
    "    dt = dt.fillna(dt_monthly)\n",
    "    dt = dt.fillna(dt_daily)\n",
    "\n",
    "    fx[\"date\"] = dt\n",
    "    fx = fx.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "fx.head(), fx.tail(), fx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8bd6f1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        date ticker      close    volume\n",
       " 0 2022-01-03    EEM  44.624969  27572700\n",
       " 1 2022-01-04    EEM  44.470764  24579500\n",
       " 2 2022-01-05    EEM  43.745163  46425100\n",
       " 3 2022-01-06    EEM  43.944714  34288700\n",
       " 4 2022-01-07    EEM  44.343792  32640900,\n",
       " (4970, 4))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Yahoo Finance via yfinance: US tickers (real market data) ---\n",
    "tickers = [\"SPY\", \"QQQ\", \"TLT\", \"GLD\", \"EEM\"]\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception as e:\n",
    "    yf = None\n",
    "    print(\"Could not import yfinance:\", type(e).__name__, str(e))\n",
    "\n",
    "if yf is not None:\n",
    "    try:\n",
    "        data = yf.download(tickers, start=START, end=END, auto_adjust=True, progress=False)\n",
    "    except Exception as e:\n",
    "        data = pd.DataFrame()\n",
    "        print(\"yfinance download failed:\", type(e).__name__, str(e))\n",
    "else:\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "# Convert to long format: date, ticker, close, volume\n",
    "if isinstance(data, pd.DataFrame) and data.shape[0] > 0:\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        close = data[\"Close\"].copy()\n",
    "        vol = data[\"Volume\"].copy()\n",
    "    else:\n",
    "        close = data[[\"Close\"]].rename(columns={\"Close\": tickers[0]})\n",
    "        vol = data[[\"Volume\"]].rename(columns={\"Volume\": tickers[0]})\n",
    "\n",
    "    close.index.name = \"date\"\n",
    "    vol.index.name = \"date\"\n",
    "\n",
    "    us_close_long = close.reset_index().melt(id_vars=\"date\", var_name=\"ticker\", value_name=\"close\")\n",
    "    us_vol_long = vol.reset_index().melt(id_vars=\"date\", var_name=\"ticker\", value_name=\"volume\")\n",
    "    us_mkt = us_close_long.merge(us_vol_long, on=[\"date\",\"ticker\"], how=\"inner\").dropna(subset=[\"close\"])\n",
    "else:\n",
    "    us_mkt = pd.DataFrame(columns=[\"date\",\"ticker\",\"close\",\"volume\"])\n",
    "\n",
    "us_mkt.head(), us_mkt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1b460fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 286),\n",
       " ['ANIO', 'MES', 'CONGLOME', 'VIVIENDA', 'HOGAR'],\n",
       "    ANIO  MES  CONGLOME  VIVIENDA  HOGAR  UBIGEO  DOMINIO  ESTRATO  PERIODO  \\\n",
       " 0  2008    6         1        15     11   10101        4        4        3   \n",
       " 1  2008    6         1        46     11   10101        4        4        3   \n",
       " 2  2008    6         1        62     11   10101        4        4        3   \n",
       " 3  2008    6         1        88     11   10101        4        4        3   \n",
       " 4  2008    6         1        99     11   10101        4        4        3   \n",
       " \n",
       "    TIPO  RFINAL  REEMPLAZO  P101  P102  P102_A  P103  P104_1  P104_A1  \\\n",
       " 0     1       4          0   NaN   NaN     NaN   NaN     NaN      NaN   \n",
       " 1     1       1          0   1.0   2.0     NaN  40.0     2.0      NaN   \n",
       " 2     1       1          0   1.0   2.0     NaN  50.0     2.0      NaN   \n",
       " 3     1       1          0   1.0   2.0     NaN  30.0     2.0      NaN   \n",
       " 4     1       1          0   1.0   2.0     NaN  20.0     2.0      NaN   \n",
       " \n",
       "    P104_B1  P104_2  P104_A2  P104_B2  P104_3  P104_A3  P104_B3  P105_1  \\\n",
       " 0      NaN     NaN      NaN      NaN     NaN      NaN      NaN     NaN   \n",
       " 1      NaN     2.0      NaN      NaN     2.0      NaN      NaN     1.0   \n",
       " 2      NaN     2.0      NaN      NaN     2.0      NaN      NaN     1.0   \n",
       " 3      NaN     2.0      NaN      NaN     2.0      NaN      NaN     1.0   \n",
       " 4      NaN     2.0      NaN      NaN     2.0      NaN      NaN     0.0   \n",
       " \n",
       "    P105_2  P105_3  P105_4  P105_5  P105_6  P105_7O  P106  P107_1  P107_2  \\\n",
       " 0     NaN     NaN     NaN     NaN     NaN      NaN   NaN     NaN     NaN   \n",
       " 1     0.0     0.0     0.0     0.0     0.0      0.0   1.0     1.0     0.0   \n",
       " 2     0.0     0.0     0.0     0.0     0.0      0.0   1.0     1.0     0.0   \n",
       " 3     0.0     0.0     0.0     0.0     0.0      0.0   1.0     1.0     0.0   \n",
       " 4     0.0     0.0     0.0     0.0     0.0      7.0   6.0     0.0     0.0   \n",
       " \n",
       "    P107_3  P107_4  P107_5  P107_6  P108_1  P108_2  P108_3  P108_4  P108_5  \\\n",
       " 0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       " 1     0.0     0.0     0.0     0.0     0.0     2.0     0.0     0.0     0.0   \n",
       " 2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     5.0   \n",
       " 3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 4     0.0     4.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " \n",
       "    P108_6  P108_7  P108_8  P108_9  P109_1  P109_2  P109_3  P109_4  P109_5  \\\n",
       " 0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       " 1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 2     6.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 3     6.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 4     6.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " \n",
       "    P109_6  P110_1  P110_A1  P110_B1  P110_C1  P110_2  P110_A2  ...  P117_A  \\\n",
       " 0     NaN     NaN      NaN      NaN      NaN     NaN      NaN  ...     NaN   \n",
       " 1     6.0     1.0     11.0      1.0      NaN     2.0      6.5  ...     NaN   \n",
       " 2     6.0     1.0     25.0      1.0      NaN     2.0      4.0  ...     NaN   \n",
       " 3     6.0     1.0      9.0      1.0      NaN     2.0      7.0  ...     NaN   \n",
       " 4     6.0     1.0      6.0      1.0      NaN     0.0      NaN  ...     NaN   \n",
       " \n",
       "    P118_1  P118_A1  P118_B1  P118_2  P118_A2  P118_B2  P119  P119_A  P120  \\\n",
       " 0     NaN      NaN      NaN     NaN      NaN      NaN   NaN     NaN   NaN   \n",
       " 1     NaN      NaN      NaN     NaN      NaN      NaN   4.0     0.0  64.0   \n",
       " 2     NaN      NaN      NaN     NaN      NaN      NaN  10.0     0.0  96.0   \n",
       " 3     NaN      NaN      NaN     NaN      NaN      NaN   8.0     0.0  50.0   \n",
       " 4     NaN      NaN      NaN     NaN      NaN      NaN   8.0     0.0  28.0   \n",
       " \n",
       "    I102_A   I103  I104_B1  I104_B2  I104_B3  I110_A1  I110_A2  I110_A3  \\\n",
       " 0     NaN    NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       " 1     NaN  484.0      NaN      NaN      NaN   133.22    78.72      NaN   \n",
       " 2     NaN  611.0      NaN      NaN      NaN   305.68    48.91      NaN   \n",
       " 3     NaN  366.0      NaN      NaN      NaN   110.04    85.59      NaN   \n",
       " 4     NaN  244.0      NaN      NaN      NaN    73.36      NaN      NaN   \n",
       " \n",
       "    I110_A4  I110_A5  I110_A6  I110_A7  I110_A8  I110_A9  I111_C1  I111_C2  \\\n",
       " 0      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       " 1      NaN      NaN      NaN      NaN      NaN      NaN   399.65      NaN   \n",
       " 2      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       " 3      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       " 4      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       " \n",
       "    I111_C3  I111_C4  I111_C5  I111_C6  I111_C7  I111_C8  I112_A1  I112_A2  \\\n",
       " 0      NaN      NaN      NaN      NaN      NaN      NaN        0      NaN   \n",
       " 1      NaN      NaN      NaN      NaN      NaN      NaN        0      NaN   \n",
       " 2      NaN      NaN   110.04   391.27      NaN      NaN        0      NaN   \n",
       " 3      NaN      NaN      NaN   342.36      NaN      NaN        0      NaN   \n",
       " 4      NaN    46.46      NaN     0.00      NaN      NaN        0      NaN   \n",
       " \n",
       "    I112_A3  I112_A4  I112_A5  I113_A1  I113_A2  I113_A3  I113_A4  I113_A5  \\\n",
       " 0        0      0.0        0      NaN      NaN      NaN      NaN      NaN   \n",
       " 1        0      0.0        0      NaN      NaN      NaN      NaN      NaN   \n",
       " 2        0      0.0        0      NaN      NaN      NaN      NaN      NaN   \n",
       " 3        0      0.0        0      NaN      NaN      NaN      NaN      NaN   \n",
       " 4        0      0.0        0      NaN      NaN      NaN      NaN      NaN   \n",
       " \n",
       "    I113_A6  I113_A7  I113_A8  I113_A9  I114_A  I116_A1  I116_A2  I116_A3  \\\n",
       " 0      NaN      NaN      NaN      NaN       0      0.0      0.0      0.0   \n",
       " 1      NaN      NaN      NaN      NaN       0      0.0      0.0      0.0   \n",
       " 2      NaN      NaN      NaN      NaN       0      0.0      0.0      0.0   \n",
       " 3      NaN      NaN      NaN      NaN       0      0.0      0.0      0.0   \n",
       " 4      NaN      NaN      NaN      NaN       0      0.0      0.0      0.0   \n",
       " \n",
       "    I116_A4  I116_A5  I116_A6  I116_A7  I116_A8  I116_A9  I117_A  I118_B1  \\\n",
       " 0        0        0        0        0      0.0        0       0        0   \n",
       " 1        0        0        0        0      0.0        0       0        0   \n",
       " 2        0        0        0        0      0.0        0       0        0   \n",
       " 3        0        0        0        0      0.0        0       0        0   \n",
       " 4        0        0        0        0      0.0        0       0        0   \n",
       " \n",
       "    I118_B2  FACTOR_EXPANSION_2008  \n",
       " 0        0                   9.71  \n",
       " 1        0                   9.71  \n",
       " 2        0                   9.71  \n",
       " 3        0                   9.71  \n",
       " 4        0                   9.71  \n",
       " \n",
       " [5 rows x 286 columns])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INEI: read correctly using '|' as delimiter (pipe-separated file)\n",
    "inei_url = \"https://www.datosabiertos.gob.pe/sites/default/files/Cap.%20100_Vivienda-Hogar_Muestra.csv\"\n",
    "\n",
    "inei = pd.read_csv(\n",
    "    inei_url,\n",
    "    sep=\"|\",            # <-- key fix: the file is pipe-separated\n",
    "    nrows=3000,\n",
    "    low_memory=False,\n",
    "    encoding_errors=\"replace\"\n",
    ")\n",
    "\n",
    "inei.shape, list(inei.columns)[:5], inei.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcd74d0",
   "metadata": {},
   "source": [
    "## 3.2 Pandas Series\n",
    "\n",
    "### 3.2.1 From lists to Series exercise\n",
    "\n",
    "Using `fx`:\n",
    "\n",
    "1. Create `PENUSD_mid = (PENUSD_buy + PENUSD_sell) / 2`.\n",
    "2. Take the **last 15 mid values** as a Python list.\n",
    "3. Create a `pd.Series` from that list.\n",
    "4. Name it `PENUSD_mid_last15`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a7684c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     3.370750\n",
       "1     3.359500\n",
       "2     3.368214\n",
       "3     3.364071\n",
       "4     3.370393\n",
       "5     3.362321\n",
       "6     3.367964\n",
       "7     3.376429\n",
       "8     3.379857\n",
       "9     3.388214\n",
       "10    3.384071\n",
       "11    3.376179\n",
       "12    3.365929\n",
       "13    3.365286\n",
       "14    3.361607\n",
       "Name: PENUSD_mid_last15, dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx[\"PENUSD_mid\"] = (fx[\"PENUSD_buy\"] + fx[\"PENUSD_sell\"]) / 2\n",
    "\n",
    "PENUSD_mid_last15 = pd.Series(\n",
    "    fx[\"PENUSD_mid\"].dropna().tail(15).tolist(),\n",
    "    name=\"PENUSD_mid_last15\"\n",
    ")\n",
    "\n",
    "PENUSD_mid_last15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be48bf22",
   "metadata": {},
   "source": [
    "### 3.2.2 From NumPy array to Series \n",
    "\n",
    "Using `us_mkt`:\n",
    "\n",
    "1. Filter to `ticker == \"SPY\"`.\n",
    "2. Take `close` as a NumPy array.\n",
    "3. Create a Series indexed by `date` named `SPY_close_series`.\n",
    "4. Compute the mean/min/max with Series methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7332e45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(date\n",
       " 2022-01-03    451.875183\n",
       " 2022-01-04    451.723785\n",
       " 2022-01-05    443.049744\n",
       " 2022-01-06    442.633545\n",
       " 2022-01-07    440.883545\n",
       " Name: SPY_close_series, dtype: float64,\n",
       " {'mean': np.float64(485.6085832008653),\n",
       "  'min': 341.1820983886719,\n",
       "  'max': 687.1395263671875})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 1 Filter rows where ticker == \"SPY\" and keep only the columns we need.\n",
    "spy_df = us_mkt.loc[us_mkt[\"ticker\"].eq(\"SPY\"), [\"date\", \"close\"]].dropna(subset=[\"date\", \"close\"])\n",
    "\n",
    "# 2 Convert the 'close' column to a NumPy array (this is what the exercise asks for)\n",
    "spy_close_np = spy_df[\"close\"].to_numpy()\n",
    "\n",
    "# 3 Convert the 'date' column to datetime and use it as the Series index.\n",
    "#    Name the Series exactly as required: \"SPY_close_series\".\n",
    "SPY_close_series = pd.Series(\n",
    "    spy_close_np,\n",
    "    index=pd.to_datetime(spy_df[\"date\"]),\n",
    "    name=\"SPY_close_series\"\n",
    ")\n",
    "\n",
    "# 4 Compute summary statistics using pandas Series methods.\n",
    "summary_stats = {\n",
    "    \"mean\": SPY_close_series.mean(),\n",
    "    \"min\": SPY_close_series.min(),\n",
    "    \"max\": SPY_close_series.max(),\n",
    "}\n",
    "\n",
    "SPY_close_series.head(), summary_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114e9b8",
   "metadata": {},
   "source": [
    "### 3.2.3 From Dictionary to Series \n",
    "\n",
    "Using `us_mkt`:\n",
    "\n",
    "1. Compute the **last available close** for each ticker in `tickers`.\n",
    "2. Store it in a dict `{ticker: last_close}`.\n",
    "3. Convert to a Series and sort descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e2d0bb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SPY    669.421936\n",
       "QQQ    600.409973\n",
       "GLD    399.290009\n",
       "TLT     87.459633\n",
       "EEM     52.599998\n",
       "dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure we have only the relevant columns, remove missing closes, and sort by date\n",
    "tmp = us_mkt.loc[:, [\"date\", \"ticker\", \"close\"]].dropna(subset=[\"date\", \"close\"]).copy()\n",
    "tmp[\"date\"] = pd.to_datetime(tmp[\"date\"])\n",
    "tmp = tmp.sort_values([\"ticker\", \"date\"])\n",
    "\n",
    "# Step 1 + 2: build dict {ticker: last_close}\n",
    "last_close_by_ticker = (\n",
    "    tmp.groupby(\"ticker\")[\"close\"]\n",
    "       .last()                 # last row per ticker after sorting by date\n",
    "       .to_dict()              # convert to dict\n",
    ")\n",
    "\n",
    "# Step 3: convert dict -> Series and sort descending\n",
    "last_close_series = (\n",
    "    pd.Series(last_close_by_ticker, dtype=float)\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "last_close_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d96f725",
   "metadata": {},
   "source": [
    "### 3.2.4 Series vs NumPy \n",
    "\n",
    "Goal: show why pandas alignment matters.\n",
    "\n",
    "1. Create two Series indexed by date:\n",
    "   - FX mid-rate from `fx`\n",
    "   - SPY close from `us_mkt`\n",
    "2. Combine them into a DataFrame (pandas aligns on dates).\n",
    "3. Separately, build two NumPy arrays by truncating to the same length.\n",
    "4. In markdown: explain why alignment is safer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da53183f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(              FX_mid   SPY_close\n",
       " date                            \n",
       " 2022-02-01  3.874500  428.454163\n",
       " 2022-02-02  3.854500  432.616180\n",
       " 2022-02-03  3.859667  422.447632\n",
       " 2022-02-04  3.865417  424.434082\n",
       " 2022-02-07  3.842167  423.071960,\n",
       " (584,),\n",
       " (584,))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- FX mid-rate Series (indexed by date) ---\n",
    "fx_mid = (\n",
    "    fx.loc[:, [\"date\", \"PENUSD_buy\", \"PENUSD_sell\"]]\n",
    "      .dropna(subset=[\"date\", \"PENUSD_buy\", \"PENUSD_sell\"])\n",
    "      .assign(PENUSD_mid=lambda d: (d[\"PENUSD_buy\"] + d[\"PENUSD_sell\"]) / 2)\n",
    "      .set_index(\"date\")[\"PENUSD_mid\"]\n",
    "      .sort_index()\n",
    ")\n",
    "\n",
    "# --- SPY close Series (indexed by date) ---\n",
    "spy_close = (\n",
    "    us_mkt.loc[us_mkt[\"ticker\"].eq(\"SPY\"), [\"date\", \"close\"]]\n",
    "         .dropna(subset=[\"date\", \"close\"])\n",
    ")\n",
    "\n",
    "spy_close[\"date\"] = pd.to_datetime(spy_close[\"date\"])\n",
    "spy_close_series = spy_close.set_index(\"date\")[\"close\"].sort_index()\n",
    "\n",
    "# --- pandas alignment: align on the date index automatically ---\n",
    "aligned_df = pd.DataFrame({\"FX_mid\": fx_mid, \"SPY_close\": spy_close_series}).dropna()\n",
    "\n",
    "# --- NumPy approach: truncate arrays to the same length (no date alignment) ---\n",
    "# NOTE: This can compare different dates if the series have missing days or different calendars.\n",
    "n = min(len(fx_mid), len(spy_close_series))\n",
    "fx_np = fx_mid.to_numpy()[:n]\n",
    "spy_np = spy_close_series.to_numpy()[:n]\n",
    "\n",
    "aligned_df.head(), fx_np.shape, spy_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12676d3c",
   "metadata": {},
   "source": [
    "### 3.2.5 Indexing \n",
    "\n",
    "1. From `last_close_series`, use `.iloc` to select the top 3 tickers.\n",
    "2. Use `.loc` to select the SPY value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a7da3c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SPY    669.421936\n",
       " QQQ    600.409973\n",
       " GLD    399.290009\n",
       " dtype: float64,\n",
       " np.float64(669.4219360351562))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Top 3 by position (first three entries)\n",
    "top3 = last_close_series.iloc[:3]\n",
    "\n",
    "# 2) SPY value by label (safe lookup: returns NaN if SPY is missing)\n",
    "spy_value = last_close_series.get(\"SPY\", np.nan)\n",
    "\n",
    "top3, spy_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264d8fd",
   "metadata": {},
   "source": [
    "## 3.3 DataFrame\n",
    "\n",
    "### 3.3.1 DataFrame Generation \n",
    "\n",
    "1. Build a DataFrame with daily FX mid-rate and SPY close aligned by date.\n",
    "2. Create columns:\n",
    "   - `FX_ret` and `SPY_ret` using `pct_change()`\n",
    "   - `FX_abs_change` = absolute day-to-day change in FX mid\n",
    "3. Keep columns in this order:\n",
    "   `[\"PENUSD_mid\",\"SPY_close\",\"FX_ret\",\"SPY_ret\",\"FX_abs_change\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7c6f118a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PENUSD_mid</th>\n",
       "      <th>SPY_close</th>\n",
       "      <th>FX_ret</th>\n",
       "      <th>SPY_ret</th>\n",
       "      <th>FX_abs_change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-02-01</th>\n",
       "      <td>3.874500</td>\n",
       "      <td>428.454163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-02</th>\n",
       "      <td>3.854500</td>\n",
       "      <td>432.616180</td>\n",
       "      <td>-0.005162</td>\n",
       "      <td>0.009714</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-03</th>\n",
       "      <td>3.859667</td>\n",
       "      <td>422.447632</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>-0.023505</td>\n",
       "      <td>0.005167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-04</th>\n",
       "      <td>3.865417</td>\n",
       "      <td>424.434082</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>0.005750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-07</th>\n",
       "      <td>3.842167</td>\n",
       "      <td>423.071960</td>\n",
       "      <td>-0.006015</td>\n",
       "      <td>-0.003209</td>\n",
       "      <td>0.023250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            PENUSD_mid   SPY_close    FX_ret   SPY_ret  FX_abs_change\n",
       "date                                                                 \n",
       "2022-02-01    3.874500  428.454163       NaN       NaN            NaN\n",
       "2022-02-02    3.854500  432.616180 -0.005162  0.009714       0.020000\n",
       "2022-02-03    3.859667  422.447632  0.001340 -0.023505       0.005167\n",
       "2022-02-04    3.865417  424.434082  0.001490  0.004702       0.005750\n",
       "2022-02-07    3.842167  423.071960 -0.006015 -0.003209       0.023250"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 1: Create the FX mid-rate series indexed by date ---\n",
    "# We compute the mid-rate as the average of buy and sell, then keep it as a clean, date-indexed Series.\n",
    "fx_mid = (\n",
    "    fx.loc[:, [\"date\", \"PENUSD_buy\", \"PENUSD_sell\"]]\n",
    "      .dropna(subset=[\"date\", \"PENUSD_buy\", \"PENUSD_sell\"])                 # remove rows with missing pieces\n",
    "      .assign(PENUSD_mid=lambda d: (d[\"PENUSD_buy\"] + d[\"PENUSD_sell\"]) / 2) # mid = (buy + sell)/2\n",
    "      .set_index(\"date\")[\"PENUSD_mid\"]                                      # turn into a Series indexed by date\n",
    "      .sort_index()                                                         # sort so time-based operations make sense\n",
    ")\n",
    "\n",
    "# --- Step 2: Create the SPY close series indexed by date ---\n",
    "# We filter us_mkt to SPY only, then build a clean Series with date as the index.\n",
    "spy_close_series = (\n",
    "    us_mkt.loc[us_mkt[\"ticker\"].eq(\"SPY\"), [\"date\", \"close\"]]   # keep only SPY rows and the needed columns\n",
    "         .dropna(subset=[\"date\", \"close\"])                      # drop missing dates/closes\n",
    ")\n",
    "spy_close_series[\"date\"] = pd.to_datetime(spy_close_series[\"date\"])          # ensure dates are true datetimes\n",
    "spy_close_series = (\n",
    "    spy_close_series.set_index(\"date\")[\"close\"]                # make it a Series indexed by date\n",
    "                  .sort_index()                                # sort by date\n",
    ")\n",
    "\n",
    "# --- Step 3: Align FX and SPY by date in a single DataFrame ---\n",
    "# Pandas aligns automatically by the index (date). dropna() keeps only dates present in BOTH series.\n",
    "df = pd.DataFrame({\n",
    "    \"PENUSD_mid\": fx_mid,\n",
    "    \"SPY_close\": spy_close_series\n",
    "}).dropna()\n",
    "\n",
    "# --- Step 4: Create return and change features ---\n",
    "# pct_change() computes (today / yesterday - 1), i.e., simple daily returns.\n",
    "df_features = df.copy()\n",
    "df_features[\"FX_ret\"] = df_features[\"PENUSD_mid\"].pct_change()               # daily FX return\n",
    "df_features[\"SPY_ret\"] = df_features[\"SPY_close\"].pct_change()               # daily SPY return\n",
    "\n",
    "# Absolute day-to-day change in FX mid-rate (in PEN per USD).\n",
    "df_features[\"FX_abs_change\"] = df_features[\"PENUSD_mid\"].diff().abs()        # |mid_t - mid_{t-1}|\n",
    "\n",
    "# --- Step 5: Keep columns in the required order ---\n",
    "df_features = df_features[[\"PENUSD_mid\", \"SPY_close\", \"FX_ret\", \"SPY_ret\", \"FX_abs_change\"]]\n",
    "\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e6398",
   "metadata": {},
   "source": [
    "### 3.3.2 Indexing \n",
    "\n",
    "1. Use `.iloc` to select first 10 rows of returns only.\n",
    "2. Use `.loc` to select a date range in 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ca96ade7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(              FX_ret   SPY_ret\n",
       " date                          \n",
       " 2022-02-01       NaN       NaN\n",
       " 2022-02-02 -0.005162  0.009714\n",
       " 2022-02-03  0.001340 -0.023505\n",
       " 2022-02-04  0.001490  0.004702\n",
       " 2022-02-07 -0.006015 -0.003209\n",
       " 2022-02-08 -0.000564  0.008228\n",
       " 2022-02-09 -0.004970  0.014636\n",
       " 2022-02-10 -0.017252 -0.017965\n",
       " 2022-02-11 -0.000222 -0.019719\n",
       " 2022-02-14  0.008835 -0.003269,\n",
       "             PENUSD_mid   SPY_close    FX_ret   SPY_ret  FX_abs_change\n",
       " date                                                                 \n",
       " 2024-02-01    3.805250  477.398163  0.018104  0.076213       0.067667\n",
       " 2024-02-02    3.832000  482.423981  0.007030  0.010528       0.026750\n",
       " 2024-02-05    3.855667  480.667389  0.006176 -0.003641       0.023667\n",
       " 2024-02-06    3.856250  482.062866  0.000151  0.002903       0.000583\n",
       " 2024-02-07    3.864667  486.083527  0.002183  0.008341       0.008417)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) First 10 rows of returns only (position-based)\n",
    "first10_rets = df_features.loc[:, [\"FX_ret\", \"SPY_ret\"]].iloc[:10]\n",
    "\n",
    "# 2) Date range selection in 2024 (label-based, using the datetime index)\n",
    "range_2024 = df_features.loc[\"2024-01-01\":\"2024-12-31\"]\n",
    "\n",
    "first10_rets, range_2024.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc6662",
   "metadata": {},
   "source": [
    "### 3.3.3 General Methods \n",
    "1. Use `.describe()` for return columns.\n",
    "2. Find the 5 highest SPY daily returns and the dates.\n",
    "3. Create `FX_direction` = \"up\" if `FX_ret > 0` else \"down\".\n",
    "4. Count how many \"up\" days per calendar year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "21006629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2022    66\n",
       "2023    71\n",
       "2024    72\n",
       "2025    57\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure FX_direction exists (from the previous step)\n",
    "df_features[\"FX_direction\"] = np.where(df_features[\"FX_ret\"] > 0, \"up\", \"down\")\n",
    "\n",
    "# Extract years only for \"up\" days (this returns an array of years, one per \"up\" day)\n",
    "up_years = df_features.index[df_features[\"FX_direction\"].eq(\"up\")].year\n",
    "\n",
    "# Count occurrences of each year (no groupby used)\n",
    "up_days_per_year = pd.Series(up_years).value_counts().sort_index()\n",
    "\n",
    "up_days_per_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d1bea",
   "metadata": {},
   "source": [
    "### 3.3.4 Importing Data \n",
    "\n",
    "Using INEI sample `inei`:\n",
    "\n",
    "1. Display shape and first 5 columns.\n",
    "2. Pick 2 columns and rename to snake_case.\n",
    "3. Keep only those 2 columns in a new DataFrame `inei_small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fbd09724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 286),\n",
       " ['ANIO', 'MES', 'CONGLOME', 'VIVIENDA', 'HOGAR'],\n",
       "    year  month\n",
       " 0  2008      6\n",
       " 1  2008      6\n",
       " 2  2008      6\n",
       " 3  2008      6\n",
       " 4  2008      6)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.3.4 Importing Data (solution after reading with sep=\"|\")\n",
    "#\n",
    "# 1) Show shape and first 5 columns\n",
    "inei_shape = inei.shape\n",
    "inei_first5_cols = list(inei.columns)[:5]\n",
    "\n",
    "# 2) Pick two columns and rename to snake_case\n",
    "inei_small = (\n",
    "    inei.loc[:, [\"ANIO\", \"MES\"]]\n",
    "        .rename(columns={\"ANIO\": \"year\", \"MES\": \"month\"})\n",
    ")\n",
    "\n",
    "inei_shape, inei_first5_cols, inei_small.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ed20c",
   "metadata": {},
   "source": [
    "### 3.3.5 Filtering Data \n",
    "\n",
    "Using `df_features`:\n",
    "\n",
    "1. Filter days where `SPY_ret < -0.02` (large negative days).\n",
    "2. Filter days where `FX_abs_change` is in the top 1%.\n",
    "3. Compare counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cfc87e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 6)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Days where SPY had a large negative return (< -2%)\n",
    "neg_spy_days = df_features.loc[df_features[\"SPY_ret\"] < -0.02]\n",
    "\n",
    "# 2) Days where FX_abs_change is in the top 1% (>= 99th percentile)\n",
    "fx_threshold = df_features[\"FX_abs_change\"].quantile(0.99)\n",
    "fx_jump_days = df_features.loc[df_features[\"FX_abs_change\"] >= fx_threshold]\n",
    "\n",
    "len(neg_spy_days), len(fx_jump_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eeddea",
   "metadata": {},
   "source": [
    "### 3.3.6 Dealing with Nulls \n",
    "Using `us_mkt`:\n",
    "\n",
    "1. Copy `us_mkt` to `us_mkt_nan`.\n",
    "2. Set 1% of `close` to NaN (fixed random seed).\n",
    "3. Create:\n",
    "   - `us_drop`: drop NaNs\n",
    "   - `us_fill`: fill NaNs with ticker-specific median close\n",
    "4. Compare shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "93c7212c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4970, 4), (4970, 4), (4920, 4), (4970, 4))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 Copy\n",
    "us_mkt_nan = us_mkt.copy()\n",
    "\n",
    "# 2 Set ~1% of close to NaN \n",
    "idx = us_mkt_nan.sample(frac=0.01, random_state=42).index\n",
    "us_mkt_nan.loc[idx, \"close\"] = np.nan\n",
    "\n",
    "# 3 Drop NaNs\n",
    "us_drop = us_mkt_nan.dropna(subset=[\"close\"]).copy()\n",
    "\n",
    "# Fill NaNs with ticker-specific median (NO groupby)\n",
    "tickers_unique = us_mkt_nan[\"ticker\"].dropna().unique()\n",
    "\n",
    "median_by_ticker = {\n",
    "    t: us_mkt_nan.loc[us_mkt_nan[\"ticker\"].eq(t), \"close\"].median()\n",
    "    for t in tickers_unique\n",
    "}\n",
    "\n",
    "us_fill = us_mkt_nan.copy()\n",
    "us_fill[\"close\"] = us_fill[\"close\"].fillna(us_fill[\"ticker\"].map(median_by_ticker))\n",
    "\n",
    "# 4 Compare shapes\n",
    "us_mkt.shape, us_mkt_nan.shape, us_drop.shape, us_fill.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a42c6",
   "metadata": {},
   "source": [
    "### 3.3.7 Duplicates \n",
    "\n",
    "1. Create `dup_df` by stacking the last 5 rows of `us_mkt` twice.\n",
    "2. Detect duplicates using `.duplicated()`.\n",
    "3. Remove them using `.drop_duplicates()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f8e9a103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 4), (5, 4))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1Stack the last 5 rows twice\n",
    "last5 = us_mkt.tail(5)\n",
    "dup_df = pd.concat([last5, last5], axis=0, ignore_index=True)\n",
    "\n",
    "# 2 Detect duplicates (True means \"this row is a duplicate of a previous row\")\n",
    "dup_mask = dup_df.duplicated()\n",
    "\n",
    "# 3 Remove duplicates (keep the first occurrence by default)\n",
    "dedup_df = dup_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "dup_df.shape, dedup_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a6365",
   "metadata": {},
   "source": [
    "### 3.3.8 Groupby \n",
    "\n",
    "\n",
    "Using `us_mkt`:\n",
    "\n",
    "1. Group by `ticker` and compute:\n",
    "   - mean close\n",
    "   - median close\n",
    "   - max volume\n",
    "2. Rename columns clearly.\n",
    "3. Sort by mean close descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "942f3fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_close</th>\n",
       "      <th>median_close</th>\n",
       "      <th>max_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>485.608583</td>\n",
       "      <td>460.739990</td>\n",
       "      <td>256611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QQQ</th>\n",
       "      <td>411.806857</td>\n",
       "      <td>400.730164</td>\n",
       "      <td>198685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLD</th>\n",
       "      <td>220.130422</td>\n",
       "      <td>187.864998</td>\n",
       "      <td>62025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TLT</th>\n",
       "      <td>91.395622</td>\n",
       "      <td>88.549709</td>\n",
       "      <td>131353500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EEM</th>\n",
       "      <td>40.462350</td>\n",
       "      <td>39.107407</td>\n",
       "      <td>134225700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean_close  median_close  max_volume\n",
       "ticker                                      \n",
       "SPY     485.608583    460.739990   256611400\n",
       "QQQ     411.806857    400.730164   198685800\n",
       "GLD     220.130422    187.864998    62025000\n",
       "TLT      91.395622     88.549709   131353500\n",
       "EEM      40.462350     39.107407   134225700"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_summary = (\n",
    "    us_mkt\n",
    "    .dropna(subset=[\"ticker\", \"close\"])              # ensure close exists for the stats\n",
    "    .groupby(\"ticker\")\n",
    "    .agg(\n",
    "        mean_close=(\"close\", \"mean\"),                # average close per ticker\n",
    "        median_close=(\"close\", \"median\"),            # median close per ticker\n",
    "        max_volume=(\"volume\", \"max\")                 # max volume per ticker (NaN-safe)\n",
    "    )\n",
    "    .sort_values(\"mean_close\", ascending=False)      # highest mean close first\n",
    ")\n",
    "\n",
    "ticker_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2136d1b",
   "metadata": {},
   "source": [
    "### 3.3.9 Reshape \n",
    "\n",
    "1. Create a 1-row wide DataFrame with last closes per ticker.\n",
    "2. Convert it to long format with `melt()` into columns: `ticker`, `last_close`.\n",
    "3. Pivot `us_mkt` into a wide table: index=`date`, columns=`ticker`, values=`close` (keep first 50 dates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d5c63a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ticker            EEM         GLD         QQQ         SPY        TLT\n",
       " last_close  52.599998  399.290009  600.409973  669.421936  87.459633,\n",
       "   ticker  last_close\n",
       " 0    EEM   52.599998\n",
       " 1    GLD  399.290009\n",
       " 2    QQQ  600.409973\n",
       " 3    SPY  669.421936\n",
       " 4    TLT   87.459633,\n",
       " ticker            EEM         GLD         QQQ         SPY         TLT\n",
       " date                                                                 \n",
       " 2022-01-03  44.624969  168.330002  392.184082  451.875183  125.295334\n",
       " 2022-01-04  44.470764  169.570007  387.097321  451.723785  124.774353\n",
       " 2022-01-05  43.745163  169.059998  375.205231  443.049744  124.097099\n",
       " 2022-01-06  43.944714  166.990005  374.941620  442.633545  124.418365\n",
       " 2022-01-07  44.343792  167.750000  370.879974  440.883545  123.524025)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1  1-row wide DataFrame with last closes per ticker ---\n",
    "# We sort by ticker and date so \"last\" means the most recent observation for each ticker.\n",
    "tmp = us_mkt.loc[:, [\"date\", \"ticker\", \"close\"]].dropna(subset=[\"date\", \"ticker\", \"close\"]).copy()\n",
    "tmp[\"date\"] = pd.to_datetime(tmp[\"date\"])\n",
    "tmp = tmp.sort_values([\"ticker\", \"date\"])\n",
    "\n",
    "# Get last close per ticker (Series indexed by ticker), then convert to a 1-row DataFrame\n",
    "last_close_series = tmp.groupby(\"ticker\")[\"close\"].last()\n",
    "wide_last = last_close_series.to_frame().T\n",
    "wide_last.index = [\"last_close\"]  # optional: label the single row\n",
    "\n",
    "# 2 Convert wide -> long using melt() ---\n",
    "# Reset index so melt has a normal DataFrame to work with.\n",
    "long_last = (\n",
    "    wide_last.reset_index(drop=True)\n",
    "            .melt(var_name=\"ticker\", value_name=\"last_close\")\n",
    ")\n",
    "\n",
    "# 3 Pivot long us_mkt into a wide close table (date x ticker) ---\n",
    "# Pivot creates a table where each ticker is a column and each row is a date.\n",
    "wide_close = (\n",
    "    tmp.pivot(index=\"date\", columns=\"ticker\", values=\"close\")\n",
    "       .sort_index()\n",
    "       .head(50)\n",
    ")\n",
    "\n",
    "wide_last, long_last.head(), wide_close.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e3991b",
   "metadata": {},
   "source": [
    "### 3.3.10 Merge (Assignment)\n",
    "\n",
    "Goal: merge Peru macro data (BCRP) with US market data (Yahoo) at monthly frequency.\n",
    "\n",
    "1. Fetch BCRP monthly policy rate: code `PD12301MD`.\n",
    "2. Build a monthly SPY average close from `us_mkt`.\n",
    "3. Merge the two tables.\n",
    "4. Save to `outputs/lecture2_policy_spy_monthly.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d7adeb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_17484\\4062083830.py:26: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt_iso = pd.to_datetime(s, errors=\"coerce\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    date_raw  policy_rate       date\n",
       " 0  01.Feb.22          3.0 2022-02-01\n",
       " 1  02.Feb.22          3.0 2022-02-02\n",
       " 2  03.Feb.22          3.0 2022-02-03\n",
       " 3  04.Feb.22          3.0 2022-02-04\n",
       " 4  07.Feb.22          3.0 2022-02-07,\n",
       "       date_raw  policy_rate       date\n",
       " 589  24.Nov.25         4.25 2025-11-24\n",
       " 590  25.Nov.25         4.25 2025-11-25\n",
       " 591  26.Nov.25         4.25 2025-11-26\n",
       " 592  27.Nov.25         4.25 2025-11-27\n",
       " 593  28.Nov.25         4.25 2025-11-28)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch BCRP monthly policy rate (official API)\n",
    "policy_url = f\"https://estadisticas.bcrp.gob.pe/estadisticas/series/api/PD12301MD/json/{START}/{END}/esp\"\n",
    "try:\n",
    "    r = requests.get(policy_url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    pol_obj = r.json()\n",
    "except Exception as e:\n",
    "    pol_obj = {\"periods\": []}\n",
    "    print(\"BCRP policy request failed:\", type(e).__name__, str(e))\n",
    "\n",
    "rows = []\n",
    "for p in pol_obj.get(\"periods\", []):\n",
    "    name = p.get(\"name\")\n",
    "    vals = p.get(\"values\", [])\n",
    "    if isinstance(vals, str):\n",
    "        vals = [vals]\n",
    "    if name is None or not isinstance(vals, list) or len(vals) < 1:\n",
    "        continue\n",
    "    rows.append([name, vals[0]])\n",
    "\n",
    "policy = pd.DataFrame(rows, columns=[\"date_raw\", \"policy_rate\"])\n",
    "if policy.shape[0] > 0:\n",
    "    policy[\"policy_rate\"] = pd.to_numeric(policy[\"policy_rate\"].replace({\"n.d.\": np.nan, \"nd\": np.nan, \"N.D.\": np.nan}), errors=\"coerce\")\n",
    "\n",
    "    s = policy[\"date_raw\"].astype(str).str.strip()\n",
    "    dt_iso = pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "    mask_monthly = s.str.match(r\"^[A-Za-zÁÉÍÓÚÑñ]{3}\\.[0-9]{4}$\", na=False)\n",
    "    mon_map = {\n",
    "        \"Ene\":\"Jan\",\"Feb\":\"Feb\",\"Mar\":\"Mar\",\"Abr\":\"Apr\",\"May\":\"May\",\"Jun\":\"Jun\",\n",
    "        \"Jul\":\"Jul\",\"Ago\":\"Aug\",\"Set\":\"Sep\",\"Sep\":\"Sep\",\"Oct\":\"Oct\",\"Nov\":\"Nov\",\"Dic\":\"Dec\"\n",
    "    }\n",
    "    mon_es = s.where(mask_monthly).str.slice(0,3)\n",
    "    year4 = s.where(mask_monthly).str.slice(4,8)\n",
    "    mon_en = mon_es.map(mon_map)\n",
    "    dt_monthly = pd.to_datetime(mon_en + \".\" + year4, format=\"%b.%Y\", errors=\"coerce\")\n",
    "\n",
    "    policy[\"date\"] = dt_iso.fillna(dt_monthly)\n",
    "    policy = policy.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "policy.head(), policy.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "24326939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_17484\\2495546726.py:9: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  .resample(\"M\")\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "Path(\"outputs\").mkdir(exist_ok=True)\n",
    "\n",
    "spy_monthly = (\n",
    "    us_mkt.loc[us_mkt[\"ticker\"].eq(\"SPY\"), [\"date\", \"close\"]]\n",
    "         .dropna(subset=[\"date\", \"close\"])\n",
    "         .assign(date=lambda d: pd.to_datetime(d[\"date\"]))\n",
    "         .set_index(\"date\")[\"close\"]\n",
    "         .resample(\"M\")\n",
    "         .mean()\n",
    "         .rename(\"spy_close_avg\")\n",
    "         .reset_index()\n",
    ")\n",
    "\n",
    "policy_monthly = (\n",
    "    policy.loc[:, [\"date\", \"policy_rate\"]]\n",
    "          .dropna(subset=[\"date\", \"policy_rate\"])\n",
    "          .assign(date=lambda d: pd.to_datetime(d[\"date\"]))\n",
    "          .sort_values(\"date\")\n",
    ")\n",
    "\n",
    "merged_monthly = policy_monthly.merge(spy_monthly, on=\"date\", how=\"inner\")\n",
    "\n",
    "out_path = Path(\"outputs/lecture_2_finance.csv\")\n",
    "merged_monthly.to_csv(out_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ra_task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
