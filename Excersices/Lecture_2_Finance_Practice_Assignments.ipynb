{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed1fd2d",
   "metadata": {},
   "source": [
    "# 3. <a id='intro'>Pandas</a>\n",
    "\n",
    "This practice notebook is **guided by the original Lecture 2** structure. All exercises use **real financial / economic data** from Peru and the US.\n",
    "\n",
    "**Rule for students:** do *not* paste solutions. Fill the TODO blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029387d",
   "metadata": {},
   "source": [
    "## 3.1. <a id='def'>Definition</a>\n",
    "\n",
    "Pandas is a Python library for working with tabular data (Series and DataFrames), including importing, cleaning, reshaping, and merging datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72df12c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use postponed evaluation of type annotations (helps with `str | None` on older Python versions).\n",
    "from __future__ import annotations\n",
    "\n",
    "# Path: cross-platform file/folder paths.\n",
    "from pathlib import Path\n",
    "# hashlib: create stable hashes for cache filenames.\n",
    "import hashlib\n",
    "# re: regular expressions for validating/parsing date strings.\n",
    "import re\n",
    "# warnings: control warning messages.\n",
    "import warnings\n",
    "\n",
    "# numpy: numeric operations + NaN handling.\n",
    "import numpy as np\n",
    "# pandas: tables (Series/DataFrame) + parsing dates + IO (parquet).\n",
    "import pandas as pd\n",
    "\n",
    "# Hide warnings in notebook output (keeps cells clean; you can remove this while debugging).\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define a local folder to store cached downloads.\n",
    "CACHE_DIR = Path(\".cache\")\n",
    "# Create the cache folder if it doesn't exist.\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Map Spanish 3-letter month abbreviations to English ones (needed for parsing BCRP date labels).\n",
    "_ES_TO_EN_MONTH = {\n",
    "    \"Ene\": \"Jan\", \"Feb\": \"Feb\", \"Mar\": \"Mar\", \"Abr\": \"Apr\", \"May\": \"May\", \"Jun\": \"Jun\",\n",
    "    \"Jul\": \"Jul\", \"Ago\": \"Aug\", \"Set\": \"Sep\", \"Sep\": \"Sep\", \"Oct\": \"Oct\", \"Nov\": \"Nov\", \"Dic\": \"Dec\"\n",
    "}\n",
    "\n",
    "def _hash_key(*parts: str) -> str:\n",
    "    # Create a SHA-256 hash object.\n",
    "    h = hashlib.sha256()\n",
    "    # Update the hash with each part (as UTF-8 bytes), plus a separator.\n",
    "    for p in parts:\n",
    "        h.update(str(p).encode(\"utf-8\"))\n",
    "        h.update(b\"|\")\n",
    "    # Return a short hash prefix to use in filenames (still very unlikely to collide).\n",
    "    return h.hexdigest()[:24]\n",
    "\n",
    "def _normalize_period(code: str, period: str | None) -> str | None:\n",
    "    # If no period provided, return None.\n",
    "    if period is None:\n",
    "        return None\n",
    "    # Convert to string and trim spaces.\n",
    "    period = str(period).strip()\n",
    "    # Use the last 2 characters of the BCRP code to infer frequency (PD daily, PM monthly, PA annual).\n",
    "    freq = code[-2:].upper() if len(code) >= 2 else \"\"\n",
    "\n",
    "    if freq == \"PD\":  # daily frequency\n",
    "        # If user passes \"YYYY-M\" or \"YYYY-MM\", convert to \"YYYY-MM-01\" (first day of month).\n",
    "        if re.fullmatch(r\"\\d{4}-\\d{1,2}\", period):\n",
    "            y, m = period.split(\"-\")\n",
    "            return f\"{int(y):04d}-{int(m):02d}-01\"\n",
    "        # If user passes just \"YYYY\", convert to \"YYYY-01-01\".\n",
    "        if re.fullmatch(r\"\\d{4}\", period):\n",
    "            return f\"{int(period):04d}-01-01\"\n",
    "        # Otherwise keep the period as-is (e.g., already \"YYYY-MM-DD\").\n",
    "        return period\n",
    "\n",
    "    if freq == \"PM\":  # monthly frequency\n",
    "        # If user passes \"YYYY-MM-DD\", convert to \"YYYY-M\" (month index).\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{1,2})-(\\d{1,2})\", period)\n",
    "        if m:\n",
    "            y, mo, _ = m.groups()\n",
    "            return f\"{int(y):04d}-{int(mo)}\"\n",
    "        # If user passes \"YYYY-MM\", convert to \"YYYY-M\".\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{1,2})\", period)\n",
    "        if m:\n",
    "            y, mo = m.groups()\n",
    "            return f\"{int(y):04d}-{int(mo)}\"\n",
    "        # If user passes \"YYYY\", default to \"YYYY-1\" (January).\n",
    "        if re.fullmatch(r\"\\d{4}\", period):\n",
    "            return f\"{int(period):04d}-1\"\n",
    "        # Otherwise keep the period as-is.\n",
    "        return period\n",
    "\n",
    "    if freq == \"PA\":  # annual frequency\n",
    "        # Extract the year \"YYYY\" if present at the start.\n",
    "        m = re.match(r\"(\\d{4})\", period)\n",
    "        return m.group(1) if m else period\n",
    "\n",
    "    # If frequency is unknown, return the original period string.\n",
    "    return period\n",
    "\n",
    "def _parse_bcrp_period_name(name: str) -> pd.Timestamp:\n",
    "    # Convert to string and trim.\n",
    "    s = str(name).strip()\n",
    "\n",
    "    # --- Case 1: ISO-like strings: \"YYYY\", \"YYYY-MM\", \"YYYY-MM-DD\" ---\n",
    "    try:\n",
    "        # Validate ISO-like patterns with regex.\n",
    "        if re.fullmatch(r\"\\d{4}(-\\d{1,2}){0,2}\", s):\n",
    "            # Convert to datetime; raise on failure.\n",
    "            return pd.to_datetime(s, errors=\"raise\")\n",
    "    except Exception:\n",
    "        # If it fails, continue to other formats.\n",
    "        pass\n",
    "\n",
    "    # --- Case 2: Monthly label like \"Mar.2020\" (often used by BCRP monthly series) ---\n",
    "    m = re.fullmatch(r\"([A-Za-zÁÉÍÓÚÑñ]{3})\\.(\\d{4})\", s)\n",
    "    if m:\n",
    "        # Extract Spanish month abbreviation and year.\n",
    "        mon_es, y = m.groups()\n",
    "        # Convert Spanish month to English month abbreviation if possible.\n",
    "        mon = _ES_TO_EN_MONTH.get(mon_es[:3], mon_es[:3])\n",
    "        # Parse using the specified format \"%b.%Y\".\n",
    "        return pd.to_datetime(f\"{mon}.{y}\", format=\"%b.%Y\", errors=\"coerce\")\n",
    "\n",
    "    # --- Case 3: Daily label like \"18Nov25\" or \"02Ene97\" (DDMonYY) ---\n",
    "    m = re.fullmatch(r\"(\\d{2})([A-Za-zÁÉÍÓÚÑñ]{3})(\\d{2})\", s)\n",
    "    if m:\n",
    "        # Extract day, Spanish month abbreviation, 2-digit year.\n",
    "        d, mon_es, yy = m.groups()\n",
    "        # Convert Spanish month to English month abbreviation if possible.\n",
    "        mon = _ES_TO_EN_MONTH.get(mon_es[:3], mon_es[:3])\n",
    "        # Convert 2-digit year to 4-digit year (00–69 => 2000–2069, else 1900–1999).\n",
    "        year = 2000 + int(yy) if int(yy) <= 69 else 1900 + int(yy)\n",
    "        # Parse using \"%d%b%Y\" (e.g., \"18Nov2025\").\n",
    "        return pd.to_datetime(f\"{d}{mon}{year}\", format=\"%d%b%Y\", errors=\"coerce\")\n",
    "\n",
    "    # --- Fallback: let pandas try its best; invalid parses become NaT ---\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def bcrp_get(series_codes, start: str | None = None, end: str | None = None, lang: str = \"esp\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch BCRPData series (JSON API) into a DataFrame.\n",
    "\n",
    "    Returns columns: [\"date\", <code1>, <code2>, ...]\n",
    "    \"\"\"\n",
    "    # Try importing requests (needed for HTTP calls). If missing, return empty DataFrame.\n",
    "    try:\n",
    "        import requests\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Accept one code or multiple codes.\n",
    "    if isinstance(series_codes, (list, tuple)):\n",
    "        # Clean each code string.\n",
    "        codes_list = [str(c).strip() for c in series_codes]\n",
    "        # BCRP API allows multiple codes joined by '-'.\n",
    "        codes = \"-\".join(codes_list)\n",
    "        # Use the first code to infer frequency for date normalization.\n",
    "        freq_code = codes_list[0]\n",
    "    else:\n",
    "        # Single code (string).\n",
    "        codes = str(series_codes).strip()\n",
    "        # Split anyway so we keep a list for consistent column naming.\n",
    "        codes_list = codes.split(\"-\")\n",
    "        # Use the first code to infer frequency.\n",
    "        freq_code = codes_list[0]\n",
    "\n",
    "    # Normalize start/end based on frequency (daily/monthly/annual).\n",
    "    start_n = _normalize_period(freq_code, start)\n",
    "    end_n = _normalize_period(freq_code, end)\n",
    "\n",
    "    # Build a deterministic cache key and cache filename.\n",
    "    key = _hash_key(\"bcrp\", codes, start_n or \"\", end_n or \"\", lang)\n",
    "    cache_path = CACHE_DIR / f\"bcrp_{key}.parquet\"\n",
    "    # If cached file exists, load it and return immediately.\n",
    "    if cache_path.exists():\n",
    "        return pd.read_parquet(cache_path)\n",
    "\n",
    "    # Base endpoint for the BCRP series API.\n",
    "    base_url = \"https://estadisticas.bcrp.gob.pe/estadisticas/series/api\"\n",
    "    # Start building URL parts.\n",
    "    parts = [base_url, codes, \"json\"]\n",
    "    # Add start/end only if both are provided.\n",
    "    if start_n and end_n:\n",
    "        parts += [start_n, end_n]\n",
    "    # Add language parameter (e.g., \"esp\").\n",
    "    if lang:\n",
    "        parts += [lang]\n",
    "    # Join into final URL string.\n",
    "    url = \"/\".join(parts)\n",
    "\n",
    "    # Make the HTTP request (30s timeout).\n",
    "    r = requests.get(url, timeout=30)\n",
    "    # Raise an exception if HTTP status is not 200.\n",
    "    r.raise_for_status()\n",
    "    # Parse JSON response body.\n",
    "    obj = r.json()\n",
    "\n",
    "    # Get the list of periods (each period has a label and values).\n",
    "    periods = obj.get(\"periods\", [])\n",
    "    rows = []\n",
    "    # Convert the JSON structure into rows for a DataFrame.\n",
    "    for p in periods:\n",
    "        # Period label (date-like string).\n",
    "        name = p.get(\"name\")\n",
    "        # Values are ordered to match the requested codes.\n",
    "        vals = p.get(\"values\", [])\n",
    "        # If API returns a single string, wrap it into a list for consistency.\n",
    "        if isinstance(vals, str):\n",
    "            vals = [vals]\n",
    "        # Skip malformed entries.\n",
    "        if name is None or not isinstance(vals, list):\n",
    "            continue\n",
    "        # Pad/truncate values to match number of codes.\n",
    "        vals = (vals + [None] * len(codes_list))[:len(codes_list)]\n",
    "        # Append row: [date_label, value1, value2, ...]\n",
    "        rows.append([name] + vals)\n",
    "\n",
    "    # Create a DataFrame with \"date\" + one column per code.\n",
    "    df = pd.DataFrame(rows, columns=[\"date\"] + codes_list)\n",
    "    # If no rows, return an empty DataFrame with the right columns.\n",
    "    if df.shape[0] == 0:\n",
    "        return pd.DataFrame(columns=[\"date\"] + codes_list)\n",
    "\n",
    "    # Parse the \"date\" strings into actual timestamps.\n",
    "    df[\"date\"] = df[\"date\"].apply(_parse_bcrp_period_name)\n",
    "    # Convert each code column to numeric.\n",
    "    for c in codes_list:\n",
    "        # Replace known \"no data\" markers with NaN.\n",
    "        df[c] = df[c].replace({\"n.d.\": np.nan, \"nd\": np.nan, \"N.D.\": np.nan})\n",
    "        # Coerce to numeric (invalid -> NaN).\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows where date failed to parse; sort by date; reset index.\n",
    "    df = df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "    # Save to parquet cache for faster reruns.\n",
    "    df.to_parquet(cache_path)\n",
    "    # Return the cleaned data.\n",
    "    return df\n",
    "\n",
    "def bcrp_get_cached_or_empty(series_codes, start: str, end: str) -> pd.DataFrame:\n",
    "    # Safe wrapper: if network/API fails, return an empty DataFrame with expected columns.\n",
    "    try:\n",
    "        return bcrp_get(series_codes, start=start, end=end)\n",
    "    except Exception:\n",
    "        # Ensure we return the correct columns even when failing.\n",
    "        if isinstance(series_codes, (list, tuple)):\n",
    "            codes_list = [str(c).strip() for c in series_codes]\n",
    "        else:\n",
    "            codes_list = [str(series_codes).strip()]\n",
    "        return pd.DataFrame(columns=[\"date\"] + codes_list)\n",
    "\n",
    "def yf_download_close_volume(tickers, start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download Close and Volume (real market data) using yfinance.\n",
    "    Returns a DataFrame with columns: [\"date\",\"ticker\",\"close\",\"volume\"] in long format.\n",
    "    If download fails, returns an empty DataFrame with those columns.\n",
    "    \"\"\"\n",
    "    # Try importing yfinance. If missing, return empty DataFrame with expected columns.\n",
    "    try:\n",
    "        import yfinance as yf\n",
    "    except Exception:\n",
    "        return pd.DataFrame(columns=[\"date\",\"ticker\",\"close\",\"volume\"])\n",
    "\n",
    "    # Allow passing a single ticker or a list/tuple of tickers.\n",
    "    cols = tickers if isinstance(tickers, (list, tuple)) else [tickers]\n",
    "    # Create a deterministic cache key.\n",
    "    key = _hash_key(\"yf_long\", \",\".join(cols), start, end)\n",
    "    # Cache filename for this request.\n",
    "    cache_path = CACHE_DIR / f\"yf_long_{key}.parquet\"\n",
    "    # If cached file exists, load it.\n",
    "    if cache_path.exists():\n",
    "        return pd.read_parquet(cache_path)\n",
    "\n",
    "    try:\n",
    "        # Download OHLCV data; auto_adjust=True returns adjusted prices.\n",
    "        data = yf.download(cols, start=start, end=end, auto_adjust=True, progress=False)\n",
    "        # If nothing returned, return empty DataFrame with expected columns.\n",
    "        if data.empty:\n",
    "            return pd.DataFrame(columns=[\"date\",\"ticker\",\"close\",\"volume\"])\n",
    "        # If multiple tickers, yfinance returns MultiIndex columns: (\"Close\", ticker), etc.\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            close = data[\"Close\"].copy()\n",
    "            vol = data[\"Volume\"].copy()\n",
    "        else:\n",
    "            # Single ticker: rename to keep ticker as column label.\n",
    "            close = data[[\"Close\"]].rename(columns={\"Close\": cols[0]})\n",
    "            vol = data[[\"Volume\"]].rename(columns={\"Volume\": cols[0]})\n",
    "        # Name the index so it becomes a column after reset_index().\n",
    "        close.index.name = \"date\"\n",
    "        vol.index.name = \"date\"\n",
    "        # Convert wide -> long: columns become rows with a \"ticker\" column.\n",
    "        long_close = close.reset_index().melt(id_vars=\"date\", var_name=\"ticker\", value_name=\"close\")\n",
    "        long_vol = vol.reset_index().melt(id_vars=\"date\", var_name=\"ticker\", value_name=\"volume\")\n",
    "        # Merge close and volume long tables on (date, ticker).\n",
    "        out = long_close.merge(long_vol, on=[\"date\",\"ticker\"], how=\"inner\").dropna(subset=[\"close\"])\n",
    "        # Cache to parquet.\n",
    "        out.to_parquet(cache_path)\n",
    "        # Return the final long-format DataFrame.\n",
    "        return out\n",
    "    except Exception:\n",
    "        # If anything fails, return an empty DataFrame with expected columns.\n",
    "        return pd.DataFrame(columns=[\"date\",\"ticker\",\"close\",\"volume\"])\n",
    "\n",
    "def safe_head(df: pd.DataFrame, n: int = 5) -> pd.DataFrame:\n",
    "    # If df is a DataFrame, return df.head(n); otherwise return an empty DataFrame.\n",
    "    return df.head(n) if isinstance(df, pd.DataFrame) else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3e812",
   "metadata": {},
   "source": [
    "## 3.2. <a id='series'>Pandas Series</a>\n",
    "\n",
    "We will use:\n",
    "- **BCRPData API**: daily PEN/USD exchange rate (buy/sell)\n",
    "- **Yahoo Finance** via `yfinance`: close/volume for US tickers\n",
    "\n",
    "Data sources:\n",
    "- BCRP API help: https://estadisticas.bcrp.gob.pe/estadisticas/series/ayuda/api\n",
    "- yfinance: https://ranaroussi.github.io/yfinance/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b68879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((584, 3),\n",
       " (4970, 4),\n",
       "         date  PEN_USD_buy  PEN_USD_sell\n",
       " 0 2022-02-01     3.871333      3.877667\n",
       " 1 2022-02-02     3.852000      3.857000\n",
       " 2 2022-02-03     3.858500      3.860833\n",
       " 3 2022-02-04     3.863000      3.867833\n",
       " 4 2022-02-07     3.838500      3.845833,\n",
       "         date ticker      close    volume\n",
       " 0 2022-01-03    EEM  44.624969  27572700\n",
       " 1 2022-01-04    EEM  44.470772  24579500\n",
       " 2 2022-01-05    EEM  43.745163  46425100\n",
       " 3 2022-01-06    EEM  43.944714  34288700\n",
       " 4 2022-01-07    EEM  44.343792  32640900)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START = \"2022-01-01\"\n",
    "END = \"2025-12-18\"\n",
    "\n",
    "# BCRP: daily USD/PEN buy & sell\n",
    "fx = bcrp_get_cached_or_empty([\"PD04637PD\",\"PD04638PD\"], start=START, end=END).rename(\n",
    "    columns={\"PD04637PD\":\"PEN_USD_buy\", \"PD04638PD\":\"PEN_USD_sell\"}\n",
    ")\n",
    "\n",
    "# Yahoo Finance: long-format table (date, ticker, close, volume)\n",
    "tickers = [\"SPY\", \"QQQ\", \"TLT\", \"GLD\", \"EEM\"]\n",
    "us_mkt = yf_download_close_volume(tickers, start=START, end=END)\n",
    "\n",
    "fx.shape, us_mkt.shape, safe_head(fx), safe_head(us_mkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0427a811",
   "metadata": {},
   "source": [
    "### 3.2.1. <a id='3.2.1'>From `lists` to `Series`</a>\n",
    "\n",
    "**Assignment:** create a Series from a Python list using FX mid-rate.\n",
    "\n",
    "1. Create `PENUSD_mid = (buy + sell)/2`.\n",
    "2. Take the **last 15 values** as a Python list.\n",
    "3. Build a `pd.Series` with those values (index can be 0..14).\n",
    "4. Name the Series `PENUSD_mid_last15`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c867c5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENUSD_mid_last15 (15,)\n",
      "0    3.370750\n",
      "1    3.359500\n",
      "2    3.368214\n",
      "3    3.364071\n",
      "4    3.370393\n",
      "Name: PENUSD_mid_last15, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "PENUSD_mid_last15 = pd.Series(dtype=float, name=\"PENUSD_mid_last15\")\n",
    "\n",
    "if isinstance(fx, pd.DataFrame) and fx.shape[0] > 0:\n",
    "    fx_mid = (pd.to_numeric(fx[\"PEN_USD_buy\"], errors=\"coerce\") + pd.to_numeric(fx[\"PEN_USD_sell\"], errors=\"coerce\")) / 2\n",
    "    vals = fx_mid.dropna().tail(15).to_list()\n",
    "    PENUSD_mid_last15 = pd.Series(vals, name=\"PENUSD_mid_last15\", dtype=float)\n",
    "\n",
    "# Optional self-check\n",
    "if len(PENUSD_mid_last15) > 0:\n",
    "    print(PENUSD_mid_last15.name, PENUSD_mid_last15.shape)\n",
    "    print(PENUSD_mid_last15.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100847d",
   "metadata": {},
   "source": [
    "### 3.2.2. <a id='3.2.2'>From `NumPy array` to `Series`</a>\n",
    "\n",
    "**Assignment:** create a Series from a NumPy array using US market close prices.\n",
    "\n",
    "1. Filter `us_mkt` for ticker `SPY`.\n",
    "2. Extract the `close` column as a NumPy array.\n",
    "3. Build a `pd.Series` with:\n",
    "   - data = the NumPy array\n",
    "   - index = the corresponding dates\n",
    "4. Compute `mean`, `min`, `max` using Series methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f394a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "2022-01-03    453.210388\n",
      "2022-01-04    453.058594\n",
      "2022-01-05    444.358948\n",
      "2022-01-06    443.941528\n",
      "2022-01-07    442.186310\n",
      "Name: SPY_close, dtype: float64\n",
      "{'mean': 487.0435106058716, 'min': 342.1902160644531, 'max': 689.1699829101562}\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "SPY_close_series = pd.Series(dtype=float, name=\"SPY_close\")\n",
    "\n",
    "summary_stats = {\"mean\": np.nan, \"min\": np.nan, \"max\": np.nan}\n",
    "\n",
    "if isinstance(us_mkt, pd.DataFrame) and us_mkt.shape[0] > 0:\n",
    "    spy = us_mkt.loc[us_mkt[\"ticker\"] == \"SPY\", [\"date\", \"close\"]].copy()\n",
    "    spy[\"date\"] = pd.to_datetime(spy[\"date\"], errors=\"coerce\")\n",
    "    spy[\"close\"] = pd.to_numeric(spy[\"close\"], errors=\"coerce\")\n",
    "    spy = spy.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    if spy.shape[0] > 0:\n",
    "        SPY_close_series = pd.Series(spy[\"close\"].to_numpy(), index=spy[\"date\"], name=\"SPY_close\")\n",
    "        summary_stats = {\n",
    "            \"mean\": float(SPY_close_series.mean(skipna=True)),\n",
    "            \"min\": float(SPY_close_series.min(skipna=True)),\n",
    "            \"max\": float(SPY_close_series.max(skipna=True)),\n",
    "        }\n",
    "\n",
    "# Optional self-check\n",
    "if len(SPY_close_series) > 0:\n",
    "    print(SPY_close_series.head())\n",
    "    print(summary_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec93acb",
   "metadata": {},
   "source": [
    "### 3.2.3. <a id='3.2.3'>From `Dictionary` to `Series`</a>\n",
    "\n",
    "**Assignment:** build a dict and convert to a Series.\n",
    "\n",
    "1. Using `us_mkt`, compute the **last available close** for each ticker in `tickers`.\n",
    "2. Store results in a dict: `{ticker: last_close}`.\n",
    "3. Convert to a Series and sort descending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d001ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY    671.400024\n",
      "QQQ    600.409973\n",
      "GLD    399.290009\n",
      "TLT     87.800003\n",
      "EEM     52.599998\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "last_close_by_ticker = {}  # dict: ticker -> last close\n",
    "\n",
    "if isinstance(us_mkt, pd.DataFrame) and us_mkt.shape[0] > 0:\n",
    "    for t in tickers:\n",
    "        sub = us_mkt.loc[us_mkt[\"ticker\"] == t, [\"date\", \"close\"]].copy()\n",
    "        sub[\"date\"] = pd.to_datetime(sub[\"date\"], errors=\"coerce\")\n",
    "        sub[\"close\"] = pd.to_numeric(sub[\"close\"], errors=\"coerce\")\n",
    "        sub = sub.dropna(subset=[\"date\", \"close\"]).sort_values(\"date\")\n",
    "        if sub.shape[0] > 0:\n",
    "            last_close_by_ticker[t] = float(sub[\"close\"].iloc[-1])\n",
    "\n",
    "last_close_series = pd.Series(last_close_by_ticker, dtype=float).sort_values(ascending=False)\n",
    "\n",
    "# Optional self-check\n",
    "if len(last_close_series) > 0:\n",
    "    print(last_close_series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1c3de",
   "metadata": {},
   "source": [
    "### 3.2.4. <a id='3.2.4'>`Series` vs `NumPy`</a>\n",
    "\n",
    "**Assignment:** show why alignment matters.\n",
    "\n",
    "1. Create two Series:\n",
    "   - `fx_mid`: FX mid-rate indexed by date\n",
    "   - `spy_close`: SPY close indexed by date\n",
    "2. Create a DataFrame by combining them (pandas aligns on dates).\n",
    "3. Separately, create two NumPy arrays of the same length by truncating to the same number of rows.\n",
    "4. Explain in markdown why pandas alignment is safer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e84680a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  fx_mid   spy_close\n",
      "0 2022-01-03     NaN  453.210388\n",
      "1 2022-01-04     NaN  453.058594\n",
      "2 2022-01-05     NaN  444.358948\n",
      "3 2022-01-06     NaN  443.941528\n",
      "4 2022-01-07     NaN  442.186310\n",
      "NumPy shapes: (584,) (584,)\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "aligned_df = pd.DataFrame()\n",
    "\n",
    "fx_np = np.array([])\n",
    "spy_np = np.array([])\n",
    "\n",
    "if isinstance(fx, pd.DataFrame) and fx.shape[0] > 0 and isinstance(us_mkt, pd.DataFrame) and us_mkt.shape[0] > 0:\n",
    "    fx_mid = ((pd.to_numeric(fx[\"PEN_USD_buy\"], errors=\"coerce\") + pd.to_numeric(fx[\"PEN_USD_sell\"], errors=\"coerce\")) / 2).copy()\n",
    "    fx_mid.index = pd.to_datetime(fx[\"date\"], errors=\"coerce\")\n",
    "    fx_mid = fx_mid.dropna()\n",
    "    fx_mid.name = \"fx_mid\"\n",
    "\n",
    "    spy = us_mkt.loc[us_mkt[\"ticker\"] == \"SPY\", [\"date\", \"close\"]].copy()\n",
    "    spy[\"date\"] = pd.to_datetime(spy[\"date\"], errors=\"coerce\")\n",
    "    spy[\"close\"] = pd.to_numeric(spy[\"close\"], errors=\"coerce\")\n",
    "    spy = spy.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "    spy_close = pd.Series(spy[\"close\"].to_numpy(), index=spy[\"date\"], name=\"spy_close\")\n",
    "\n",
    "    aligned_df = pd.concat([fx_mid, spy_close], axis=1).reset_index().rename(columns={\"index\": \"date\"})\n",
    "\n",
    "    # NumPy: force same length by truncation (this ignores date alignment)\n",
    "    fx_vals = fx_mid.dropna().to_numpy()\n",
    "    spy_vals = spy_close.dropna().to_numpy()\n",
    "    n = int(min(len(fx_vals), len(spy_vals)))\n",
    "    if n > 0:\n",
    "        fx_np = fx_vals[:n]\n",
    "        spy_np = spy_vals[:n]\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(aligned_df, pd.DataFrame) and aligned_df.shape[0] > 0:\n",
    "    print(aligned_df.head())\n",
    "    print(\"NumPy shapes:\", fx_np.shape, spy_np.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02fa6f",
   "metadata": {},
   "source": [
    "### 3.2.5. <a id='3.2.5'>Indexing</a>\n",
    "\n",
    "**Assignment:** practice `.loc` and `.iloc`.\n",
    "\n",
    "1. From `last_close_series`, use `.iloc` to take the top 3 tickers.\n",
    "2. Use `.loc` to select the value for `SPY`.\n",
    "3. If `SPY` is not present, explain why (in markdown).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54c656de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY    671.400024\n",
      "QQQ    600.409973\n",
      "GLD    399.290009\n",
      "dtype: float64\n",
      "SPY value: 671.4000244140625\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "top3 = pd.Series(dtype=float)\n",
    "spy_value = np.nan\n",
    "\n",
    "if isinstance(last_close_series, pd.Series) and len(last_close_series) > 0:\n",
    "    top3 = last_close_series.iloc[:3]\n",
    "    if \"SPY\" in last_close_series.index:\n",
    "        spy_value = float(last_close_series.loc[\"SPY\"])\n",
    "\n",
    "# Optional self-check\n",
    "if len(top3) > 0:\n",
    "    print(top3)\n",
    "print(\"SPY value:\", spy_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8d503",
   "metadata": {},
   "source": [
    "## 3.3. <a id='3.3'>DataFrame</a>\n",
    "\n",
    "We now practice DataFrame creation and common methods using the same datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799d65a",
   "metadata": {},
   "source": [
    "### 3.3.1. <a id='3.3.1'>DataFrame Generation</a>\n",
    "\n",
    "#### From `lists` and `dict` to `DataFrame`\n",
    "\n",
    "**Assignment:** create a DataFrame of ticker metadata.\n",
    "\n",
    "1. Make a list of tickers.\n",
    "2. Make a list of last closes (same order).\n",
    "3. Make a dict for an extra column, e.g. `{ticker: 'US'}`.\n",
    "4. Build a DataFrame with columns: `ticker`, `last_close`, `market`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e716c835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker  last_close market\n",
      "0    SPY  671.400024     US\n",
      "1    QQQ  600.409973     US\n",
      "2    GLD  399.290009     US\n",
      "3    TLT   87.800003     US\n",
      "4    EEM   52.599998     US\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "# Use tickers from `last_close_series` if available; otherwise fall back to `tickers`.\n",
    "tickers_list = list(last_close_series.index) if isinstance(last_close_series, pd.Series) and len(last_close_series) > 0 else list(tickers)\n",
    "\n",
    "# Get last closes in the same order as tickers_list.\n",
    "last_close_list = [float(last_close_by_ticker.get(t, np.nan)) for t in tickers_list]\n",
    "\n",
    "# Extra column: market label (simple example)\n",
    "market_dict = {t: \"US\" for t in tickers_list}\n",
    "\n",
    "ticker_df = pd.DataFrame(\n",
    "    {\n",
    "        \"ticker\": tickers_list,\n",
    "        \"last_close\": last_close_list,\n",
    "        \"market\": [market_dict[t] for t in tickers_list],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(ticker_df, pd.DataFrame) and ticker_df.shape[0] > 0:\n",
    "    print(ticker_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed4e31",
   "metadata": {},
   "source": [
    "#### From `lists` and `NumPy` to `DataFrame`\n",
    "\n",
    "**Assignment:** build a DataFrame from NumPy arrays.\n",
    "\n",
    "1. Take the `close` column for `SPY` and `QQQ` from `us_mkt`.\n",
    "2. Convert each to a NumPy array.\n",
    "3. Build a DataFrame with 2 columns: `SPY_close`, `QQQ_close`.\n",
    "4. Add a column with row index (0..n-1) named `t`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15813335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SPY_close   QQQ_close  t\n",
      "0  453.210388  392.184082  0\n",
      "1  453.058594  387.097229  1\n",
      "2  444.358948  375.205200  2\n",
      "3  443.941528  374.941589  3\n",
      "4  442.186310  370.879913  4\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "prices_np_df = pd.DataFrame()\n",
    "\n",
    "if isinstance(us_mkt, pd.DataFrame) and us_mkt.shape[0] > 0:\n",
    "    sub = us_mkt.loc[us_mkt[\"ticker\"].isin([\"SPY\", \"QQQ\"]), [\"date\", \"ticker\", \"close\"]].copy()\n",
    "    sub[\"date\"] = pd.to_datetime(sub[\"date\"], errors=\"coerce\")\n",
    "    sub[\"close\"] = pd.to_numeric(sub[\"close\"], errors=\"coerce\")\n",
    "    sub = sub.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "\n",
    "    wide = sub.pivot_table(index=\"date\", columns=\"ticker\", values=\"close\", aggfunc=\"first\").sort_index()\n",
    "    wide = wide.dropna(subset=[\"SPY\", \"QQQ\"], how=\"any\")\n",
    "\n",
    "    if wide.shape[0] > 0:\n",
    "        prices_np_df = pd.DataFrame(\n",
    "            {\n",
    "                \"SPY_close\": wide[\"SPY\"].to_numpy(),\n",
    "                \"QQQ_close\": wide[\"QQQ\"].to_numpy(),\n",
    "            }\n",
    "        )\n",
    "        prices_np_df[\"t\"] = np.arange(prices_np_df.shape[0])\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(prices_np_df, pd.DataFrame) and prices_np_df.shape[0] > 0:\n",
    "    print(prices_np_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd1ec2",
   "metadata": {},
   "source": [
    "### 3.3.2. <a id='3.3.2'>Indexing</a>\n",
    "\n",
    "**Assignment:** `.loc` and `.iloc` on DataFrames.\n",
    "\n",
    "1. Use `.iloc` to take first 5 rows of `us_mkt`.\n",
    "2. Use `.loc` with a boolean condition to keep only rows where `ticker == 'SPY'`.\n",
    "3. Select only the columns `date`, `ticker`, `close`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f4adde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date ticker       close\n",
      "2982 2022-01-03    SPY  453.210388\n",
      "2983 2022-01-04    SPY  453.058594\n",
      "2984 2022-01-05    SPY  444.358948\n",
      "2985 2022-01-06    SPY  443.941528\n",
      "2986 2022-01-07    SPY  442.186310\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "first5 = pd.DataFrame()\n",
    "only_spy = pd.DataFrame()\n",
    "\n",
    "if isinstance(us_mkt, pd.DataFrame) and us_mkt.shape[0] > 0:\n",
    "    first5 = us_mkt.iloc[:5].copy()\n",
    "    only_spy = us_mkt.loc[us_mkt[\"ticker\"] == \"SPY\", [\"date\", \"ticker\", \"close\"]].copy()\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(only_spy, pd.DataFrame) and only_spy.shape[0] > 0:\n",
    "    print(only_spy.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3a27c",
   "metadata": {},
   "source": [
    "### 3.3.3. <a id='3.3.3'>General Methods</a>\n",
    "\n",
    "**Assignment:** basic methods: `.shape`, `.columns`, `.info`, `.describe`, `.sort_values`.\n",
    "\n",
    "1. Show `us_mkt.shape` and `us_mkt.columns`.\n",
    "2. Use `.describe()` on `close` and `volume`.\n",
    "3. Sort `us_mkt` by `volume` descending and keep top 10 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9da61ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date ticker       close     volume\n",
      "0 2025-04-07    SPY  501.502930  256611400\n",
      "1 2022-01-24    SPY  417.282562  251783900\n",
      "2 2025-04-09    SPY  545.490601  241867300\n",
      "3 2025-04-04    SPY  502.397797  217965100\n",
      "4 2022-02-24    SPY  406.334412  213942900\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "desc = pd.DataFrame()\n",
    "top10_volume = pd.DataFrame()\n",
    "\n",
    "if isinstance(us_mkt, pd.DataFrame) and us_mkt.shape[0] > 0:\n",
    "    tmp = us_mkt.copy()\n",
    "    tmp[\"close\"] = pd.to_numeric(tmp[\"close\"], errors=\"coerce\")\n",
    "    tmp[\"volume\"] = pd.to_numeric(tmp[\"volume\"], errors=\"coerce\")\n",
    "\n",
    "    desc = tmp[[\"close\", \"volume\"]].describe()\n",
    "\n",
    "    top10_volume = (\n",
    "        tmp.sort_values(\"volume\", ascending=False)\n",
    "        .loc[:, [\"date\", \"ticker\", \"close\", \"volume\"]]\n",
    "        .head(10)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(top10_volume, pd.DataFrame) and top10_volume.shape[0] > 0:\n",
    "    print(top10_volume.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5dd1b9",
   "metadata": {},
   "source": [
    "### 3.3.4. <a id='3.3.4'>Importing Data</a>\n",
    "\n",
    "**Assignment:** `to_csv` + `read_csv` using real data.\n",
    "\n",
    "1. Save a subset of `us_mkt` to `data/us_mkt_sample.csv` (e.g., 500 rows).\n",
    "2. Read it back using `pd.read_csv`.\n",
    "3. Rename columns to snake_case.\n",
    "4. Check dtypes and missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "658402d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtypes:\n",
      " date      datetime64[ns]\n",
      "ticker            object\n",
      "close            float64\n",
      "volume             int64\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      " date      0\n",
      "ticker    0\n",
      "close     0\n",
      "volume    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "# Solution\n",
    "us_sample = pd.DataFrame()\n",
    "us_from_csv = pd.DataFrame()\n",
    "\n",
    "if isinstance(us_mkt, pd.DataFrame) and us_mkt.shape[0] > 0:\n",
    "    us_sample = us_mkt.head(500).copy()\n",
    "\n",
    "    csv_path = Path(\"data/us_mkt_sample.csv\")\n",
    "    us_sample.to_csv(csv_path, index=False)\n",
    "\n",
    "    us_from_csv = pd.read_csv(csv_path)\n",
    "\n",
    "    # Rename columns to snake_case\n",
    "    us_from_csv.columns = [c.strip().lower().replace(\" \", \"_\") for c in us_from_csv.columns]\n",
    "\n",
    "    # Parse dates (safe)\n",
    "    if \"date\" in us_from_csv.columns:\n",
    "        us_from_csv[\"date\"] = pd.to_datetime(us_from_csv[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    # Quick checks\n",
    "    _dtypes = us_from_csv.dtypes\n",
    "    _missing = us_from_csv.isna().sum()\n",
    "\n",
    "    print(\"Dtypes:\\n\", _dtypes)\n",
    "    print(\"\\nMissing values:\\n\", _missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b051852",
   "metadata": {},
   "source": [
    "### 3.3.5. <a id='3.3.5'>Filtering data</a>\n",
    "\n",
    "**Assignment:** filtering with conditions.\n",
    "\n",
    "1. Filter `us_mkt` for rows where `close` is above the 90th percentile **within each ticker**.\n",
    "2. Filter rows with `volume` missing (if any) and count them.\n",
    "3. Create a filtered DataFrame for tickers `['SPY','GLD']` only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa4f33e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date ticker       close    volume\n",
      "994 2022-01-03    GLD  168.330002   9014400\n",
      "995 2022-01-04    GLD  169.570007   6965600\n",
      "996 2022-01-05    GLD  169.059998   8715600\n",
      "997 2022-01-06    GLD  166.990005  10902700\n",
      "998 2022-01-07    GLD  167.750000   8191900\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "high_close = pd.DataFrame()\n",
    "missing_volume = pd.DataFrame()\n",
    "spy_gld = pd.DataFrame()\n",
    "\n",
    "if isinstance(us_mkt, pd.DataFrame) and us_mkt.shape[0] > 0:\n",
    "    tmp = us_mkt.copy()\n",
    "    tmp[\"close\"] = pd.to_numeric(tmp[\"close\"], errors=\"coerce\")\n",
    "    tmp[\"volume\"] = pd.to_numeric(tmp[\"volume\"], errors=\"coerce\")\n",
    "\n",
    "    # 90th percentile within each ticker\n",
    "    q90 = tmp.groupby(\"ticker\")[\"close\"].transform(lambda s: s.quantile(0.9))\n",
    "    high_close = tmp[tmp[\"close\"] > q90].copy()\n",
    "\n",
    "    # rows with missing volume\n",
    "    missing_volume = tmp[tmp[\"volume\"].isna()].copy()\n",
    "\n",
    "    # keep only SPY and GLD\n",
    "    spy_gld = tmp[tmp[\"ticker\"].isin([\"SPY\", \"GLD\"])].copy()\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(spy_gld, pd.DataFrame) and spy_gld.shape[0] > 0:\n",
    "    print(spy_gld.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6b93b",
   "metadata": {},
   "source": [
    "### 3.3.6. <a id='3.3.6'>Dealing with nulls</a>\n",
    "\n",
    "**Assignment:** introduce NaNs and handle them.\n",
    "\n",
    "1. Copy `us_mkt` to `us_mkt_nan`.\n",
    "2. Set 1% of the `close` values to NaN (fixed random seed).\n",
    "3. Create two cleaned versions:\n",
    "   - dropped NaNs\n",
    "   - filled NaNs with the ticker-specific median close\n",
    "4. Compare shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7cbc4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (4970, 4) With NaNs: (4970, 4)\n",
      "Drop: (4920, 4) Fill: (4970, 4)\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "us_mkt_nan = us_mkt.copy()\n",
    "us_drop = pd.DataFrame()\n",
    "us_fill = pd.DataFrame()\n",
    "\n",
    "if isinstance(us_mkt_nan, pd.DataFrame) and us_mkt_nan.shape[0] > 0:\n",
    "    tmp = us_mkt_nan.copy()\n",
    "    tmp[\"close\"] = pd.to_numeric(tmp[\"close\"], errors=\"coerce\")\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "    n = int(max(1, round(0.01 * tmp.shape[0])))\n",
    "    idx = rng.choice(tmp.index.to_numpy(), size=n, replace=False)\n",
    "    tmp.loc[idx, \"close\"] = np.nan\n",
    "\n",
    "    us_mkt_nan = tmp\n",
    "\n",
    "    us_drop = us_mkt_nan.dropna(subset=[\"close\"]).copy()\n",
    "\n",
    "    med = us_mkt_nan.groupby(\"ticker\")[\"close\"].transform(\"median\")\n",
    "    us_fill = us_mkt_nan.copy()\n",
    "    us_fill[\"close\"] = us_fill[\"close\"].fillna(med)\n",
    "\n",
    "# Optional self-check\n",
    "print(\"Original:\", us_mkt.shape, \"With NaNs:\", us_mkt_nan.shape)\n",
    "print(\"Drop:\", us_drop.shape, \"Fill:\", us_fill.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c49c5",
   "metadata": {},
   "source": [
    "### 3.3.7. <a id='3.3.7'>Duplicates</a>\n",
    "\n",
    "**Assignment:** create duplicates and remove them.\n",
    "\n",
    "1. Create `dup_df` by stacking the last 5 rows of `us_mkt` twice.\n",
    "2. Use `.duplicated()` to detect duplicates.\n",
    "3. Use `.drop_duplicates()` to remove duplicates.\n",
    "4. Verify row counts before/after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e8b1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dup_df rows: 10\n",
      "dedup_df rows: 5\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "dup_df = pd.DataFrame()\n",
    "dup_mask = pd.Series(dtype=bool)\n",
    "dedup_df = pd.DataFrame()\n",
    "\n",
    "if isinstance(us_mkt, pd.DataFrame) and us_mkt.shape[0] > 0:\n",
    "    last5 = us_mkt.tail(5).copy()\n",
    "    dup_df = pd.concat([last5, last5], ignore_index=True)\n",
    "    dup_mask = dup_df.duplicated()\n",
    "    dedup_df = dup_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Optional self-check\n",
    "print(\"dup_df rows:\", getattr(dup_df, \"shape\", (0,0))[0])\n",
    "print(\"dedup_df rows:\", getattr(dedup_df, \"shape\", (0,0))[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba91b8dd",
   "metadata": {},
   "source": [
    "### 3.3.8. <a id='3.3.8'>Groupby</a>\n",
    "\n",
    "**Assignment:** groupby + aggregation.\n",
    "\n",
    "1. Group `us_mkt` by `ticker` and compute:\n",
    "   - mean close\n",
    "   - median close\n",
    "   - max volume\n",
    "2. Rename the resulting columns clearly.\n",
    "3. Sort by mean close descending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51da27e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker  mean_close  median_close  max_volume\n",
      "0    SPY  487.043511    462.101456   256611400\n",
      "1    QQQ  411.806856    400.730148   198685800\n",
      "2    GLD  220.130422    187.864998    62025000\n",
      "3    TLT   91.751313     88.894325   131353500\n",
      "4    EEM   40.462350     39.107405   134225700\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "ticker_summary = pd.DataFrame()\n",
    "\n",
    "if isinstance(us_mkt, pd.DataFrame) and us_mkt.shape[0] > 0:\n",
    "    tmp = us_mkt.copy()\n",
    "    tmp[\"close\"] = pd.to_numeric(tmp[\"close\"], errors=\"coerce\")\n",
    "    tmp[\"volume\"] = pd.to_numeric(tmp[\"volume\"], errors=\"coerce\")\n",
    "\n",
    "    ticker_summary = (\n",
    "        tmp.groupby(\"ticker\")\n",
    "        .agg(\n",
    "            mean_close=(\"close\", \"mean\"),\n",
    "            median_close=(\"close\", \"median\"),\n",
    "            max_volume=(\"volume\", \"max\"),\n",
    "        )\n",
    "        .sort_values(\"mean_close\", ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(ticker_summary, pd.DataFrame) and ticker_summary.shape[0] > 0:\n",
    "    print(ticker_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196dffb",
   "metadata": {},
   "source": [
    "### 3.3.9. <a id='3.3.9'>Reshape</a>\n",
    "\n",
    "##### From Wide to Long\n",
    "\n",
    "**Assignment:** melt a wide table.\n",
    "\n",
    "1. Create a small wide DataFrame with 1 row containing last closes for each ticker.\n",
    "2. Melt it to long format with columns: `ticker`, `last_close`.\n",
    "\n",
    "##### From Long to Wide\n",
    "\n",
    "**Assignment:** pivot back.\n",
    "\n",
    "3. Using `us_mkt`, create a pivot table with:\n",
    "   - index = `date`\n",
    "   - columns = `ticker`\n",
    "   - values = `close`\n",
    "4. Keep only the first 50 dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bda69690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker  last_close\n",
      "0    SPY  671.400024\n",
      "1    QQQ  600.409973\n",
      "2    GLD  399.290009\n",
      "3    TLT   87.800003\n",
      "4    EEM   52.599998\n",
      "ticker       date        EEM         GLD         QQQ         SPY         TLT\n",
      "0      2022-01-03  44.624969  168.330002  392.184082  453.210388  125.782967\n",
      "1      2022-01-04  44.470772  169.570007  387.097229  453.058594  125.259987\n",
      "2      2022-01-05  43.745163  169.059998  375.205200  444.358948  124.580070\n",
      "3      2022-01-06  43.944714  166.990005  374.941589  443.941528  124.902588\n",
      "4      2022-01-07  44.343792  167.750000  370.879913  442.186310  124.004715\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "wide_last = pd.DataFrame()\n",
    "long_last = pd.DataFrame()\n",
    "\n",
    "wide_close = pd.DataFrame()\n",
    "\n",
    "# Wide (1 row): last close per ticker\n",
    "if isinstance(last_close_series, pd.Series) and len(last_close_series) > 0:\n",
    "    wide_last = pd.DataFrame([last_close_series.to_dict()])\n",
    "\n",
    "# Melt wide -> long\n",
    "if isinstance(wide_last, pd.DataFrame) and wide_last.shape[0] > 0:\n",
    "    long_last = wide_last.melt(var_name=\"ticker\", value_name=\"last_close\")\n",
    "else:\n",
    "    long_last = pd.DataFrame(columns=[\"ticker\", \"last_close\"])\n",
    "\n",
    "# Long -> wide (pivot close by date and ticker)\n",
    "if isinstance(us_mkt, pd.DataFrame) and us_mkt.shape[0] > 0:\n",
    "    tmp = us_mkt.copy()\n",
    "    tmp[\"date\"] = pd.to_datetime(tmp[\"date\"], errors=\"coerce\")\n",
    "    tmp = tmp.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "    wide_close = (\n",
    "        tmp.pivot_table(index=\"date\", columns=\"ticker\", values=\"close\", aggfunc=\"first\")\n",
    "        .sort_index()\n",
    "        .head(50)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(long_last, pd.DataFrame) and long_last.shape[0] > 0:\n",
    "    print(long_last.head())\n",
    "if isinstance(wide_close, pd.DataFrame) and wide_close.shape[0] > 0:\n",
    "    print(wide_close.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2170e6e",
   "metadata": {},
   "source": [
    "### 3.3.10. <a id='3.3.10'>Merge</a>\n",
    "\n",
    "**Assignment:** merge Peru macro data (BCRP) with US market data (Yahoo).\n",
    "\n",
    "1. Fetch BCRP monthly policy rate: `PD12301MD`.\n",
    "2. Create a monthly table from US market data by extracting `year` and `month` from the `date` column.\n",
    "   Hint: you can use `pd.to_datetime` **only here**.\n",
    "3. Compute the monthly average close for SPY.\n",
    "4. Merge policy rate with monthly SPY average using `merge`.\n",
    "5. Save to `outputs/lecture2_policy_spy_monthly.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbb87fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs/lecture2_policy_spy_monthly.csv rows: 28\n"
     ]
    }
   ],
   "source": [
    "policy = bcrp_get_cached_or_empty(\"PD12301MD\", start=START, end=END).rename(columns={\"PD12301MD\":\"policy_rate\"})\n",
    "\n",
    "from pathlib import Path\n",
    "Path(\"outputs\").mkdir(exist_ok=True)\n",
    "\n",
    "# Solution\n",
    "spy_monthly = pd.DataFrame()\n",
    "policy_monthly = pd.DataFrame()\n",
    "merged_monthly = pd.DataFrame()\n",
    "\n",
    "# Monthly average SPY close\n",
    "if isinstance(us_mkt, pd.DataFrame) and us_mkt.shape[0] > 0:\n",
    "    spy = us_mkt.loc[us_mkt[\"ticker\"] == \"SPY\", [\"date\", \"close\"]].copy()\n",
    "    spy[\"date\"] = pd.to_datetime(spy[\"date\"], errors=\"coerce\")\n",
    "    spy[\"close\"] = pd.to_numeric(spy[\"close\"], errors=\"coerce\")\n",
    "    spy = spy.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "\n",
    "    if spy.shape[0] > 0:\n",
    "        spy[\"year\"] = spy[\"date\"].dt.year\n",
    "        spy[\"month\"] = spy[\"date\"].dt.month\n",
    "        spy_monthly = (\n",
    "            spy.groupby([\"year\", \"month\"], as_index=False)\n",
    "            .agg(spy_close_avg=(\"close\", \"mean\"))\n",
    "            .sort_values([\"year\", \"month\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "# Monthly average policy rate (from daily series)\n",
    "if isinstance(policy, pd.DataFrame) and policy.shape[0] > 0:\n",
    "    pol = policy.copy()\n",
    "    pol[\"date\"] = pd.to_datetime(pol[\"date\"], errors=\"coerce\")\n",
    "    pol[\"policy_rate\"] = pd.to_numeric(pol[\"policy_rate\"], errors=\"coerce\")\n",
    "    pol = pol.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "\n",
    "    if pol.shape[0] > 0:\n",
    "        pol[\"year\"] = pol[\"date\"].dt.year\n",
    "        pol[\"month\"] = pol[\"date\"].dt.month\n",
    "        policy_monthly = (\n",
    "            pol.groupby([\"year\", \"month\"], as_index=False)\n",
    "            .agg(policy_rate=(\"policy_rate\", \"mean\"))\n",
    "            .sort_values([\"year\", \"month\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "# Merge and save\n",
    "if policy_monthly.shape[0] > 0 and spy_monthly.shape[0] > 0:\n",
    "    merged_monthly = pd.merge(policy_monthly, spy_monthly, on=[\"year\", \"month\"], how=\"inner\")\n",
    "    merged_monthly[\"date\"] = pd.to_datetime(dict(year=merged_monthly[\"year\"], month=merged_monthly[\"month\"], day=1))\n",
    "    merged_monthly = merged_monthly[[\"date\", \"year\", \"month\", \"policy_rate\", \"spy_close_avg\"]].sort_values([\"year\",\"month\"]).reset_index(drop=True)\n",
    "\n",
    "out_path = Path(\"outputs/lecture2_policy_spy_monthly.csv\")\n",
    "merged_monthly.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"Saved:\", out_path.as_posix(), \"rows:\", merged_monthly.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16669b3",
   "metadata": {},
   "source": [
    "## 3.4. <a id='3.4'>References</a>\n",
    "\n",
    "- Pandas Series: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html\n",
    "- Pandas DataFrame: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\n",
    "- Pandas melt: https://pandas.pydata.org/docs/reference/api/pandas.melt.html\n",
    "- Pandas pivot_table: https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html\n",
    "- Pandas merge: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html\n",
    "- BCRP API help: https://estadisticas.bcrp.gob.pe/estadisticas/series/ayuda/api\n",
    "- yfinance: https://ranaroussi.github.io/yfinance/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ra_task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
