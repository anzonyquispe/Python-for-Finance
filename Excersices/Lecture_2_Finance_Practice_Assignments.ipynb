{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed1fd2d",
   "metadata": {},
   "source": [
    "# 3. <a id='intro'>Pandas</a>\n",
    "\n",
    "This practice notebook is **guided by the original Lecture 2** structure. All exercises use **real financial / economic data** from Peru and the US.\n",
    "\n",
    "**Rule for students:** do *not* paste solutions. Fill the TODO blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029387d",
   "metadata": {},
   "source": [
    "## 3.1. <a id='def'>Definition</a>\n",
    "\n",
    "Pandas is a Python library for working with tabular data (Series and DataFrames), including importing, cleaning, reshaping, and merging datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df12c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use postponed evaluation of type annotations (helps with `str | None` on older Python versions).\n",
    "from __future__ import annotations\n",
    "\n",
    "# Path: cross-platform file/folder paths.\n",
    "from pathlib import Path\n",
    "# hashlib: create stable hashes for cache filenames.\n",
    "import hashlib\n",
    "# re: regular expressions for validating/parsing date strings.\n",
    "import re\n",
    "# warnings: control warning messages.\n",
    "import warnings\n",
    "\n",
    "# numpy: numeric operations + NaN handling.\n",
    "import numpy as np\n",
    "# pandas: tables (Series/DataFrame) + parsing dates + IO (parquet).\n",
    "import pandas as pd\n",
    "\n",
    "# Hide warnings in notebook output (keeps cells clean; you can remove this while debugging).\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define a local folder to store cached downloads.\n",
    "CACHE_DIR = Path(\".cache\")\n",
    "# Create the cache folder if it doesn't exist.\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Map Spanish 3-letter month abbreviations to English ones (needed for parsing BCRP date labels).\n",
    "_ES_TO_EN_MONTH = {\n",
    "    \"Ene\": \"Jan\", \"Feb\": \"Feb\", \"Mar\": \"Mar\", \"Abr\": \"Apr\", \"May\": \"May\", \"Jun\": \"Jun\",\n",
    "    \"Jul\": \"Jul\", \"Ago\": \"Aug\", \"Set\": \"Sep\", \"Sep\": \"Sep\", \"Oct\": \"Oct\", \"Nov\": \"Nov\", \"Dic\": \"Dec\"\n",
    "}\n",
    "\n",
    "def _hash_key(*parts: str) -> str:\n",
    "    # Create a SHA-256 hash object.\n",
    "    h = hashlib.sha256()\n",
    "    # Update the hash with each part (as UTF-8 bytes), plus a separator.\n",
    "    for p in parts:\n",
    "        h.update(str(p).encode(\"utf-8\"))\n",
    "        h.update(b\"|\")\n",
    "    # Return a short hash prefix to use in filenames (still very unlikely to collide).\n",
    "    return h.hexdigest()[:24]\n",
    "\n",
    "def _normalize_period(code: str, period: str | None) -> str | None:\n",
    "    # If no period provided, return None.\n",
    "    if period is None:\n",
    "        return None\n",
    "    # Convert to string and trim spaces.\n",
    "    period = str(period).strip()\n",
    "    # Use the last 2 characters of the BCRP code to infer frequency (PD daily, PM monthly, PA annual).\n",
    "    freq = code[-2:].upper() if len(code) >= 2 else \"\"\n",
    "\n",
    "    if freq == \"PD\":  # daily frequency\n",
    "        # If user passes \"YYYY-M\" or \"YYYY-MM\", convert to \"YYYY-MM-01\" (first day of month).\n",
    "        if re.fullmatch(r\"\\d{4}-\\d{1,2}\", period):\n",
    "            y, m = period.split(\"-\")\n",
    "            return f\"{int(y):04d}-{int(m):02d}-01\"\n",
    "        # If user passes just \"YYYY\", convert to \"YYYY-01-01\".\n",
    "        if re.fullmatch(r\"\\d{4}\", period):\n",
    "            return f\"{int(period):04d}-01-01\"\n",
    "        # Otherwise keep the period as-is (e.g., already \"YYYY-MM-DD\").\n",
    "        return period\n",
    "\n",
    "    if freq == \"PM\":  # monthly frequency\n",
    "        # If user passes \"YYYY-MM-DD\", convert to \"YYYY-M\" (month index).\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{1,2})-(\\d{1,2})\", period)\n",
    "        if m:\n",
    "            y, mo, _ = m.groups()\n",
    "            return f\"{int(y):04d}-{int(mo)}\"\n",
    "        # If user passes \"YYYY-MM\", convert to \"YYYY-M\".\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{1,2})\", period)\n",
    "        if m:\n",
    "            y, mo = m.groups()\n",
    "            return f\"{int(y):04d}-{int(mo)}\"\n",
    "        # If user passes \"YYYY\", default to \"YYYY-1\" (January).\n",
    "        if re.fullmatch(r\"\\d{4}\", period):\n",
    "            return f\"{int(period):04d}-1\"\n",
    "        # Otherwise keep the period as-is.\n",
    "        return period\n",
    "\n",
    "    if freq == \"PA\":  # annual frequency\n",
    "        # Extract the year \"YYYY\" if present at the start.\n",
    "        m = re.match(r\"(\\d{4})\", period)\n",
    "        return m.group(1) if m else period\n",
    "\n",
    "    # If frequency is unknown, return the original period string.\n",
    "    return period\n",
    "\n",
    "def _parse_bcrp_period_name(name: str) -> pd.Timestamp:\n",
    "    # Convert to string and trim.\n",
    "    s = str(name).strip()\n",
    "\n",
    "    # --- Case 1: ISO-like strings: \"YYYY\", \"YYYY-MM\", \"YYYY-MM-DD\" ---\n",
    "    try:\n",
    "        # Validate ISO-like patterns with regex.\n",
    "        if re.fullmatch(r\"\\d{4}(-\\d{1,2}){0,2}\", s):\n",
    "            # Convert to datetime; raise on failure.\n",
    "            return pd.to_datetime(s, errors=\"raise\")\n",
    "    except Exception:\n",
    "        # If it fails, continue to other formats.\n",
    "        pass\n",
    "\n",
    "    # --- Case 2: Monthly label like \"Mar.2020\" (often used by BCRP monthly series) ---\n",
    "    m = re.fullmatch(r\"([A-Za-zÁÉÍÓÚÑñ]{3})\\.(\\d{4})\", s)\n",
    "    if m:\n",
    "        # Extract Spanish month abbreviation and year.\n",
    "        mon_es, y = m.groups()\n",
    "        # Convert Spanish month to English month abbreviation if possible.\n",
    "        mon = _ES_TO_EN_MONTH.get(mon_es[:3], mon_es[:3])\n",
    "        # Parse using the specified format \"%b.%Y\".\n",
    "        return pd.to_datetime(f\"{mon}.{y}\", format=\"%b.%Y\", errors=\"coerce\")\n",
    "\n",
    "    # --- Case 3: Daily label like \"18Nov25\" or \"02Ene97\" (DDMonYY) ---\n",
    "    m = re.fullmatch(r\"(\\d{2})([A-Za-zÁÉÍÓÚÑñ]{3})(\\d{2})\", s)\n",
    "    if m:\n",
    "        # Extract day, Spanish month abbreviation, 2-digit year.\n",
    "        d, mon_es, yy = m.groups()\n",
    "        # Convert Spanish month to English month abbreviation if possible.\n",
    "        mon = _ES_TO_EN_MONTH.get(mon_es[:3], mon_es[:3])\n",
    "        # Convert 2-digit year to 4-digit year (00–69 => 2000–2069, else 1900–1999).\n",
    "        year = 2000 + int(yy) if int(yy) <= 69 else 1900 + int(yy)\n",
    "        # Parse using \"%d%b%Y\" (e.g., \"18Nov2025\").\n",
    "        return pd.to_datetime(f\"{d}{mon}{year}\", format=\"%d%b%Y\", errors=\"coerce\")\n",
    "\n",
    "    # --- Fallback: let pandas try its best; invalid parses become NaT ---\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def bcrp_get(series_codes, start: str | None = None, end: str | None = None, lang: str = \"esp\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch BCRPData series (JSON API) into a DataFrame.\n",
    "\n",
    "    Returns columns: [\"date\", <code1>, <code2>, ...]\n",
    "    \"\"\"\n",
    "    # Try importing requests (needed for HTTP calls). If missing, return empty DataFrame.\n",
    "    try:\n",
    "        import requests\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Accept one code or multiple codes.\n",
    "    if isinstance(series_codes, (list, tuple)):\n",
    "        # Clean each code string.\n",
    "        codes_list = [str(c).strip() for c in series_codes]\n",
    "        # BCRP API allows multiple codes joined by '-'.\n",
    "        codes = \"-\".join(codes_list)\n",
    "        # Use the first code to infer frequency for date normalization.\n",
    "        freq_code = codes_list[0]\n",
    "    else:\n",
    "        # Single code (string).\n",
    "        codes = str(series_codes).strip()\n",
    "        # Split anyway so we keep a list for consistent column naming.\n",
    "        codes_list = codes.split(\"-\")\n",
    "        # Use the first code to infer frequency.\n",
    "        freq_code = codes_list[0]\n",
    "\n",
    "    # Normalize start/end based on frequency (daily/monthly/annual).\n",
    "    start_n = _normalize_period(freq_code, start)\n",
    "    end_n = _normalize_period(freq_code, end)\n",
    "\n",
    "    # Build a deterministic cache key and cache filename.\n",
    "    key = _hash_key(\"bcrp\", codes, start_n or \"\", end_n or \"\", lang)\n",
    "    cache_path = CACHE_DIR / f\"bcrp_{key}.parquet\"\n",
    "    # If cached file exists, load it and return immediately.\n",
    "    if cache_path.exists():\n",
    "        return pd.read_parquet(cache_path)\n",
    "\n",
    "    # Base endpoint for the BCRP series API.\n",
    "    base_url = \"https://estadisticas.bcrp.gob.pe/estadisticas/series/api\"\n",
    "    # Start building URL parts.\n",
    "    parts = [base_url, codes, \"json\"]\n",
    "    # Add start/end only if both are provided.\n",
    "    if start_n and end_n:\n",
    "        parts += [start_n, end_n]\n",
    "    # Add language parameter (e.g., \"esp\").\n",
    "    if lang:\n",
    "        parts += [lang]\n",
    "    # Join into final URL string.\n",
    "    url = \"/\".join(parts)\n",
    "\n",
    "    # Make the HTTP request (30s timeout).\n",
    "    r = requests.get(url, timeout=30)\n",
    "    # Raise an exception if HTTP status is not 200.\n",
    "    r.raise_for_status()\n",
    "    # Parse JSON response body.\n",
    "    obj = r.json()\n",
    "\n",
    "    # Get the list of periods (each period has a label and values).\n",
    "    periods = obj.get(\"periods\", [])\n",
    "    rows = []\n",
    "    # Convert the JSON structure into rows for a DataFrame.\n",
    "    for p in periods:\n",
    "        # Period label (date-like string).\n",
    "        name = p.get(\"name\")\n",
    "        # Values are ordered to match the requested codes.\n",
    "        vals = p.get(\"values\", [])\n",
    "        # If API returns a single string, wrap it into a list for consistency.\n",
    "        if isinstance(vals, str):\n",
    "            vals = [vals]\n",
    "        # Skip malformed entries.\n",
    "        if name is None or not isinstance(vals, list):\n",
    "            continue\n",
    "        # Pad/truncate values to match number of codes.\n",
    "        vals = (vals + [None] * len(codes_list))[:len(codes_list)]\n",
    "        # Append row: [date_label, value1, value2, ...]\n",
    "        rows.append([name] + vals)\n",
    "\n",
    "    # Create a DataFrame with \"date\" + one column per code.\n",
    "    df = pd.DataFrame(rows, columns=[\"date\"] + codes_list)\n",
    "    # If no rows, return an empty DataFrame with the right columns.\n",
    "    if df.shape[0] == 0:\n",
    "        return pd.DataFrame(columns=[\"date\"] + codes_list)\n",
    "\n",
    "    # Parse the \"date\" strings into actual timestamps.\n",
    "    df[\"date\"] = df[\"date\"].apply(_parse_bcrp_period_name)\n",
    "    # Convert each code column to numeric.\n",
    "    for c in codes_list:\n",
    "        # Replace known \"no data\" markers with NaN.\n",
    "        df[c] = df[c].replace({\"n.d.\": np.nan, \"nd\": np.nan, \"N.D.\": np.nan})\n",
    "        # Coerce to numeric (invalid -> NaN).\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows where date failed to parse; sort by date; reset index.\n",
    "    df = df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "    # Save to parquet cache for faster reruns.\n",
    "    df.to_parquet(cache_path)\n",
    "    # Return the cleaned data.\n",
    "    return df\n",
    "\n",
    "def bcrp_get_cached_or_empty(series_codes, start: str, end: str) -> pd.DataFrame:\n",
    "    # Safe wrapper: if network/API fails, return an empty DataFrame with expected columns.\n",
    "    try:\n",
    "        return bcrp_get(series_codes, start=start, end=end)\n",
    "    except Exception:\n",
    "        # Ensure we return the correct columns even when failing.\n",
    "        if isinstance(series_codes, (list, tuple)):\n",
    "            codes_list = [str(c).strip() for c in series_codes]\n",
    "        else:\n",
    "            codes_list = [str(series_codes).strip()]\n",
    "        return pd.DataFrame(columns=[\"date\"] + codes_list)\n",
    "\n",
    "def yf_download_close_volume(tickers, start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download Close and Volume (real market data) using yfinance.\n",
    "    Returns a DataFrame with columns: [\"date\",\"ticker\",\"close\",\"volume\"] in long format.\n",
    "    If download fails, returns an empty DataFrame with those columns.\n",
    "    \"\"\"\n",
    "    # Try importing yfinance. If missing, return empty DataFrame with expected columns.\n",
    "    try:\n",
    "        import yfinance as yf\n",
    "    except Exception:\n",
    "        return pd.DataFrame(columns=[\"date\",\"ticker\",\"close\",\"volume\"])\n",
    "\n",
    "    # Allow passing a single ticker or a list/tuple of tickers.\n",
    "    cols = tickers if isinstance(tickers, (list, tuple)) else [tickers]\n",
    "    # Create a deterministic cache key.\n",
    "    key = _hash_key(\"yf_long\", \",\".join(cols), start, end)\n",
    "    # Cache filename for this request.\n",
    "    cache_path = CACHE_DIR / f\"yf_long_{key}.parquet\"\n",
    "    # If cached file exists, load it.\n",
    "    if cache_path.exists():\n",
    "        return pd.read_parquet(cache_path)\n",
    "\n",
    "    try:\n",
    "        # Download OHLCV data; auto_adjust=True returns adjusted prices.\n",
    "        data = yf.download(cols, start=start, end=end, auto_adjust=True, progress=False)\n",
    "        # If nothing returned, return empty DataFrame with expected columns.\n",
    "        if data.empty:\n",
    "            return pd.DataFrame(columns=[\"date\",\"ticker\",\"close\",\"volume\"])\n",
    "        # If multiple tickers, yfinance returns MultiIndex columns: (\"Close\", ticker), etc.\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            close = data[\"Close\"].copy()\n",
    "            vol = data[\"Volume\"].copy()\n",
    "        else:\n",
    "            # Single ticker: rename to keep ticker as column label.\n",
    "            close = data[[\"Close\"]].rename(columns={\"Close\": cols[0]})\n",
    "            vol = data[[\"Volume\"]].rename(columns={\"Volume\": cols[0]})\n",
    "        # Name the index so it becomes a column after reset_index().\n",
    "        close.index.name = \"date\"\n",
    "        vol.index.name = \"date\"\n",
    "        # Convert wide -> long: columns become rows with a \"ticker\" column.\n",
    "        long_close = close.reset_index().melt(id_vars=\"date\", var_name=\"ticker\", value_name=\"close\")\n",
    "        long_vol = vol.reset_index().melt(id_vars=\"date\", var_name=\"ticker\", value_name=\"volume\")\n",
    "        # Merge close and volume long tables on (date, ticker).\n",
    "        out = long_close.merge(long_vol, on=[\"date\",\"ticker\"], how=\"inner\").dropna(subset=[\"close\"])\n",
    "        # Cache to parquet.\n",
    "        out.to_parquet(cache_path)\n",
    "        # Return the final long-format DataFrame.\n",
    "        return out\n",
    "    except Exception:\n",
    "        # If anything fails, return an empty DataFrame with expected columns.\n",
    "        return pd.DataFrame(columns=[\"date\",\"ticker\",\"close\",\"volume\"])\n",
    "\n",
    "def safe_head(df: pd.DataFrame, n: int = 5) -> pd.DataFrame:\n",
    "    # If df is a DataFrame, return df.head(n); otherwise return an empty DataFrame.\n",
    "    return df.head(n) if isinstance(df, pd.DataFrame) else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3e812",
   "metadata": {},
   "source": [
    "## 3.2. <a id='series'>Pandas Series</a>\n",
    "\n",
    "We will use:\n",
    "- **BCRPData API**: daily PEN/USD exchange rate (buy/sell)\n",
    "- **Yahoo Finance** via `yfinance`: close/volume for US tickers\n",
    "\n",
    "Data sources:\n",
    "- BCRP API help: https://estadisticas.bcrp.gob.pe/estadisticas/series/ayuda/api\n",
    "- yfinance: https://ranaroussi.github.io/yfinance/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b68879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((584, 3),\n",
       " (4970, 4),\n",
       "         date  PEN_USD_buy  PEN_USD_sell\n",
       " 0 2022-02-01     3.871333      3.877667\n",
       " 1 2022-02-02     3.852000      3.857000\n",
       " 2 2022-02-03     3.858500      3.860833\n",
       " 3 2022-02-04     3.863000      3.867833\n",
       " 4 2022-02-07     3.838500      3.845833,\n",
       "         date ticker      close    volume\n",
       " 0 2022-01-03    EEM  44.624966  27572700\n",
       " 1 2022-01-04    EEM  44.470776  24579500\n",
       " 2 2022-01-05    EEM  43.745167  46425100\n",
       " 3 2022-01-06    EEM  43.944710  34288700\n",
       " 4 2022-01-07    EEM  44.343792  32640900)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START = \"2022-01-01\"\n",
    "END = \"2025-12-18\"\n",
    "\n",
    "# BCRP: daily USD/PEN buy & sell\n",
    "fx = bcrp_get_cached_or_empty([\"PD04637PD\",\"PD04638PD\"], start=START, end=END).rename(\n",
    "    columns={\"PD04637PD\":\"PEN_USD_buy\", \"PD04638PD\":\"PEN_USD_sell\"}\n",
    ")\n",
    "\n",
    "# Yahoo Finance: long-format table (date, ticker, close, volume)\n",
    "tickers = [\"SPY\", \"QQQ\", \"TLT\", \"GLD\", \"EEM\"]\n",
    "us_mkt = yf_download_close_volume(tickers, start=START, end=END)\n",
    "\n",
    "fx.shape, us_mkt.shape, safe_head(fx), safe_head(us_mkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0427a811",
   "metadata": {},
   "source": [
    "### 3.2.1. <a id='3.2.1'>From `lists` to `Series`</a>\n",
    "\n",
    "**Assignment:** create a Series from a Python list using FX mid-rate.\n",
    "\n",
    "1. Create `PENUSD_mid = (buy + sell)/2`.\n",
    "2. Take the **last 15 values** as a Python list.\n",
    "3. Build a `pd.Series` with those values (index can be 0..14).\n",
    "4. Name the Series `PENUSD_mid_last15`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (students): fill this block\n",
    "PENUSD_mid_last15 = pd.Series(dtype=float)\n",
    "\n",
    "# Optional self-check (runs only if you filled it)\n",
    "if len(PENUSD_mid_last15) > 0:\n",
    "    print(PENUSD_mid_last15.name, PENUSD_mid_last15.shape)\n",
    "    print(PENUSD_mid_last15.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100847d",
   "metadata": {},
   "source": [
    "### 3.2.2. <a id='3.2.2'>From `NumPy array` to `Series`</a>\n",
    "\n",
    "**Assignment:** create a Series from a NumPy array using US market close prices.\n",
    "\n",
    "1. Filter `us_mkt` for ticker `SPY`.\n",
    "2. Extract the `close` column as a NumPy array.\n",
    "3. Build a `pd.Series` with:\n",
    "   - data = the NumPy array\n",
    "   - index = the corresponding dates\n",
    "4. Compute `mean`, `min`, `max` using Series methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f394a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (students): fill this block\n",
    "SPY_close_series = pd.Series(dtype=float)\n",
    "\n",
    "summary_stats = {\"mean\": np.nan, \"min\": np.nan, \"max\": np.nan}\n",
    "\n",
    "# Optional self-check\n",
    "if len(SPY_close_series) > 0:\n",
    "    print(SPY_close_series.head())\n",
    "    print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec93acb",
   "metadata": {},
   "source": [
    "### 3.2.3. <a id='3.2.3'>From `Dictionary` to `Series`</a>\n",
    "\n",
    "**Assignment:** build a dict and convert to a Series.\n",
    "\n",
    "1. Using `us_mkt`, compute the **last available close** for each ticker in `tickers`.\n",
    "2. Store results in a dict: `{ticker: last_close}`.\n",
    "3. Convert to a Series and sort descending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d001ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (students): fill this block\n",
    "last_close_by_ticker = {}  # dict: ticker -> last close\n",
    "last_close_series = pd.Series(last_close_by_ticker, dtype=float).sort_values(ascending=False)\n",
    "\n",
    "# Optional self-check\n",
    "if len(last_close_series) > 0:\n",
    "    print(last_close_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1c3de",
   "metadata": {},
   "source": [
    "### 3.2.4. <a id='3.2.4'>`Series` vs `NumPy`</a>\n",
    "\n",
    "**Assignment:** show why alignment matters.\n",
    "\n",
    "1. Create two Series:\n",
    "   - `fx_mid`: FX mid-rate indexed by date\n",
    "   - `spy_close`: SPY close indexed by date\n",
    "2. Create a DataFrame by combining them (pandas aligns on dates).\n",
    "3. Separately, create two NumPy arrays of the same length by truncating to the same number of rows.\n",
    "4. Explain in markdown why pandas alignment is safer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e84680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (students): fill this block\n",
    "aligned_df = pd.DataFrame()\n",
    "\n",
    "fx_np = np.array([])\n",
    "spy_np = np.array([])\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(aligned_df, pd.DataFrame) and aligned_df.shape[0] > 0:\n",
    "    print(aligned_df.head())\n",
    "    print(\"NumPy shapes:\", fx_np.shape, spy_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02fa6f",
   "metadata": {},
   "source": [
    "### 3.2.5. <a id='3.2.5'>Indexing</a>\n",
    "\n",
    "**Assignment:** practice `.loc` and `.iloc`.\n",
    "\n",
    "1. From `last_close_series`, use `.iloc` to take the top 3 tickers.\n",
    "2. Use `.loc` to select the value for `SPY`.\n",
    "3. If `SPY` is not present, explain why (in markdown).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c656de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY value: nan\n"
     ]
    }
   ],
   "source": [
    "# TODO (students): fill this block\n",
    "top3 = pd.Series(dtype=float)\n",
    "spy_value = np.nan\n",
    "\n",
    "# Optional self-check\n",
    "if len(top3) > 0:\n",
    "    print(top3)\n",
    "print(\"SPY value:\", spy_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8d503",
   "metadata": {},
   "source": [
    "## 3.3. <a id='3.3'>DataFrame</a>\n",
    "\n",
    "We now practice DataFrame creation and common methods using the same datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799d65a",
   "metadata": {},
   "source": [
    "### 3.3.1. <a id='3.3.1'>DataFrame Generation</a>\n",
    "\n",
    "#### From `lists` and `dict` to `DataFrame`\n",
    "\n",
    "**Assignment:** create a DataFrame of ticker metadata.\n",
    "\n",
    "1. Make a list of tickers.\n",
    "2. Make a list of last closes (same order).\n",
    "3. Make a dict for an extra column, e.g. `{ticker: 'US'}`.\n",
    "4. Build a DataFrame with columns: `ticker`, `last_close`, `market`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (students): fill this block\n",
    "tickers_list = []\n",
    "last_close_list = []\n",
    "market_dict = {}\n",
    "\n",
    "ticker_df = pd.DataFrame()\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(ticker_df, pd.DataFrame) and ticker_df.shape[0] > 0:\n",
    "    print(ticker_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed4e31",
   "metadata": {},
   "source": [
    "#### From `lists` and `NumPy` to `DataFrame`\n",
    "\n",
    "**Assignment:** build a DataFrame from NumPy arrays.\n",
    "\n",
    "1. Take the `close` column for `SPY` and `QQQ` from `us_mkt`.\n",
    "2. Convert each to a NumPy array.\n",
    "3. Build a DataFrame with 2 columns: `SPY_close`, `QQQ_close`.\n",
    "4. Add a column with row index (0..n-1) named `t`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15813335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (students): fill this block\n",
    "prices_np_df = pd.DataFrame()\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(prices_np_df, pd.DataFrame) and prices_np_df.shape[0] > 0:\n",
    "    print(prices_np_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd1ec2",
   "metadata": {},
   "source": [
    "### 3.3.2. <a id='3.3.2'>Indexing</a>\n",
    "\n",
    "**Assignment:** `.loc` and `.iloc` on DataFrames.\n",
    "\n",
    "1. Use `.iloc` to take first 5 rows of `us_mkt`.\n",
    "2. Use `.loc` with a boolean condition to keep only rows where `ticker == 'SPY'`.\n",
    "3. Select only the columns `date`, `ticker`, `close`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4adde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (students): fill this block\n",
    "first5 = pd.DataFrame()\n",
    "only_spy = pd.DataFrame()\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(only_spy, pd.DataFrame) and only_spy.shape[0] > 0:\n",
    "    print(only_spy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3a27c",
   "metadata": {},
   "source": [
    "### 3.3.3. <a id='3.3.3'>General Methods</a>\n",
    "\n",
    "**Assignment:** basic methods: `.shape`, `.columns`, `.info`, `.describe`, `.sort_values`.\n",
    "\n",
    "1. Show `us_mkt.shape` and `us_mkt.columns`.\n",
    "2. Use `.describe()` on `close` and `volume`.\n",
    "3. Sort `us_mkt` by `volume` descending and keep top 10 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da61ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (students): fill this block\n",
    "desc = pd.DataFrame()\n",
    "top10_volume = pd.DataFrame()\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(top10_volume, pd.DataFrame) and top10_volume.shape[0] > 0:\n",
    "    print(top10_volume.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5dd1b9",
   "metadata": {},
   "source": [
    "### 3.3.4. <a id='3.3.4'>Importing Data</a>\n",
    "\n",
    "**Assignment:** `to_csv` + `read_csv` using real data.\n",
    "\n",
    "1. Save a subset of `us_mkt` to `data/us_mkt_sample.csv` (e.g., 500 rows).\n",
    "2. Read it back using `pd.read_csv`.\n",
    "3. Rename columns to snake_case.\n",
    "4. Check dtypes and missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658402d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "# TODO (students): fill this block\n",
    "us_sample = pd.DataFrame()\n",
    "us_from_csv = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b051852",
   "metadata": {},
   "source": [
    "### 3.3.5. <a id='3.3.5'>Filtering data</a>\n",
    "\n",
    "**Assignment:** filtering with conditions.\n",
    "\n",
    "1. Filter `us_mkt` for rows where `close` is above the 90th percentile **within each ticker**.\n",
    "2. Filter rows with `volume` missing (if any) and count them.\n",
    "3. Create a filtered DataFrame for tickers `['SPY','GLD']` only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (students): fill this block\n",
    "high_close = pd.DataFrame()\n",
    "missing_volume = pd.DataFrame()\n",
    "spy_gld = pd.DataFrame()\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(spy_gld, pd.DataFrame) and spy_gld.shape[0] > 0:\n",
    "    print(spy_gld.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6b93b",
   "metadata": {},
   "source": [
    "### 3.3.6. <a id='3.3.6'>Dealing with nulls</a>\n",
    "\n",
    "**Assignment:** introduce NaNs and handle them.\n",
    "\n",
    "1. Copy `us_mkt` to `us_mkt_nan`.\n",
    "2. Set 1% of the `close` values to NaN (fixed random seed).\n",
    "3. Create two cleaned versions:\n",
    "   - dropped NaNs\n",
    "   - filled NaNs with the ticker-specific median close\n",
    "4. Compare shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cbc4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (4970, 4) With NaNs: (4970, 4)\n",
      "Drop: (0, 0) Fill: (0, 0)\n"
     ]
    }
   ],
   "source": [
    "# TODO (students): fill this block\n",
    "us_mkt_nan = us_mkt.copy()\n",
    "us_drop = pd.DataFrame()\n",
    "us_fill = pd.DataFrame()\n",
    "\n",
    "# Optional self-check (does not error even if you keep defaults)\n",
    "print(\"Original:\", us_mkt.shape, \"With NaNs:\", us_mkt_nan.shape)\n",
    "print(\"Drop:\", us_drop.shape, \"Fill:\", us_fill.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c49c5",
   "metadata": {},
   "source": [
    "### 3.3.7. <a id='3.3.7'>Duplicates</a>\n",
    "\n",
    "**Assignment:** create duplicates and remove them.\n",
    "\n",
    "1. Create `dup_df` by stacking the last 5 rows of `us_mkt` twice.\n",
    "2. Use `.duplicated()` to detect duplicates.\n",
    "3. Use `.drop_duplicates()` to remove duplicates.\n",
    "4. Verify row counts before/after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e8b1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dup_df rows: 0\n",
      "dedup_df rows: 0\n"
     ]
    }
   ],
   "source": [
    "# TODO (students): fill this block\n",
    "dup_df = pd.DataFrame()\n",
    "dup_mask = pd.Series(dtype=bool)\n",
    "dedup_df = pd.DataFrame()\n",
    "\n",
    "# Optional self-check\n",
    "print(\"dup_df rows:\", getattr(dup_df, \"shape\", (0,0))[0])\n",
    "print(\"dedup_df rows:\", getattr(dedup_df, \"shape\", (0,0))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba91b8dd",
   "metadata": {},
   "source": [
    "### 3.3.8. <a id='3.3.8'>Groupby</a>\n",
    "\n",
    "**Assignment:** groupby + aggregation.\n",
    "\n",
    "1. Group `us_mkt` by `ticker` and compute:\n",
    "   - mean close\n",
    "   - median close\n",
    "   - max volume\n",
    "2. Rename the resulting columns clearly.\n",
    "3. Sort by mean close descending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (students): fill this block\n",
    "ticker_summary = pd.DataFrame()\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(ticker_summary, pd.DataFrame) and ticker_summary.shape[0] > 0:\n",
    "    print(ticker_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196dffb",
   "metadata": {},
   "source": [
    "### 3.3.9. <a id='3.3.9'>Reshape</a>\n",
    "\n",
    "##### From Wide to Long\n",
    "\n",
    "**Assignment:** melt a wide table.\n",
    "\n",
    "1. Create a small wide DataFrame with 1 row containing last closes for each ticker.\n",
    "2. Melt it to long format with columns: `ticker`, `last_close`.\n",
    "\n",
    "##### From Long to Wide\n",
    "\n",
    "**Assignment:** pivot back.\n",
    "\n",
    "3. Using `us_mkt`, create a pivot table with:\n",
    "   - index = `date`\n",
    "   - columns = `ticker`\n",
    "   - values = `close`\n",
    "4. Keep only the first 50 dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda69690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (students): fill this block\n",
    "wide_last = pd.DataFrame()\n",
    "long_last = pd.DataFrame()\n",
    "\n",
    "wide_close = pd.DataFrame()\n",
    "\n",
    "# Optional self-check\n",
    "if isinstance(long_last, pd.DataFrame) and long_last.shape[0] > 0:\n",
    "    print(long_last.head())\n",
    "if isinstance(wide_close, pd.DataFrame) and wide_close.shape[0] > 0:\n",
    "    print(wide_close.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2170e6e",
   "metadata": {},
   "source": [
    "### 3.3.10. <a id='3.3.10'>Merge</a>\n",
    "\n",
    "**Assignment:** merge Peru macro data (BCRP) with US market data (Yahoo).\n",
    "\n",
    "1. Fetch BCRP monthly policy rate: `PD12301MD`.\n",
    "2. Create a monthly table from US market data by extracting `year` and `month` from the `date` column.\n",
    "   Hint: you can use `pd.to_datetime` **only here**.\n",
    "3. Compute the monthly average close for SPY.\n",
    "4. Merge policy rate with monthly SPY average using `merge`.\n",
    "5. Save to `outputs/lecture2_policy_spy_monthly.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb87fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = bcrp_get_cached_or_empty(\"PD12301MD\", start=START, end=END).rename(columns={\"PD12301MD\":\"policy_rate\"})\n",
    "\n",
    "from pathlib import Path\n",
    "Path(\"outputs\").mkdir(exist_ok=True)\n",
    "\n",
    "# TODO (students): fill this block\n",
    "spy_monthly = pd.DataFrame()\n",
    "policy_monthly = pd.DataFrame()\n",
    "merged_monthly = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71115b2",
   "metadata": {},
   "source": [
    "## Homework (assigned)\n",
    "Deliver ONE notebook that includes:\n",
    "\n",
    "1. **Series tasks** completed (3.2.1 to 3.2.5) with short markdown explanations.\n",
    "2. A **ticker summary table** (3.3.8) and a short interpretation (3–5 bullet points).\n",
    "3. A **reshape demonstration** (3.3.9): wide → long and long → wide.\n",
    "4. The **monthly merge** between BCRP policy rate and SPY monthly average (3.3.10), saved to CSV.\n",
    "5. A section titled **\"Data Sources\"** explaining what BCRP and Yahoo Finance provide.\n",
    "\n",
    "Submit with clean outputs, and do not introduce extra topics not covered in the original lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16669b3",
   "metadata": {},
   "source": [
    "## 3.4. <a id='3.4'>References</a>\n",
    "\n",
    "- Pandas Series: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html\n",
    "- Pandas DataFrame: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\n",
    "- Pandas melt: https://pandas.pydata.org/docs/reference/api/pandas.melt.html\n",
    "- Pandas pivot_table: https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html\n",
    "- Pandas merge: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html\n",
    "- BCRP API help: https://estadisticas.bcrp.gob.pe/estadisticas/series/ayuda/api\n",
    "- yfinance: https://ranaroussi.github.io/yfinance/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ra_task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
