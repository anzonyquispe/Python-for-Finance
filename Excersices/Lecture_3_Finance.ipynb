{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7786de6",
   "metadata": {},
   "source": [
    "# 4. If and Loops — Finance Practice (Assignments Only)\n",
    "\n",
    "This notebook follows the **same topics and order** as the original Lecture 3 (`If and Loops`). All tasks use **real financial/economic data** from Peru and the US.\n",
    "\n",
    "**Student rule:** fill only the `TODO` blocks. Do *not* paste full solutions from elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674befd",
   "metadata": {},
   "source": [
    "## Data sources used\n",
    "\n",
    "- **BCRPData (Peru, official)**: BCRP statistical series API.\n",
    "  - API guide: https://estadisticas.bcrp.gob.pe/estadisticas/series/ayuda/api\n",
    "  - Example daily series list includes BVL indexes and FX.\n",
    "- **Yahoo Finance (US market data)** via `yfinance` (community wrapper for Yahoo Finance).\n",
    "  - Docs: https://ranaroussi.github.io/yfinance/\n",
    "- **SEC EDGAR Data APIs (US, official)**: company facts JSON.\n",
    "  - Docs: https://www.sec.gov/search-filings/edgar-application-programming-interfaces\n",
    "\n",
    "If any download fails (internet/firewall), this notebook is designed to **keep running** with empty data, so you can still practice control flow syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "9f34032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "0691b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CACHE_DIR = Path(\".cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "_ES_TO_EN_MONTH = {\n",
    "    \"Ene\": \"Jan\", \"Feb\": \"Feb\", \"Mar\": \"Mar\", \"Abr\": \"Apr\", \"May\": \"May\", \"Jun\": \"Jun\",\n",
    "    \"Jul\": \"Jul\", \"Ago\": \"Aug\", \"Set\": \"Sep\", \"Sep\": \"Sep\", \"Oct\": \"Oct\", \"Nov\": \"Nov\", \"Dic\": \"Dec\"\n",
    "}\n",
    "\n",
    "def _hash_key(*parts: str) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    for p in parts:\n",
    "        h.update(str(p).encode(\"utf-8\"))\n",
    "        h.update(b\"|\")\n",
    "    return h.hexdigest()[:24]\n",
    "\n",
    "def _normalize_period(code: str, period: str | None) -> str | None:\n",
    "    if period is None:\n",
    "        return None\n",
    "    period = str(period).strip()\n",
    "    freq = code[-2:].upper() if len(code) >= 2 else \"\"\n",
    "\n",
    "    if freq == \"PD\":  # daily\n",
    "        if re.fullmatch(r\"\\d{4}-\\d{1,2}\", period):\n",
    "            y, m = period.split(\"-\")\n",
    "            return f\"{int(y):04d}-{int(m):02d}-01\"\n",
    "        if re.fullmatch(r\"\\d{4}\", period):\n",
    "            return f\"{int(period):04d}-01-01\"\n",
    "        return period\n",
    "\n",
    "    if freq == \"PM\":  # monthly\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{1,2})-(\\d{1,2})\", period)\n",
    "        if m:\n",
    "            y, mo, _ = m.groups()\n",
    "            return f\"{int(y):04d}-{int(mo)}\"\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{1,2})\", period)\n",
    "        if m:\n",
    "            y, mo = m.groups()\n",
    "            return f\"{int(y):04d}-{int(mo)}\"\n",
    "        if re.fullmatch(r\"\\d{4}\", period):\n",
    "            return f\"{int(period):04d}-1\"\n",
    "        return period\n",
    "\n",
    "    if freq == \"MD\":  # daily index\n",
    "        return period\n",
    "\n",
    "    return period\n",
    "\n",
    "def _parse_bcrp_period_name(name: str) -> pd.Timestamp:\n",
    "    s = str(name).strip()\n",
    "\n",
    "    # ISO-ish\n",
    "    try:\n",
    "        if re.fullmatch(r\"\\d{4}(-\\d{1,2}){0,2}\", s):\n",
    "            return pd.to_datetime(s, errors=\"raise\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Monthly like \"Mar.2020\"\n",
    "    m = re.fullmatch(r\"([A-Za-zÁÉÍÓÚÑñ]{3})\\.(\\d{4})\", s)\n",
    "    if m:\n",
    "        mon_es, y = m.groups()\n",
    "        mon = _ES_TO_EN_MONTH.get(mon_es[:3], mon_es[:3])\n",
    "        return pd.to_datetime(f\"{mon}.{y}\", format=\"%b.%Y\", errors=\"coerce\")\n",
    "\n",
    "    # Daily like \"18Nov25\" or \"02Ene97\"\n",
    "    m = re.fullmatch(r\"(\\d{2})([A-Za-zÁÉÍÓÚÑñ]{3})(\\d{2})\", s)\n",
    "    if m:\n",
    "        d, mon_es, yy = m.groups()\n",
    "        mon = _ES_TO_EN_MONTH.get(mon_es[:3], mon_es[:3])\n",
    "        year = 2000 + int(yy) if int(yy) <= 69 else 1900 + int(yy)\n",
    "        return pd.to_datetime(f\"{d}{mon}{year}\", format=\"%d%b%Y\", errors=\"coerce\")\n",
    "\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def bcrp_get(series_codes, start: str | None = None, end: str | None = None, lang: str = \"esp\") -> pd.DataFrame:\n",
    "    \"\"\"Fetch BCRPData series (official API) into a DataFrame.\n",
    "\n",
    "    Returns columns: [\"date\", <code1>, <code2>, ...]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import requests\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if isinstance(series_codes, (list, tuple)):\n",
    "        codes_list = [str(c).strip() for c in series_codes]\n",
    "        codes = \"-\".join(codes_list)\n",
    "        freq_code = codes_list[0]\n",
    "    else:\n",
    "        codes = str(series_codes).strip()\n",
    "        codes_list = codes.split(\"-\")\n",
    "        freq_code = codes_list[0]\n",
    "\n",
    "    start_n = _normalize_period(freq_code, start)\n",
    "    end_n = _normalize_period(freq_code, end)\n",
    "\n",
    "    key = _hash_key(\"bcrp\", codes, start_n or \"\", end_n or \"\", lang)\n",
    "    cache_path = CACHE_DIR / f\"bcrp_{key}.pkl\"\n",
    "    if cache_path.exists():\n",
    "        return pd.read_pickle(cache_path)\n",
    "\n",
    "    base_url = \"https://estadisticas.bcrp.gob.pe/estadisticas/series/api\"\n",
    "    parts = [base_url, codes, \"json\"]\n",
    "    if start_n and end_n:\n",
    "        parts += [start_n, end_n]\n",
    "    if lang:\n",
    "        parts += [lang]\n",
    "    url = \"/\".join(parts)\n",
    "\n",
    "    r = requests.get(url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    obj = r.json()\n",
    "\n",
    "    periods = obj.get(\"periods\", [])\n",
    "    rows = []\n",
    "    for p in periods:\n",
    "        name = p.get(\"name\")\n",
    "        vals = p.get(\"values\", [])\n",
    "        if isinstance(vals, str):\n",
    "            vals = [vals]\n",
    "        if name is None or not isinstance(vals, list):\n",
    "            continue\n",
    "        vals = (vals + [None] * len(codes_list))[:len(codes_list)]\n",
    "        rows.append([name] + vals)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"date\"] + codes_list)\n",
    "    if df.shape[0] == 0:\n",
    "        return pd.DataFrame(columns=[\"date\"] + codes_list)\n",
    "\n",
    "    df[\"date\"] = df[\"date\"].apply(_parse_bcrp_period_name)\n",
    "    for c in codes_list:\n",
    "        df[c] = df[c].replace({\"n.d.\": np.nan, \"nd\": np.nan, \"N.D.\": np.nan})\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "    df.to_pickle(cache_path)\n",
    "    return df\n",
    "\n",
    "def bcrp_get_cached_or_empty(series_codes, start: str, end: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return bcrp_get(series_codes, start=start, end=end)\n",
    "    except Exception:\n",
    "        if isinstance(series_codes, (list, tuple)):\n",
    "            codes_list = [str(c).strip() for c in series_codes]\n",
    "        else:\n",
    "            codes_list = [str(series_codes).strip()]\n",
    "        return pd.DataFrame(columns=[\"date\"] + codes_list)\n",
    "\n",
    "def yf_download_wide(tickers, start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"Download Close and Volume using yfinance.\n",
    "\n",
    "    Returns a DataFrame indexed by date with columns:\n",
    "      Close_<TICKER>, Volume_<TICKER>\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import yfinance as yf\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    cols = tickers if isinstance(tickers, (list, tuple)) else [tickers]\n",
    "    key = _hash_key(\"yf_wide\", \",\".join(cols), start, end)\n",
    "    cache_path = CACHE_DIR / f\"yf_wide_{key}.pkl\"\n",
    "    if cache_path.exists():\n",
    "        return pd.read_pickle(cache_path)\n",
    "\n",
    "    try:\n",
    "        data = yf.download(cols, start=start, end=end, auto_adjust=True, progress=False)\n",
    "        if data.empty:\n",
    "            return pd.DataFrame()\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            close = data[\"Close\"].copy()\n",
    "            vol = data[\"Volume\"].copy()\n",
    "        else:\n",
    "            close = data[[\"Close\"]].rename(columns={\"Close\": cols[0]})\n",
    "            vol = data[[\"Volume\"]].rename(columns={\"Volume\": cols[0]})\n",
    "        close = close.add_prefix(\"Close_\")\n",
    "        vol = vol.add_prefix(\"Volume_\")\n",
    "        out = close.join(vol, how=\"outer\")\n",
    "        out.index.name = \"date\"\n",
    "        out.to_pickle(cache_path)\n",
    "        return out\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def sec_companyfacts(cik10: str, user_agent: str) -> dict:\n",
    "    \"\"\"Fetch SEC company facts (official EDGAR Data APIs).\n",
    "\n",
    "    Endpoint:\n",
    "      https://data.sec.gov/api/xbrl/companyfacts/CIK##########.json\n",
    "\n",
    "    The SEC requires a User-Agent identifying the requester.\n",
    "\n",
    "    Returns {} on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import requests, json\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "    cik10 = str(cik10).zfill(10)\n",
    "    key = _hash_key(\"sec_companyfacts\", cik10)\n",
    "    cache_path = CACHE_DIR / f\"sec_companyfacts_{key}.json\"\n",
    "    if cache_path.exists():\n",
    "        try:\n",
    "            return json.loads(cache_path.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik10}.json\"\n",
    "    headers = {\"User-Agent\": user_agent, \"Accept-Encoding\": \"gzip, deflate\", \"Host\": \"data.sec.gov\"}\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        obj = r.json()\n",
    "        try:\n",
    "            cache_path.write_text(json.dumps(obj), encoding=\"utf-8\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        return obj\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def safe_head(df: pd.DataFrame, n: int = 5) -> pd.DataFrame:\n",
    "    return df.head(n) if isinstance(df, pd.DataFrame) else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e95b6",
   "metadata": {},
   "source": [
    "### Load datasets (used across exercises)\n",
    "\n",
    "We load:\n",
    "- Peru: **BVL General Index** (BCRP series `PD38026MD`) and **CPI Non-Tradables** (monthly, `PN01282PM`).\n",
    "- US: prices/volumes for selected tickers via `yfinance`.\n",
    "- US fundamentals: SEC company facts (optional) for Apple (CIK 0000320193).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "9b53a07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BVL: (729, 2) CPI NT: (59, 2) US: (1246, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(        date  BVL_index\n",
       " 0 2021-02-01   21610.03\n",
       " 1 2021-02-02   21451.89\n",
       " 2 2021-02-03   21632.78\n",
       " 3 2021-02-04   21552.20\n",
       " 4 2021-02-05   21783.37,\n",
       "         date  CPI_non_tradables\n",
       " 0 2021-01-01           0.832155\n",
       " 1 2021-02-01          -0.424668\n",
       " 2 2021-03-01           1.026397\n",
       " 3 2021-04-01          -0.361899\n",
       " 4 2021-05-01           0.158005,\n",
       " Ticker      Close_AAPL   Close_JPM  Close_MSFT  Close_NVDA   Close_SPY  \\\n",
       " date                                                                     \n",
       " 2021-01-04  125.974487  110.548325  208.882187   13.076726  345.273926   \n",
       " 2021-01-05  127.531975  111.149849  209.083710   13.367159  347.651978   \n",
       " 2021-01-06  123.239067  116.369003  203.662308   12.579123  349.730438   \n",
       " 2021-01-07  127.444382  120.190437  209.457947   13.306580  354.926575   \n",
       " 2021-01-08  128.544388  120.323151  210.734116   13.239517  356.948822   \n",
       " \n",
       " Ticker      Volume_AAPL  Volume_JPM  Volume_MSFT  Volume_NVDA  Volume_SPY  \n",
       " date                                                                       \n",
       " 2021-01-04    143301900    16819900     37130100    560640000   110210800  \n",
       " 2021-01-05     97664900    13731200     23823000    322760000    66426200  \n",
       " 2021-01-06    155088000    24909100     35930700    580424000   107997700  \n",
       " 2021-01-07    109578200    21940400     27694500    461480000    68766800  \n",
       " 2021-01-08    105158200    12035100     22956200    292528000    71677200  )"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START = \"2021-01-01\"\n",
    "END = \"2025-12-18\"\n",
    "\n",
    "# Peru — BCRPData (official)\n",
    "bvl = bcrp_get_cached_or_empty(\"PD38026MD\", start=START, end=END).rename(columns={\"PD38026MD\":\"BVL_index\"})\n",
    "cpi_nt = bcrp_get_cached_or_empty(\"PN01282PM\", start=START, end=END).rename(columns={\"PN01282PM\":\"CPI_non_tradables\"})\n",
    "\n",
    "# US — Yahoo Finance (real market data)\n",
    "tickers = [\"SPY\", \"AAPL\", \"MSFT\", \"JPM\", \"NVDA\"]\n",
    "us = yf_download_wide(tickers, start=START, end=END)\n",
    "\n",
    "print(\"BVL:\", bvl.shape, \"CPI NT:\", cpi_nt.shape, \"US:\", us.shape)\n",
    "safe_head(bvl), safe_head(cpi_nt), safe_head(us)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15619d1c",
   "metadata": {},
   "source": [
    "# 4.1. <a id='def'> If condition </a>\n",
    "\n",
    "In this section you will practice `if / elif / else` using real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7c0dc",
   "metadata": {},
   "source": [
    "## 4.1.1.  <a id='4.1.1.'> The structure of the code </a>\n",
    "\n",
    "**Exercise 4.1.1 — Classify a day (single `if`)**\n",
    "\n",
    "Using the **BVL index**:\n",
    "1. Compute daily returns: `ret = pct_change()`.\n",
    "2. Take the most recent non-missing return.\n",
    "3. Write a single `if` statement that prints a message only when the last return is **positive**.\n",
    "\n",
    "Keep it minimal: one `if`, one `print`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "d348d456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BVL last return is positive: 0.01196384727960198\n"
     ]
    }
   ],
   "source": [
    "# Solution (Exercise 4.1.1)\n",
    "# bvl is expected to have columns: [\"date\", \"BVL_index\"]\n",
    "\n",
    "last_ret = np.nan  # We start with \"no value yet\", just in case something goes wrong.\n",
    "\n",
    "try:\n",
    "    # We treat the BVL index as a time series:\n",
    "    # - set_index(\"date\") makes the dates the index (so pandas knows the timeline)\n",
    "    # - [\"BVL_index\"] selects the index level/price series\n",
    "    # - pct_change() computes the day-to-day return:\n",
    "    #     return_t = (BVL_t / BVL_{t-1}) - 1\n",
    "    ret = bvl.set_index(\"date\")[\"BVL_index\"].pct_change()\n",
    "\n",
    "    # The first return is usually missing (because there is no previous day),\n",
    "    # so we drop missing values and then take the last available return.\n",
    "    last_ret = float(ret.dropna().iloc[-1])\n",
    "except Exception:\n",
    "    # If bvl is empty, the column is missing, or the format is not correct,\n",
    "    # we simply keep last_ret as NaN and move on.\n",
    "    pass\n",
    "\n",
    "# The exercise asks for ONE if-statement that prints ONLY when the last return is positive.\n",
    "# So: if we successfully computed a number AND it is greater than 0, we print the message.\n",
    "if pd.notna(last_ret) and last_ret > 0:\n",
    "    print(\"BVL last return is positive:\", last_ret)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae000aba",
   "metadata": {},
   "source": [
    "### 4.1.2.   <a id='4.1.2.'> If condition with more than 1 expression </a>\n",
    "\n",
    "**Exercise 4.1.2 — Two conditions with `and` / `or`**\n",
    "\n",
    "Using US data (`us`):\n",
    "1. Compute daily returns for `SPY` from `Close_SPY`.\n",
    "2. Define a threshold, e.g. `thr = 0.01` (1%).\n",
    "3. Create two booleans for the last available day:\n",
    "   - `big_move`: `abs(return) > thr`\n",
    "   - `high_volume`: `Volume_SPY` above its median\n",
    "4. Use `if / elif / else` to print one of three messages:\n",
    "   - big move AND high volume\n",
    "   - big move BUT not high volume\n",
    "   - not a big move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "de06b29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY: big move AND high volume\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Solution (Exercise 4.1.2)\n",
    "# We use the US market dataset `us`, which should contain:\n",
    "# - Close_SPY  : SPY adjusted close\n",
    "# - Volume_SPY : SPY trading volume\n",
    "#\n",
    "\n",
    "thr = 0.01  # 1% threshold for a \"big\" daily move\n",
    "\n",
    "# Default values (safe fallback if something is missing)\n",
    "big_move = False\n",
    "high_volume = False\n",
    "last_spy_ret = np.nan\n",
    "last_spy_vol = np.nan\n",
    "\n",
    "# First, make sure we actually have a DataFrame with the needed SPY columns\n",
    "if isinstance(us, pd.DataFrame) and (not us.empty) and (\"Close_SPY\" in us.columns):\n",
    "    # Compute daily returns: (today / yesterday) - 1\n",
    "    spy_ret = us[\"Close_SPY\"].pct_change()\n",
    "\n",
    "    # Take the most recent non-missing return\n",
    "    if spy_ret.dropna().shape[0] > 0:\n",
    "        last_spy_ret = float(spy_ret.dropna().iloc[-1])\n",
    "\n",
    "        # \"big_move\" is True if the absolute return is larger than the threshold\n",
    "        big_move = abs(last_spy_ret) > thr\n",
    "\n",
    "    # Now check volume: we need Volume_SPY to exist and have data\n",
    "    if \"Volume_SPY\" in us.columns and us[\"Volume_SPY\"].dropna().shape[0] > 0:\n",
    "        # Last observed volume\n",
    "        last_spy_vol = float(us[\"Volume_SPY\"].dropna().iloc[-1])\n",
    "\n",
    "        # Median volume over the sample (typical volume level)\n",
    "        vol_median = float(us[\"Volume_SPY\"].median(skipna=True))\n",
    "\n",
    "        # \"high_volume\" is True if the last volume is above the median\n",
    "        high_volume = last_spy_vol > vol_median\n",
    "\n",
    "if big_move and high_volume:\n",
    "    print(\"SPY: big move AND high volume\")\n",
    "elif big_move and (not high_volume):\n",
    "    print(\"SPY: big move BUT not high volume\")\n",
    "else:\n",
    "    print(\"SPY: not a big move\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d8fd9",
   "metadata": {},
   "source": [
    "### 4.1.3.   <a id='4.1.3.'> Logical Operators </a>\n",
    "\n",
    "**Exercise 4.1.3 — Filter with logical operators**\n",
    "\n",
    "Using CPI non-tradables (`cpi_nt`):\n",
    "1. Compute monthly inflation as `% change` of the CPI index.\n",
    "2. Create a filtered DataFrame keeping months where:\n",
    "   - inflation is positive **AND** above its own median.\n",
    "3. Create another filter where:\n",
    "   - inflation is negative **OR** missing.\n",
    "\n",
    "Store the results in `infl_above_median` and `infl_negative_or_missing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "e2b4dcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infl_above_median: (17, 3)\n",
      "infl_negative_or_missing: (42, 3)\n"
     ]
    }
   ],
   "source": [
    "# We will create two filtered DataFrames:\n",
    "# - infl_above_median: months where inflation is positive AND above its own median\n",
    "# - infl_negative_or_missing: months where inflation is negative OR missing\n",
    "\n",
    "infl_above_median = pd.DataFrame()\n",
    "infl_negative_or_missing = pd.DataFrame()\n",
    "\n",
    "# We first check that `cpi_nt` is a valid DataFrame, is not empty,\n",
    "# and contains the column we need (\"CPI_non_tradables\").\n",
    "if isinstance(cpi_nt, pd.DataFrame) and (not cpi_nt.empty) and (\"CPI_non_tradables\" in cpi_nt.columns):\n",
    "    tmp = cpi_nt.copy()\n",
    "\n",
    "    # Sort by date so pct_change() is computed in the correct chronological order\n",
    "    tmp = tmp.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # Monthly inflation in percent:\n",
    "    # pct_change() gives a decimal (e.g., 0.01 = 1%), so we multiply by 100\n",
    "    tmp[\"inflation\"] = tmp[\"CPI_non_tradables\"].pct_change() * 100\n",
    "\n",
    "    # Median inflation (ignoring missing values)\n",
    "    med = tmp[\"inflation\"].median(skipna=True)\n",
    "\n",
    "    # Filter 1: inflation > 0 AND inflation > median(inflation)\n",
    "    infl_above_median = tmp[\n",
    "        (tmp[\"inflation\"] > 0) & (tmp[\"inflation\"] > med)\n",
    "    ][[\"date\", \"CPI_non_tradables\", \"inflation\"]].reset_index(drop=True)\n",
    "\n",
    "    # Filter 2: inflation < 0 OR inflation is missing\n",
    "    infl_negative_or_missing = tmp[\n",
    "        (tmp[\"inflation\"] < 0) | (tmp[\"inflation\"].isna())\n",
    "    ][[\"date\", \"CPI_non_tradables\", \"inflation\"]].reset_index(drop=True)\n",
    "\n",
    "print(\"infl_above_median:\", getattr(infl_above_median, \"shape\", None))\n",
    "print(\"infl_negative_or_missing:\", getattr(infl_negative_or_missing, \"shape\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074b6391",
   "metadata": {},
   "source": [
    "### 4.1.4.   <a id='4.1.4.'> Python Identity Operators </a>\n",
    "\n",
    "**Exercise 4.1.4 — `==` vs `is` with missing values**\n",
    "\n",
    "Create a Python list of *observations* that mixes numbers and missing values:\n",
    "```python\n",
    "obs = [3.5, None, 3.6, np.nan, 3.55]\n",
    "```\n",
    "Tasks:\n",
    "1. Use `== None` to build a mask for `None`.\n",
    "2. Use `is None` inside a loop to count `None`.\n",
    "3. Show that `np.nan == np.nan` is `False`.\n",
    "4. Use `np.isnan` to count NaNs safely.\n",
    "\n",
    "This is useful when cleaning financial time series with missing days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "da4641ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_none: [False, True, False, False, False]\n",
      "none_count: 1\n",
      "np.nan == np.nan: False\n",
      "nan_count: 1\n"
     ]
    }
   ],
   "source": [
    "# We create a list that mixes real numbers with two kinds of \"missing\":\n",
    "# - None (Python's missing object)\n",
    "# - np.nan (a special float used to represent missing numeric data)\n",
    "\n",
    "obs = [3.5, None, 3.6, np.nan, 3.55]\n",
    "\n",
    "# 1) Build a mask for None using `== None`\n",
    "# This returns True only for elements that are equal to None.\n",
    "# (Style note: many linters prefer `is None`, but here we do it on purpose for the exercise.)\n",
    "mask_none = [(x == None) for x in obs]  # noqa: E711\n",
    "\n",
    "# 2) Count None using `is None` inside a loop\n",
    "# `is` checks identity (whether the object is literally None), which is the safest way.\n",
    "none_count = 0\n",
    "for x in obs:\n",
    "    if x is None:\n",
    "        none_count += 1\n",
    "\n",
    "# 3) Show that NaN is not equal to itself\n",
    "# This is a key property of NaN: comparisons like (nan == nan) are always False.\n",
    "nan_eq_nan = (np.nan == np.nan)\n",
    "\n",
    "# 4) Count NaNs safely using np.isnan\n",
    "# We skip None (because np.isnan(None) would error), and apply np.isnan only to floats.\n",
    "nan_count = 0\n",
    "for x in obs:\n",
    "    if x is None:\n",
    "        continue\n",
    "    try:\n",
    "        if isinstance(x, float) and np.isnan(x):\n",
    "            nan_count += 1\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Optional self-check\n",
    "print(\"mask_none:\", mask_none)\n",
    "print(\"none_count:\", none_count)\n",
    "print(\"np.nan == np.nan:\", nan_eq_nan)\n",
    "print(\"nan_count:\", nan_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69700db0",
   "metadata": {},
   "source": [
    "### 4.1.5.   <a id='4.1.5.'> Final IF condition structure </a>\n",
    "\n",
    "**Exercise 4.1.5 — Write a small decision function**\n",
    "\n",
    "Write a function `risk_label(x, low, high)`:\n",
    "- returns `'LOW'` if `x < low`\n",
    "- returns `'MEDIUM'` if `low <= x < high`\n",
    "- returns `'HIGH'` if `x >= high`\n",
    "\n",
    "Then apply it to the **last SPY daily return**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "f63f9cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_spy_ret: -0.011003536401451242 label: HIGH\n"
     ]
    }
   ],
   "source": [
    "def risk_label(x: float, low: float, high: float) -> str:\n",
    "    # Define a function that returns a label based on where x falls relative to low/high.\n",
    "    if x < low:\n",
    "        # If x is strictly smaller than low, return \"LOW\".\n",
    "        return \"LOW\"\n",
    "    elif low <= x < high:\n",
    "        # If x is between low (inclusive) and high (exclusive), return \"MEDIUM\".\n",
    "        return \"MEDIUM\"\n",
    "    else:\n",
    "        # Otherwise (x >= high), return \"HIGH\".\n",
    "        return \"HIGH\"\n",
    "\n",
    "last_spy_ret = np.nan\n",
    "# Default value in case we cannot compute the last return.\n",
    "\n",
    "label = \"NA\"\n",
    "# Default label in case we cannot assign a risk category.\n",
    "\n",
    "if isinstance(us, pd.DataFrame) and (not us.empty) and (\"Close_SPY\" in us.columns):\n",
    "    # Check that `us` is a non-empty DataFrame and contains the Close_SPY column.\n",
    "    spy_ret = us[\"Close_SPY\"].pct_change()\n",
    "    # Compute daily returns for SPY: (today / yesterday) - 1.\n",
    "    if spy_ret.dropna().shape[0] > 0:\n",
    "        # Make sure at least one non-missing return exists.\n",
    "        last_spy_ret = float(spy_ret.dropna().iloc[-1])\n",
    "        # Take the most recent non-missing return.\n",
    "        label = risk_label(abs(last_spy_ret), low=0.002, high=0.01)\n",
    "        # Use the absolute return as a simple risk proxy and map it to LOW/MEDIUM/HIGH.\n",
    "\n",
    "print(\"last_spy_ret:\", last_spy_ret, \"label:\", label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e869d",
   "metadata": {},
   "source": [
    "### 4.1.6.   <a id='4.1.6.'> Python Nested if Statement </a>\n",
    "\n",
    "**Exercise 4.1.6 — Nested if for a simple trading rule (logic only)**\n",
    "\n",
    "Using BVL daily returns:\n",
    "1. Compute `ret` and its rolling volatility proxy: rolling std over 20 days.\n",
    "2. Build a **nested if** rule:\n",
    "   - If `ret > 0`:\n",
    "       - If `volatility` is high → print `'UP but volatile'`\n",
    "       - else → print `'UP and calm'`\n",
    "   - Else:\n",
    "       - If `ret < 0` print `'DOWN'`\n",
    "       - else print `'FLAT'`\n",
    "\n",
    "Do not optimize anything; this is purely control-flow practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "fc06510b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UP but volatile\n"
     ]
    }
   ],
   "source": [
    "ret = pd.Series(dtype=float)\n",
    "# We initialize an empty Series to store daily returns (so the variable exists even if data are missing).\n",
    "\n",
    "vol20 = pd.Series(dtype=float)\n",
    "# We initialize an empty Series to store the 20-day rolling volatility proxy (rolling standard deviation).\n",
    "\n",
    "if isinstance(bvl, pd.DataFrame) and (not bvl.empty) and (\"BVL_index\" in bvl.columns):\n",
    "    # We check that `bvl` is a non-empty DataFrame and that it contains the BVL index column we need.\n",
    "\n",
    "    s = bvl.set_index(\"date\")[\"BVL_index\"].sort_index()\n",
    "    # Convert the DataFrame into a time series:\n",
    "    # - use \"date\" as the index\n",
    "    # - select the \"BVL_index\" column\n",
    "    # - sort by date to ensure chronological order\n",
    "\n",
    "    ret = s.pct_change()\n",
    "    # Compute daily returns: (today / yesterday) - 1\n",
    "\n",
    "    vol20 = ret.rolling(20).std()\n",
    "    # Compute a rolling 20-day standard deviation of returns as a simple volatility proxy.\n",
    "\n",
    "last_r = ret.dropna().iloc[-1] if ret.dropna().shape[0] > 0 else np.nan\n",
    "# Get the most recent non-missing return; if none exist, set it to NaN.\n",
    "\n",
    "last_v = vol20.dropna().iloc[-1] if vol20.dropna().shape[0] > 0 else np.nan\n",
    "# Get the most recent non-missing volatility value; if none exist, set it to NaN.\n",
    "\n",
    "vol_high = False\n",
    "# Default: assume volatility is not high (we will update this if we have enough data).\n",
    "\n",
    "if vol20.dropna().shape[0] > 0 and pd.notna(last_v):\n",
    "    # Only decide \"high volatility\" if we have a valid volatility value.\n",
    "\n",
    "    vol_high = last_v > vol20.median(skipna=True)\n",
    "    # Define \"high volatility\" as: last volatility is above the median volatility in the sample.\n",
    "\n",
    "if pd.isna(last_r):\n",
    "    # If we do not have a valid last return, we cannot apply the trading-rule logic.\n",
    "    print(\"No BVL return available.\")\n",
    "else:\n",
    "    # If we do have a valid last return, we apply the nested if rule.\n",
    "\n",
    "    if last_r > 0:\n",
    "        # First branch: the market went up.\n",
    "        if vol_high:\n",
    "            # Nested condition: it went up AND volatility is high.\n",
    "            print(\"UP but volatile\")\n",
    "        else:\n",
    "            # It went up AND volatility is not high.\n",
    "            print(\"UP and calm\")\n",
    "    else:\n",
    "        # Second branch: last_r is not > 0, so it is either negative or exactly zero.\n",
    "        if last_r < 0:\n",
    "            # If negative, print DOWN.\n",
    "            print(\"DOWN\")\n",
    "        else:\n",
    "            # Otherwise it must be zero (or extremely close to zero), so print FLAT.\n",
    "            print(\"FLAT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687d3da",
   "metadata": {},
   "source": [
    "## 4.2.   <a id='4.2.'> For Loops </a>\n",
    "\n",
    "You will practice loops over arrays, lists, dictionaries, ranges, and DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92311dd",
   "metadata": {},
   "source": [
    "### 4.2.1. <a id='4.2.1.'> In numpy </a>\n",
    "\n",
    "**Exercise 4.2.1 — Cumulative return with a loop (no vectorization)**\n",
    "\n",
    "Using SPY daily returns (from `Close_SPY`):\n",
    "1. Take the last 60 returns as a NumPy array.\n",
    "2. Using a `for` loop, compute cumulative growth starting at 1.0:\n",
    "   - update: `value *= (1 + r)`\n",
    "3. Store the cumulative values in a list `path`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "a186c711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(path): 60\n",
      "final_value: 1.0123490329911564\n"
     ]
    }
   ],
   "source": [
    "path = []\n",
    "# We will store the cumulative growth values after each day in this list.\n",
    "\n",
    "final_value = np.nan\n",
    "# Default value in case we cannot compute the cumulative return.\n",
    "\n",
    "if isinstance(us, pd.DataFrame) and (not us.empty) and (\"Close_SPY\" in us.columns):\n",
    "    # Check that `us` is a non-empty DataFrame and contains the Close_SPY column.\n",
    "\n",
    "    spy_ret = us[\"Close_SPY\"].pct_change().dropna()\n",
    "    # Compute daily returns and drop missing values (the first return is usually missing).\n",
    "\n",
    "    arr = spy_ret.to_numpy()\n",
    "    # Convert the return Series into a NumPy array.\n",
    "\n",
    "    if arr.size > 0:\n",
    "        # Make sure the array has at least one element.\n",
    "\n",
    "        arr = arr[-min(60, arr.size):]\n",
    "        # Keep only the last 60 returns (or fewer if we have less than 60).\n",
    "\n",
    "        value = 1.0\n",
    "        # Start cumulative growth at 1.0 (think of this as starting with $1).\n",
    "\n",
    "        for r in arr:\n",
    "            # Loop over each daily return r.\n",
    "\n",
    "            value *= (1 + float(r))\n",
    "            # Update the cumulative value: multiply by (1 + r).\n",
    "\n",
    "            path.append(value)\n",
    "            # Save the updated cumulative value into the path list.\n",
    "\n",
    "        final_value = float(value)\n",
    "        # After the loop ends, `value` is the final cumulative growth factor.\n",
    "print(\"len(path):\", len(path))\n",
    "print(\"final_value:\", final_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c8cdf2",
   "metadata": {},
   "source": [
    "### 4.2.2 <a id='4.2.2.'> In List </a>\n",
    "\n",
    "**Exercise 4.2.2 — Loop over tickers and compute last close**\n",
    "\n",
    "Using the `tickers` list and the `us` DataFrame:\n",
    "1. Loop over tickers.\n",
    "2. For each ticker, pick its last non-missing `Close_<T>`.\n",
    "3. Store results in a list of tuples: `(ticker, last_close)`.\n",
    "4. If a ticker column is missing, use `continue`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "ed7dd827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_close_list (first 3): [('SPY', 671.4000244140625), ('AAPL', 271.8399963378906), ('MSFT', 476.1199951171875)]\n"
     ]
    }
   ],
   "source": [
    "last_close_list = []\n",
    "# This list will store tuples of the form (ticker, last_close).\n",
    "\n",
    "if isinstance(us, pd.DataFrame) and (not us.empty):\n",
    "    # Check that `us` is a non-empty DataFrame before looping.\n",
    "\n",
    "    for t in tickers:\n",
    "        # Loop over each ticker symbol in the `tickers` list.\n",
    "\n",
    "        col = f\"Close_{t}\"\n",
    "        # Build the column name for that ticker (e.g., \"Close_SPY\").\n",
    "\n",
    "        if col not in us.columns:\n",
    "            # If the column does not exist in the DataFrame, skip this ticker.\n",
    "            continue\n",
    "\n",
    "        series = us[col].dropna()\n",
    "        # Drop missing values so we can safely take the last observed close.\n",
    "\n",
    "        if series.shape[0] == 0:\n",
    "            # If everything was missing, there is no last value to take, so skip.\n",
    "            continue\n",
    "\n",
    "        last_close = float(series.iloc[-1])\n",
    "        # Take the most recent non-missing closing price.\n",
    "\n",
    "        last_close_list.append((t, last_close))\n",
    "        # Store the result as a tuple (ticker, last_close).\n",
    "\n",
    "print(\"last_close_list (first 3):\", last_close_list[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f7f25",
   "metadata": {},
   "source": [
    "### 4.2.3  <a id='4.2.3.'>In Dictionary </a>\n",
    "\n",
    "**Exercise 4.2.3 — Build a dictionary of risk flags**\n",
    "\n",
    "1. Create a dict `risk_by_ticker = {}`.\n",
    "2. Loop over `last_close_list` (from 4.2.2).\n",
    "3. Assign `'HIGH_PRICE'` if last_close is above its own cross-ticker median, else `'LOW_PRICE'`.\n",
    "4. Store: `risk_by_ticker[ticker] = label`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "b2e15057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SPY': 'HIGH_PRICE', 'AAPL': 'LOW_PRICE', 'MSFT': 'HIGH_PRICE', 'JPM': 'LOW_PRICE', 'NVDA': 'LOW_PRICE'}\n"
     ]
    }
   ],
   "source": [
    "risk_by_ticker = {}\n",
    "# Create an empty dictionary where we will store:\n",
    "#   key   = ticker (e.g., \"SPY\")\n",
    "#   value = label (\"HIGH_PRICE\" or \"LOW_PRICE\")\n",
    "\n",
    "if isinstance(last_close_list, list) and len(last_close_list) > 0:\n",
    "    # Make sure last_close_list exists and is not empty.\n",
    "\n",
    "    vals = [v for (_, v) in last_close_list if pd.notna(v)]\n",
    "    # Extract only the non-missing last_close values into a separate list.\n",
    "\n",
    "    med = float(np.median(vals)) if len(vals) > 0 else np.nan\n",
    "    # Compute the cross-ticker median of last_close values.\n",
    "    # If vals is empty, use NaN as a fallback.\n",
    "\n",
    "    for t, v in last_close_list:\n",
    "        # Loop over each (ticker, last_close) pair.\n",
    "\n",
    "        if pd.isna(v) or pd.isna(med):\n",
    "            # If the ticker value is missing OR the median is missing, we cannot compare safely.\n",
    "            label = \"UNKNOWN\"\n",
    "        else:\n",
    "            # If we can compare, label as HIGH_PRICE if above the median, otherwise LOW_PRICE.\n",
    "            label = \"HIGH_PRICE\" if v > med else \"LOW_PRICE\"\n",
    "\n",
    "        risk_by_ticker[t] = label\n",
    "        # Store the label in the dictionary using the ticker as the key.\n",
    "\n",
    "print(risk_by_ticker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57b5b1",
   "metadata": {},
   "source": [
    "### 4.2.4 <a id = '4.2.4.'>  For loop using range </a>\n",
    "\n",
    "**Exercise 4.2.4 — Simple monthly budgeting with `range`**\n",
    "\n",
    "Goal: create a monthly savings path (control flow only).\n",
    "\n",
    "1. Choose `months = 12` and `monthly_saving = 200`.\n",
    "2. Use `for i in range(months)` to build a list with cumulative savings.\n",
    "3. Add an `if` inside the loop: every 3 months add a bonus of 50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "6826389f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "savings_path: [200, 400, 650, 850, 1050, 1300, 1500, 1700, 1950, 2150, 2350, 2600]\n"
     ]
    }
   ],
   "source": [
    "months = 12\n",
    "# We simulate savings for 12 months.\n",
    "\n",
    "monthly_saving = 200\n",
    "# This is how much money we add to savings every month.\n",
    "\n",
    "savings_path = []\n",
    "# We will store the cumulative savings total after each month in this list.\n",
    "\n",
    "total = 0\n",
    "# We start with zero savings.\n",
    "\n",
    "for i in range(months):\n",
    "    # Loop over months using an index i:\n",
    "    # i = 0 for month 1, i = 1 for month 2, ..., i = 11 for month 12.\n",
    "\n",
    "    total += monthly_saving\n",
    "    # Add the regular monthly saving to the running total.\n",
    "\n",
    "    if (i + 1) % 3 == 0:\n",
    "        # (i + 1) is the month number (1 to 12).\n",
    "        # Every 3 months (month 3, 6, 9, 12), we add a bonus.\n",
    "\n",
    "        total += 50\n",
    "        # Add the bonus of 50.\n",
    "\n",
    "    savings_path.append(total)\n",
    "    # Save the current cumulative total into the list.\n",
    "\n",
    "print(\"savings_path:\", savings_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047bb08",
   "metadata": {},
   "source": [
    "### 4.2.5 <a id='4.2.5.'>  Nested For Loop </a>\n",
    "\n",
    "**Exercise 4.2.5 — Pairwise comparison (nested loop)**\n",
    "\n",
    "Using the tickers list:\n",
    "1. Build all **unique pairs** `(i, j)` with `i < j`.\n",
    "2. For each pair, compare their last close and store the ticker with the higher value.\n",
    "3. Save outputs in a list `winners`.\n",
    "\n",
    "This is intentionally simple to focus on nested loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "12a5013c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winners (first 5): ['SPY', 'SPY', 'SPY', 'SPY', 'MSFT']\n"
     ]
    }
   ],
   "source": [
    "winners = []\n",
    "# This list will store the \"winner\" ticker (the one with the higher last close) for each unique pair.\n",
    "\n",
    "last_close = {t: v for (t, v) in last_close_list} if isinstance(last_close_list, list) else {}\n",
    "# Build a dictionary for quick lookup:\n",
    "# - key: ticker\n",
    "# - value: last close\n",
    "# If last_close_list is not a list, fall back to an empty dict.\n",
    "\n",
    "for i in range(len(tickers)):\n",
    "    # Outer loop goes through each ticker index i.\n",
    "\n",
    "    for j in range(i + 1, len(tickers)):\n",
    "        # Inner loop starts at i+1 so we only create pairs with i < j.\n",
    "        # This guarantees unique pairs (no duplicates and no (j,i) repeats).\n",
    "\n",
    "        ti, tj = tickers[i], tickers[j]\n",
    "        # Get the two ticker symbols for this pair.\n",
    "\n",
    "        vi, vj = last_close.get(ti, np.nan), last_close.get(tj, np.nan)\n",
    "        # Look up their last closes from the dictionary.\n",
    "        # If a ticker is missing, default to NaN.\n",
    "\n",
    "        if pd.isna(vi) and pd.isna(vj):\n",
    "            # If both values are missing, we cannot compare them, so skip this pair.\n",
    "            continue\n",
    "\n",
    "        if pd.isna(vi):\n",
    "            # If vi is missing but vj exists, tj automatically \"wins\".\n",
    "            winners.append(tj)\n",
    "\n",
    "        elif pd.isna(vj):\n",
    "            # If vj is missing but vi exists, ti automatically \"wins\".\n",
    "            winners.append(ti)\n",
    "\n",
    "        else:\n",
    "            # If both exist, compare the numeric values and append the ticker with the higher close.\n",
    "            # We use >= so that if they are equal, ti is chosen.\n",
    "            winners.append(ti if vi >= vj else tj)\n",
    "print(\"winners (first 5):\", winners[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2461242",
   "metadata": {},
   "source": [
    "### 4.2.6. <a id = '4.2.6.'> Iterations over Pandas</a>\n",
    "\n",
    "#### Exercise — Rename Columns (finance version)\n",
    "\n",
    "Using `us`:\n",
    "1. Copy `us` to `us2`.\n",
    "2. Rename columns so that:\n",
    "   - `Close_SPY` → `close_spy`\n",
    "   - `Volume_SPY` → `volume_spy`\n",
    "   - similarly for other tickers\n",
    "3. Do this **with a loop**, not by writing each rename manually.\n",
    "\n",
    "Store the final DataFrame in `us2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "af591fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original cols (first 6): ['Close_AAPL', 'Close_JPM', 'Close_MSFT', 'Close_NVDA', 'Close_SPY', 'Volume_AAPL']\n",
      "New cols (first 6): ['close_aapl', 'close_jpm', 'close_msft', 'close_nvda', 'close_spy', 'volume_aapl']\n"
     ]
    }
   ],
   "source": [
    "us2 = us.copy()\n",
    "# Create a copy of `us` so we do not modify the original DataFrame.\n",
    "\n",
    "rename_map = {}\n",
    "# Create an empty dictionary that will map:\n",
    "#   old_column_name -> new_column_name\n",
    "\n",
    "for col in us2.columns:\n",
    "    # Loop over every column name in us2.\n",
    "\n",
    "    if \"_\" in col:\n",
    "        # If the column contains an underscore, we assume it follows the pattern: Prefix_TICKER\n",
    "        # Examples: \"Close_SPY\", \"Volume_AAPL\"\n",
    "\n",
    "        prefix, ticker = col.split(\"_\", 1)\n",
    "        # Split only on the first underscore:\n",
    "        # - prefix = \"Close\" or \"Volume\"\n",
    "        # - ticker = \"SPY\" or \"AAPL\"\n",
    "\n",
    "        rename_map[col] = f\"{prefix.lower()}_{ticker.lower()}\"\n",
    "        # Convert both pieces to lowercase and rebuild the name:\n",
    "        # \"Close_SPY\" -> \"close_spy\"\n",
    "\n",
    "    else:\n",
    "        # If there is no underscore, just lowercase the whole column name.\n",
    "        rename_map[col] = col.lower()\n",
    "\n",
    "us2 = us2.rename(columns=rename_map)\n",
    "# Apply the renaming in a single step using the mapping dictionary.\n",
    "\n",
    "# Optional self-check\n",
    "print(\"Original cols (first 6):\", list(us.columns)[:6])\n",
    "print(\"New cols (first 6):\", list(us2.columns)[:6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2710c1d8",
   "metadata": {},
   "source": [
    "## 4.3. <a id = '4.3.'> Pass, Continue, Break, Try</a>\n",
    "\n",
    "These statements help control loop execution and handle errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098b5b8",
   "metadata": {},
   "source": [
    "### 4.3.1 <a id = '4.3.1.'> Pass</a>\n",
    "\n",
    "**Exercise 4.3.1 — Placeholder rule**\n",
    "\n",
    "Write a loop over tickers that *would* apply a rule, but for now uses `pass` when the ticker is not `'SPY'`. When the ticker is `'SPY'`, print its last close.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "d051d634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY last close: 671.4000244140625\n"
     ]
    }
   ],
   "source": [
    "# Create a lookup dictionary so we can quickly get the last close by ticker.\n",
    "lookup = {t: v for (t, v) in last_close_list} if isinstance(last_close_list, list) else {}\n",
    "# If last_close_list is a list of (ticker, value), this builds:\n",
    "#   {\"SPY\": <value>, \"AAPL\": <value>, ...}\n",
    "# If last_close_list is not available, we fall back to an empty dictionary.\n",
    "\n",
    "for t in tickers:\n",
    "    # Loop through each ticker symbol in the `tickers` list.\n",
    "\n",
    "    if t != \"SPY\":\n",
    "        # For now, we do nothing for all tickers except SPY.\n",
    "        # `pass` is a placeholder: it means \"do nothing here\".\n",
    "        pass\n",
    "    else:\n",
    "        # This branch runs only when the ticker is SPY.\n",
    "\n",
    "        v = lookup.get(\"SPY\", np.nan)\n",
    "        # Get SPY's last close from the lookup dictionary.\n",
    "        # If SPY is missing, use NaN as a default.\n",
    "\n",
    "        print(\"SPY last close:\", v)\n",
    "        # Print SPY's last close value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d37248",
   "metadata": {},
   "source": [
    "### 4.3.2. <a id = '4.3.2.'>Continue</a>\n",
    "\n",
    "**Exercise 4.3.2 — Skip short histories**\n",
    "\n",
    "1. Loop over tickers.\n",
    "2. For each ticker, count non-missing price observations.\n",
    "3. If fewer than 500 observations, `continue`.\n",
    "4. Otherwise, store the ticker in `enough_data`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "cea4813f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enough_data: ['SPY', 'AAPL', 'MSFT', 'JPM', 'NVDA']\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "enough_data = []\n",
    "\n",
    "if isinstance(us, pd.DataFrame) and (not us.empty):\n",
    "    for t in tickers:\n",
    "        col = f\"Close_{t}\"\n",
    "        if col not in us.columns:\n",
    "            continue\n",
    "        n = int(us[col].dropna().shape[0])\n",
    "        if n < 500:\n",
    "            continue\n",
    "        enough_data.append(t)\n",
    "\n",
    "# Optional self-check\n",
    "print(\"enough_data:\", enough_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1bf7f",
   "metadata": {},
   "source": [
    "### 4.3.3. <a id = '4.3.3.'>Break</a>\n",
    "\n",
    "**Exercise 4.3.3 — Stop when a drawdown threshold is hit**\n",
    "\n",
    "Using SPY returns:\n",
    "1. Walk forward day-by-day.\n",
    "2. Track a running `peak` of the cumulative value.\n",
    "3. Compute drawdown = `value / peak - 1`.\n",
    "4. If drawdown falls below `-0.10` (−10%), `break`.\n",
    "5. Record the date where you stop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "3a752e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enough_data: ['SPY', 'AAPL', 'MSFT', 'JPM', 'NVDA']\n"
     ]
    }
   ],
   "source": [
    "enough_data = []\n",
    "# This list will store tickers that have at least 500 non-missing price observations.\n",
    "\n",
    "if isinstance(us, pd.DataFrame) and (not us.empty):\n",
    "    # Make sure `us` is a non-empty DataFrame before looping.\n",
    "\n",
    "    for t in tickers:\n",
    "        # Loop over each ticker symbol.\n",
    "\n",
    "        col = f\"Close_{t}\"\n",
    "        # Build the column name for that ticker (e.g., \"Close_SPY\").\n",
    "\n",
    "        if col not in us.columns:\n",
    "            # If the column does not exist, skip this ticker immediately.\n",
    "            continue\n",
    "\n",
    "        n = int(us[col].dropna().shape[0])\n",
    "        # Count how many non-missing observations we have for this ticker.\n",
    "\n",
    "        if n < 500:\n",
    "            # If there are fewer than 500 observations, skip this ticker.\n",
    "            continue\n",
    "\n",
    "        enough_data.append(t)\n",
    "        # If we reach this line, the ticker has enough data, so we store it.\n",
    "print(\"enough_data:\", enough_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2444eb16",
   "metadata": {},
   "source": [
    "### 4.3.4. <a id = '4.3.4.'> Try </a>\n",
    "\n",
    "**Exercise 4.3.4 — Try/Except with a real API (SEC)**\n",
    "\n",
    "The SEC requires a **User-Agent**. You can put your email in it.\n",
    "\n",
    "Tasks:\n",
    "1. Set `SEC_USER_AGENT`.\n",
    "2. Fetch Apple company facts (`CIK 0000320193`).\n",
    "3. In a `try` block, navigate the nested dict to find the section `facts`.\n",
    "4. If something is missing, handle with `except` and set `facts = {}`.\n",
    "\n",
    "Do not extract any final metric here—just practice safe access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "80ba92aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facts keys (if available): ['dei', 'us-gaap']\n"
     ]
    }
   ],
   "source": [
    "SEC_USER_AGENT = \"Python Finance Course (student@example.com)\"\n",
    "# The SEC requires a User-Agent header that identifies who is making the request.\n",
    "# If you run this on your own machine, you would typically put your own email there.\n",
    "\n",
    "apple = sec_companyfacts(\"0000320193\", user_agent=SEC_USER_AGENT)\n",
    "# Fetch Apple company facts (CIK 0000320193) using the helper function.\n",
    "# This should return a Python dictionary (nested JSON), or {} if something fails.\n",
    "\n",
    "facts = {}\n",
    "# Default: start with an empty dict, so we always have a safe value.\n",
    "\n",
    "try:\n",
    "    # Try to safely navigate the nested structure without crashing.\n",
    "\n",
    "    if isinstance(apple, dict):\n",
    "        # Only try to read keys if `apple` is actually a dictionary.\n",
    "        facts = apple.get(\"facts\", {})\n",
    "        # Get the \"facts\" section; if it doesn't exist, use {} as default.\n",
    "\n",
    "    if not isinstance(facts, dict):\n",
    "        # If \"facts\" exists but is not a dictionary for some reason, reset to {}.\n",
    "        facts = {}\n",
    "\n",
    "except Exception:\n",
    "    # If anything unexpected happens (bad structure, weird types, etc.), keep facts as {}.\n",
    "    facts = {}\n",
    "\n",
    "print(\"facts keys (if available):\", list(facts.keys())[:5] if isinstance(facts, dict) else None)\n",
    "# Print a quick preview of the top-level keys inside \"facts\" (usually things like 'dei' and 'us-gaap').\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c2710",
   "metadata": {},
   "source": [
    "## 4.4. <a id='4.4.'> While Loop </a>\n",
    "\n",
    "Practice `while` loops with a simple finance simulation using real-return samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ef191",
   "metadata": {},
   "source": [
    "### 4.4.1. <a id='4.4.1.'> Structure </a>\n",
    "\n",
    "**Exercise 4.4.1 — Reach a target portfolio value**\n",
    "\n",
    "Using SPY daily returns:\n",
    "1. Create a list/array of historical daily returns.\n",
    "2. Start with `value = 1.0`.\n",
    "3. While `value < 1.2` (target +20%), repeatedly:\n",
    "   - draw one return at random (fixed seed)\n",
    "   - update value\n",
    "   - increment `steps`\n",
    "4. Add a safety stop: if `steps > 2000`, break.\n",
    "\n",
    "Store the final `steps` and `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "b7741d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 669\n",
      "value: 1.2059536907831272\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "# Counter for how many random daily returns we have drawn.\n",
    "\n",
    "value = 1.0\n",
    "# Start portfolio value at 1.0 (think of it as $1 invested).\n",
    "\n",
    "if isinstance(us, pd.DataFrame) and (not us.empty) and (\"Close_SPY\" in us.columns):\n",
    "    # Check that `us` is a non-empty DataFrame and contains Close_SPY.\n",
    "\n",
    "    spy_ret = us[\"Close_SPY\"].pct_change().dropna().to_numpy()\n",
    "    # Compute daily returns from Close_SPY, drop missing values, and convert to a NumPy array.\n",
    "    # This array is our \"historical return distribution\" to sample from.\n",
    "\n",
    "    if spy_ret.size > 0:\n",
    "        # Make sure we have at least one return observation to sample.\n",
    "\n",
    "        rng = np.random.default_rng(123)\n",
    "        # Create a random number generator with a fixed seed (123) so results are reproducible.\n",
    "\n",
    "        while value < 1.2:\n",
    "            # Keep simulating until the portfolio reaches the target value (1.2 = +20%).\n",
    "\n",
    "            r = float(rng.choice(spy_ret))\n",
    "            # Randomly draw one daily return from the historical return array.\n",
    "\n",
    "            value *= (1 + r)\n",
    "            # Update the portfolio value using the daily growth factor (1 + r).\n",
    "\n",
    "            steps += 1\n",
    "            # Increase the step counter after each simulated day.\n",
    "\n",
    "            if steps > 2000:\n",
    "                # Safety stop: avoid an infinite loop if the target is not reached quickly.\n",
    "                break\n",
    "\n",
    "print(\"steps:\", steps)\n",
    "# Print how many simulated days it took (or how many were done before the safety stop).\n",
    "\n",
    "print(\"value:\", value)\n",
    "# Print the final simulated portfolio value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da18181",
   "metadata": {},
   "source": [
    "## 4.5. <a id = '4.5.'> References </a>\n",
    "\n",
    "- BCRPData API: https://estadisticas.bcrp.gob.pe/estadisticas/series/ayuda/api\n",
    "- yfinance docs: https://ranaroussi.github.io/yfinance/\n",
    "- SEC EDGAR Data APIs: https://www.sec.gov/search-filings/edgar-application-programming-interfaces\n",
    "- Python control flow: https://docs.python.org/3/tutorial/controlflow.html\n",
    "- Pandas pct_change: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc653b7",
   "metadata": {},
   "source": [
    "# 5. Functions and Class — Finance Practice \n",
    "\n",
    "This notebook follows the structure of **Lecture 3 (Part II)**: **Functions** and **Classes**.\n",
    "\n",
    "Complete the TODO blocks. No plotting.\n",
    "\n",
    "Data sources:\n",
    "- **BCRPData API (Peru, official)**\n",
    "- **FRED (US macro series via CSV download)**\n",
    "- **U.S. Treasury Fiscal Data API (official)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "5ff413f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 150)\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "CACHE_DIR = Path(\".cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "_ES_TO_EN_MONTH = {\n",
    "    \"Ene\": \"Jan\", \"Feb\": \"Feb\", \"Mar\": \"Mar\", \"Abr\": \"Apr\", \"May\": \"May\", \"Jun\": \"Jun\",\n",
    "    \"Jul\": \"Jul\", \"Ago\": \"Aug\", \"Set\": \"Sep\", \"Sep\": \"Sep\", \"Oct\": \"Oct\", \"Nov\": \"Nov\", \"Dic\": \"Dec\"\n",
    "}\n",
    "\n",
    "def _hash_key(*parts: str) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    for p in parts:\n",
    "        h.update(str(p).encode(\"utf-8\"))\n",
    "        h.update(b\"|\")\n",
    "    return h.hexdigest()[:24]\n",
    "\n",
    "def _to_float(x):\n",
    "    \"\"\"Robust numeric parsing (handles decimal comma, 'n.d.', etc.).\"\"\"\n",
    "    if x is None:\n",
    "        return np.nan\n",
    "    if isinstance(x, (int, float, np.number)):\n",
    "        return float(x)\n",
    "    s = str(x).strip()\n",
    "    if s.lower() in {\"n.d.\", \"nd\", \"n.d\", \"na\", \"nan\", \"\"}:\n",
    "        return np.nan\n",
    "    s = s.replace(\" \", \"\").replace(\"\\u00a0\", \"\")\n",
    "    # Spanish formatting: thousands \".\" and decimal \",\"\n",
    "    if \",\" in s and \".\" in s:\n",
    "        s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "    elif \",\" in s and \".\" not in s:\n",
    "        s = s.replace(\",\", \".\")\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _parse_bcrp_period(name: str) -> pd.Timestamp:\n",
    "    s = str(name).strip()\n",
    "\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    if pd.notna(dt):\n",
    "        return dt\n",
    "\n",
    "    # Daily: 18Nov25, 02Ene97\n",
    "    m = re.fullmatch(r\"(\\d{2})([A-Za-zÁÉÍÓÚÑñ]{3})(\\d{2})\", s)\n",
    "    if m:\n",
    "        d, mon_es, yy = m.groups()\n",
    "        mon = _ES_TO_EN_MONTH.get(mon_es[:3], mon_es[:3])\n",
    "        year = 2000 + int(yy) if int(yy) <= 69 else 1900 + int(yy)\n",
    "        return pd.to_datetime(f\"{d}{mon}{year}\", format=\"%d%b%Y\", errors=\"coerce\")\n",
    "\n",
    "    # Monthly: Mar.2020\n",
    "    m = re.fullmatch(r\"([A-Za-zÁÉÍÓÚÑñ]{3})\\.(\\d{4})\", s)\n",
    "    if m:\n",
    "        mon_es, y = m.groups()\n",
    "        mon = _ES_TO_EN_MONTH.get(mon_es[:3], mon_es[:3])\n",
    "        return pd.to_datetime(f\"01{mon}{y}\", format=\"%d%b%Y\", errors=\"coerce\")\n",
    "\n",
    "    # Monthly: Ene92 or Ene.92\n",
    "    m = re.fullmatch(r\"([A-Za-zÁÉÍÓÚÑñ]{3})\\.?(\\d{2})\", s)\n",
    "    if m:\n",
    "        mon_es, yy = m.groups()\n",
    "        mon = _ES_TO_EN_MONTH.get(mon_es[:3], mon_es[:3])\n",
    "        year = 2000 + int(yy) if int(yy) <= 69 else 1900 + int(yy)\n",
    "        return pd.to_datetime(f\"01{mon}{year}\", format=\"%d%b%Y\", errors=\"coerce\")\n",
    "\n",
    "    # Numeric month: 2022-5\n",
    "    m = re.fullmatch(r\"(\\d{4})-(\\d{1,2})\", s)\n",
    "    if m:\n",
    "        y, mo = m.groups()\n",
    "        return pd.to_datetime(f\"{int(y):04d}-{int(mo):02d}-01\", errors=\"coerce\")\n",
    "\n",
    "    # Year only\n",
    "    m = re.fullmatch(r\"(\\d{4})\", s)\n",
    "    if m:\n",
    "        return pd.to_datetime(f\"{m.group(1)}-01-01\", errors=\"coerce\")\n",
    "\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def _normalize_bcrp_period(code: str, period: str | None) -> str | None:\n",
    "    \"\"\"\n",
    "    Normalize periods for BCRP API.\n",
    "    Heuristic:\n",
    "    - codes starting with PD: daily -> YYYY-MM-DD\n",
    "    - codes starting with PN: monthly -> YYYY-m (no zero-pad)\n",
    "    \"\"\"\n",
    "    if period is None:\n",
    "        return None\n",
    "    p = str(period).strip()\n",
    "\n",
    "    if code.startswith(\"PD\"):\n",
    "        if re.fullmatch(r\"\\d{4}-\\d{1,2}\", p):\n",
    "            y, m = p.split(\"-\")\n",
    "            return f\"{int(y):04d}-{int(m):02d}-01\"\n",
    "        if re.fullmatch(r\"\\d{4}\", p):\n",
    "            return f\"{int(p):04d}-01-01\"\n",
    "        return p\n",
    "\n",
    "    if code.startswith(\"PN\"):\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{1,2})-(\\d{1,2})\", p)\n",
    "        if m:\n",
    "            y, mo, _ = m.groups()\n",
    "            return f\"{int(y):04d}-{int(mo)}\"\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{1,2})\", p)\n",
    "        if m:\n",
    "            y, mo = m.groups()\n",
    "            return f\"{int(y):04d}-{int(mo)}\"\n",
    "        if re.fullmatch(r\"\\d{4}\", p):\n",
    "            return f\"{int(p):04d}-1\"\n",
    "        return p\n",
    "\n",
    "    return p\n",
    "\n",
    "def _read_parquet_safe(path: Path) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        return pd.read_parquet(path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _write_parquet_safe(df: pd.DataFrame, path: Path) -> None:\n",
    "    try:\n",
    "        df.to_parquet(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def bcrp_get(series_codes, start: str | None = None, end: str | None = None, lang: str = \"esp\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch BCRPData series using the official BCRP API (JSON).\n",
    "    Returns columns: ['date', <code1>, <code2>, ...]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import requests\n",
    "    except Exception:\n",
    "        warnings.warn(\"requests not available; returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if isinstance(series_codes, (list, tuple)):\n",
    "        codes_list = [str(c).strip() for c in series_codes]\n",
    "        codes = \"-\".join(codes_list)\n",
    "        first_code = codes_list[0]\n",
    "    else:\n",
    "        codes = str(series_codes).strip()\n",
    "        codes_list = codes.split(\"-\")\n",
    "        first_code = codes_list[0]\n",
    "\n",
    "    start_n = _normalize_bcrp_period(first_code, start)\n",
    "    end_n = _normalize_bcrp_period(first_code, end)\n",
    "\n",
    "    key = _hash_key(\"bcrp\", codes, start_n or \"\", end_n or \"\", lang)\n",
    "    cache_path = CACHE_DIR / f\"bcrp_{key}.parquet\"\n",
    "    cached = _read_parquet_safe(cache_path)\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "\n",
    "    base_url = \"https://estadisticas.bcrp.gob.pe/estadisticas/series/api\"\n",
    "    parts = [base_url, codes, \"json\"]\n",
    "    if start_n and end_n:\n",
    "        parts += [start_n, end_n]\n",
    "    if lang:\n",
    "        parts += [lang]\n",
    "    url = \"/\".join(parts)\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, timeout=45)\n",
    "        r.raise_for_status()\n",
    "        obj = r.json()\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"BCRP request failed ({repr(e)}). Returning empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=[\"date\"] + codes_list)\n",
    "\n",
    "    periods = obj.get(\"periods\", [])\n",
    "    rows = []\n",
    "    for p in periods:\n",
    "        name = p.get(\"name\")\n",
    "        vals = p.get(\"values\", [])\n",
    "        if isinstance(vals, str):\n",
    "            vals = [vals]\n",
    "        if name is None or not isinstance(vals, list):\n",
    "            continue\n",
    "        vals = (vals + [None] * len(codes_list))[:len(codes_list)]\n",
    "        rows.append([name] + vals)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"date\"] + codes_list)\n",
    "    if df.shape[0] == 0:\n",
    "        return pd.DataFrame(columns=[\"date\"] + codes_list)\n",
    "\n",
    "    df[\"date\"] = df[\"date\"].apply(_parse_bcrp_period)\n",
    "    for c in codes_list:\n",
    "        df[c] = df[c].apply(_to_float)\n",
    "\n",
    "    df = df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "    _write_parquet_safe(df, cache_path)\n",
    "    return df\n",
    "\n",
    "def fred_get(series_ids, start: str | None = None, end: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch FRED series WITHOUT an API key using the CSV export used by FRED graphs.\n",
    "\n",
    "    Uses cosd/coed to limit the download window (more reliable than downloading full history).\n",
    "    Returns columns: ['date', <series1>, <series2>, ...]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import requests\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"requests not available ({e}); returning empty DataFrame.\")\n",
    "        cols = [str(s).strip() for s in series_ids] if isinstance(series_ids, (list, tuple)) else [str(series_ids).strip()]\n",
    "        return pd.DataFrame(columns=[\"date\"] + cols)\n",
    "\n",
    "    from io import StringIO\n",
    "\n",
    "    def _fetch_one(sid: str) -> pd.DataFrame:\n",
    "        sid = str(sid).strip()\n",
    "        key = _hash_key(\"fred_v3\", sid, start or \"\", end or \"\")\n",
    "        cache_path = CACHE_DIR / f\"fred_{key}.parquet\"\n",
    "        cached = _read_parquet_safe(cache_path)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "\n",
    "        url = \"https://fred.stlouisfed.org/graph/fredgraph.csv\"\n",
    "        params = {\"id\": sid}\n",
    "        if start:\n",
    "            params[\"cosd\"] = str(start)\n",
    "        if end:\n",
    "            params[\"coed\"] = str(end)\n",
    "\n",
    "        headers = {\"User-Agent\": \"python-finance-course/1.0 (contact: student@example.com)\"}\n",
    "\n",
    "        try:\n",
    "            r = requests.get(url, params=params, headers=headers, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            text = r.text\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"FRED request failed for {sid} ({repr(e)}). Returning empty.\")\n",
    "            return pd.DataFrame(columns=[\"date\", sid])\n",
    "\n",
    "        # Guardrails: detect HTML or unexpected payloads\n",
    "        first_line = (text.splitlines()[0].strip() if text else \"\")\n",
    "        if first_line.lower().startswith(\"<!doctype\") or \"<html\" in first_line.lower():\n",
    "            warnings.warn(f\"FRED returned HTML for {sid}. Returning empty.\")\n",
    "            return pd.DataFrame(columns=[\"date\", sid])\n",
    "\n",
    "        if \"DATE\" not in first_line.upper() or sid.upper() not in first_line.upper():\n",
    "            warnings.warn(f\"FRED response header unexpected for {sid}: {first_line[:120]} ... Returning empty.\")\n",
    "            return pd.DataFrame(columns=[\"date\", sid])\n",
    "\n",
    "        df = pd.read_csv(StringIO(text))\n",
    "        # Normalize column names\n",
    "        df = df.rename(columns={df.columns[0]: \"date\", df.columns[1]: sid})\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "        # FRED missing values are often \".\"\n",
    "        s = df[sid].astype(str).str.strip().replace({\".\": np.nan, \"\": np.nan, \"NA\": np.nan, \"NaN\": np.nan})\n",
    "        df[sid] = pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "        df = df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "        _write_parquet_safe(df, cache_path)\n",
    "        return df\n",
    "\n",
    "    if isinstance(series_ids, (list, tuple)):\n",
    "        cols = [str(s).strip() for s in series_ids]\n",
    "        out = None\n",
    "        for sid in cols:\n",
    "            dfi = _fetch_one(sid)\n",
    "            out = dfi if out is None else out.merge(dfi, on=\"date\", how=\"outer\")\n",
    "        if out is None:\n",
    "            return pd.DataFrame(columns=[\"date\"] + cols)\n",
    "        return out.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    return _fetch_one(series_ids)\n",
    "\n",
    "\n",
    "\n",
    "def treasury_get_debt_to_penny(start_date: str = \"2024-01-01\") -> dict:\n",
    "    \"\"\"\n",
    "    Fetch U.S. Treasury Fiscal Data (Debt to the Penny) as a raw dictionary.\n",
    "    API docs: https://fiscaldata.treasury.gov/api-documentation/\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import requests\n",
    "    except Exception:\n",
    "        warnings.warn(\"requests not available; returning empty dict.\")\n",
    "        return {}\n",
    "\n",
    "    base = \"https://api.fiscaldata.treasury.gov/services/api/fiscal_service/v2\"\n",
    "    endpoint = \"accounting/od/debt_to_penny\"\n",
    "    url = f\"{base}/{endpoint}\"\n",
    "\n",
    "    params = {\n",
    "        \"fields\": \"record_date,tot_pub_debt_out_amt,debt_held_public_amt,intragov_hold_amt\",\n",
    "        \"filter\": f\"record_date:gte:{start_date}\",\n",
    "        \"sort\": \"-record_date\",\n",
    "        \"page[size]\": 1000\n",
    "    }\n",
    "\n",
    "    headers = {\"User-Agent\": \"python-finance-course/1.0 (contact: student@example.com)\"}\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, params=params, headers=headers, timeout=45)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Treasury API request failed ({repr(e)}). Returning empty dict.\")\n",
    "        return {}\n",
    "\n",
    "def save_pickle(obj, path: Path) -> None:\n",
    "    import pickle\n",
    "    path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path: Path):\n",
    "    import pickle\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def safe_head(df: pd.DataFrame, n: int = 5) -> pd.DataFrame:\n",
    "    return df.head(n) if isinstance(df, pd.DataFrame) else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb85d2",
   "metadata": {},
   "source": [
    "## Real data setup\n",
    "\n",
    "We fetch a small amount of real data.\n",
    "\n",
    "**Peru (BCRPData):**\n",
    "- `PD04650MD`: Net International Reserves (daily)\n",
    "- `PN01652XM`: Copper price (LME, monthly)\n",
    "\n",
    "**US (FRED):**\n",
    "- `DGS10`: 10-year Treasury yield (daily)\n",
    "- `FEDFUNDS`: Effective federal funds rate (monthly)\n",
    "\n",
    "**US Treasury Fiscal Data:**\n",
    "- Debt to the Penny: total public debt outstanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "6117f935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((877, 2), (131, 2), (1576, 3), ['data', 'meta', 'links'])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time windows (adjust if you want)\n",
    "START_DAILY = \"2020-01-01\"\n",
    "END_DAILY = \"2025-12-18\"\n",
    "START_MONTHLY = \"2015-01-01\"\n",
    "\n",
    "# Peru (BCRP)\n",
    "rin = bcrp_get(\"PD04650MD\", start=START_DAILY, end=END_DAILY).rename(columns={\"PD04650MD\": \"RIN_USD_mn\"})\n",
    "copper = bcrp_get(\"PN01652XM\", start=START_MONTHLY, end=END_DAILY).rename(columns={\"PN01652XM\": \"Copper_LME_cents_per_lb\"})\n",
    "\n",
    "# US (FRED)\n",
    "macro_us = fred_get([\"DGS10\", \"FEDFUNDS\"], start=START_DAILY)\n",
    "\n",
    "# US Treasury Fiscal Data (raw dict, then saved for the dictionary exercise)\n",
    "treasury_raw = treasury_get_debt_to_penny(start_date=\"2024-01-01\")\n",
    "save_pickle(treasury_raw, Path(\"data/treasury_debt_to_penny_raw.pkl\"))\n",
    "\n",
    "rin.shape, copper.shape, macro_us.shape, list(treasury_raw.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "fa0c1899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>RIN_USD_mn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>68820.232306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>68648.663705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>68790.147192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-06</td>\n",
       "      <td>68812.399398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-07</td>\n",
       "      <td>68976.740110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date    RIN_USD_mn\n",
       "0 2020-02-03  68820.232306\n",
       "1 2020-02-04  68648.663705\n",
       "2 2020-02-05  68790.147192\n",
       "3 2020-02-06  68812.399398\n",
       "4 2020-02-07  68976.740110"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Copper_LME_cents_per_lb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>265.576789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>259.875545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>269.418923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>273.904333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>285.478621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  Copper_LME_cents_per_lb\n",
       "0 2015-01-01               265.576789\n",
       "1 2015-02-01               259.875545\n",
       "2 2015-03-01               269.418923\n",
       "3 2015-04-01               273.904333\n",
       "4 2015-05-01               285.478621"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>DGS10</th>\n",
       "      <th>FEDFUNDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1.88</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>1.80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>1.81</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>1.83</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  DGS10  FEDFUNDS\n",
       "0 2020-01-01    NaN      1.55\n",
       "1 2020-01-02   1.88       NaN\n",
       "2 2020-01-03   1.80       NaN\n",
       "3 2020-01-06   1.81       NaN\n",
       "4 2020-01-07   1.83       NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(safe_head(rin))\n",
    "display(safe_head(copper))\n",
    "display(safe_head(macro_us))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6e919",
   "metadata": {},
   "source": [
    "##  <a id='#5.1.'>5.1. Functions</a>\n",
    "### <a id = '#5.1.1.'> 5.1.1. The importance of Python functions </a>\n",
    "\n",
    "**exercise:** write a function that standardizes a single-series DataFrame.\n",
    "\n",
    "Requirements:\n",
    "1. Input: a DataFrame with a `date` column and exactly **one value column**.\n",
    "2. Output: a DataFrame with columns `date` and `value`.\n",
    "3. Must:\n",
    "   - convert `date` to datetime\n",
    "   - sort by date\n",
    "   - drop rows where `date` is missing\n",
    "4. Use the function for both `rin` and `copper`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "88ab913d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'value'] (877, 2)\n",
      "['date', 'value'] (131, 2)\n"
     ]
    }
   ],
   "source": [
    "def standardize_single_series(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # This function takes a DataFrame with a 'date' column and exactly one value column,\n",
    "    # and returns a standardized DataFrame with columns: ['date', 'value'].\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        # If the input is not a DataFrame (or it is empty), return an empty standardized DataFrame.\n",
    "        return pd.DataFrame(columns=[\"date\", \"value\"])\n",
    "\n",
    "    if \"date\" not in df.columns:\n",
    "        # If there is no 'date' column, the function cannot proceed correctly.\n",
    "        raise ValueError(\"Input DataFrame must contain a 'date' column.\")\n",
    "\n",
    "    value_cols = [c for c in df.columns if c != \"date\"]\n",
    "    # Collect all columns except 'date'; these are supposed to be the value columns.\n",
    "\n",
    "    if len(value_cols) != 1:\n",
    "        # We require exactly one value column besides 'date'.\n",
    "        raise ValueError(f\"Expected exactly 1 value column besides 'date', got {len(value_cols)}: {value_cols}\")\n",
    "\n",
    "    val_col = value_cols[0]\n",
    "    # Store the name of the single value column.\n",
    "\n",
    "    out = df[[\"date\", val_col]].copy()\n",
    "    # Keep only the date and the value column, and copy so we don't modify the original input.\n",
    "\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "    # Convert the date column to datetime; invalid dates become NaT (missing datetime).\n",
    "\n",
    "    out[val_col] = pd.to_numeric(out[val_col], errors=\"coerce\")\n",
    "    # Convert the value column to numeric; invalid entries become NaN.\n",
    "\n",
    "    out = out.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "    # Drop rows where date is missing, sort by date, and reset the index.\n",
    "\n",
    "    return out.rename(columns={val_col: \"value\"})\n",
    "    # Rename the value column to the standard name: 'value'.\n",
    "\n",
    "rin_std = standardize_single_series(rin)\n",
    "# Apply the function to the `rin` DataFrame.\n",
    "\n",
    "copper_std = standardize_single_series(copper)\n",
    "# Apply the function to the `copper` DataFrame.\n",
    "\n",
    "print(rin_std.columns.tolist(), rin_std.shape)\n",
    "# Print the standardized column names and shape for rin.\n",
    "\n",
    "print(copper_std.columns.tolist(), copper_std.shape)\n",
    "# Print the standardized column names and shape for copper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b93f36",
   "metadata": {},
   "source": [
    "### <a id='#5.1.2.'> 5.1.2. Basic structure of a function </a>\n",
    "\n",
    "**Exercise:** create a function that computes percent changes (in %).\n",
    "\n",
    "Requirements:\n",
    "- Input: `pd.Series`\n",
    "- Parameter: `periods: int = 1`\n",
    "- Output: `pd.Series`\n",
    "\n",
    "Use it to compute:\n",
    "- daily % change of `RIN_USD_mn`\n",
    "- daily % change of `DGS10`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "66316a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "2020-02-04   -0.249300\n",
      "2020-02-05    0.206098\n",
      "2020-02-06    0.032348\n",
      "2020-02-07    0.238824\n",
      "2020-02-10    0.018453\n",
      "Name: RIN_USD_mn, dtype: float64\n",
      "date\n",
      "2020-01-03   -4.255319\n",
      "2020-01-06    0.555556\n",
      "2020-01-07    1.104972\n",
      "2020-01-08    2.185792\n",
      "2020-01-09   -1.069519\n",
      "Name: DGS10, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def pct_change_percent(x: pd.Series, periods: int = 1) -> pd.Series:\n",
    "    # This function computes percent changes in *percent units*.\n",
    "    # Example: if the return is 0.012 in decimal form, we return 1.2 (percent).\n",
    "\n",
    "    if x is None or not isinstance(x, pd.Series) or x.empty:\n",
    "        # If the input is missing, not a Series, or empty, return an empty float Series.\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    s = pd.to_numeric(x, errors=\"coerce\")\n",
    "    # Convert the series to numeric; non-numeric values become NaN.\n",
    "\n",
    "    out = s.pct_change(periods=periods) * 100.0\n",
    "    # Compute percent change over the chosen number of periods and multiply by 100\n",
    "    # to express the result in percent units.\n",
    "\n",
    "    out.name = getattr(x, \"name\", None)\n",
    "    # Keep the original series name (if it had one), so outputs stay labeled.\n",
    "\n",
    "    return out\n",
    "    # Return the percent-change series.\n",
    "\n",
    "rin_ret = pct_change_percent(rin.set_index(\"date\")[\"RIN_USD_mn\"])\n",
    "# Take the RIN_USD_mn series indexed by date and compute its daily percent change.\n",
    "\n",
    "dgs10_ret = pct_change_percent(macro_us.set_index(\"date\")[\"DGS10\"])\n",
    "# Take the DGS10 series indexed by date and compute its daily percent change.\n",
    "\n",
    "print(rin_ret.dropna().head())\n",
    "# Show the first few non-missing daily percent changes for RIN.\n",
    "\n",
    "print(dgs10_ret.dropna().head())\n",
    "# Show the first few non-missing daily percent changes for DGS10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf51bb",
   "metadata": {},
   "source": [
    "### <a id='#5.1.3.'>5.1.3. Function without `return` </a>\n",
    "\n",
    "**Exercise:** write a function that prints a compact report:\n",
    "- number of observations\n",
    "- number of missing values\n",
    "- min / max\n",
    "\n",
    "Call it for `RIN_USD_mn` and for `DGS10`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "1394582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Peru: Net International Reserves (RIN)] n=877 | missing=0 | min=66977.7541 | max=90898.1836\n",
      "[US: 10Y Treasury Yield (DGS10)] n=1576 | missing=85 | min=0.5200 | max=4.9800\n"
     ]
    }
   ],
   "source": [
    "def print_series_report(x: pd.Series, name: str) -> None:\n",
    "    # This function prints a short report about a numeric series.\n",
    "    # It does not return anything (it prints and exits).\n",
    "\n",
    "    if x is None or not isinstance(x, pd.Series):\n",
    "        # If the input is not a pandas Series, we print a message and stop.\n",
    "        print(f\"[{name}] Invalid input (expected a pandas Series).\")\n",
    "        return\n",
    "\n",
    "    s = pd.to_numeric(x, errors=\"coerce\")\n",
    "    # Convert values to numeric; anything that cannot be converted becomes NaN.\n",
    "\n",
    "    n_obs = int(s.shape[0])\n",
    "    # Total number of observations (including missing).\n",
    "\n",
    "    n_missing = int(s.isna().sum())\n",
    "    # Number of missing values (NaN).\n",
    "\n",
    "    n_non_missing = int(s.notna().sum())\n",
    "    # Number of non-missing values.\n",
    "\n",
    "    if n_non_missing == 0:\n",
    "        # If everything is missing, we cannot compute min/max.\n",
    "        print(f\"[{name}] n={n_obs} | missing={n_missing} | min=NA | max=NA\")\n",
    "        return\n",
    "\n",
    "    s_min = float(s.min(skipna=True))\n",
    "    # Minimum value ignoring missing values.\n",
    "\n",
    "    s_max = float(s.max(skipna=True))\n",
    "    # Maximum value ignoring missing values.\n",
    "\n",
    "    print(f\"[{name}] n={n_obs} | missing={n_missing} | min={s_min:.4f} | max={s_max:.4f}\")\n",
    "    # Print the report in one compact line.\n",
    "\n",
    "if rin.shape[0] > 0:\n",
    "    # Only run the report if rin has rows.\n",
    "    print_series_report(rin[\"RIN_USD_mn\"], \"Peru: Net International Reserves (RIN)\")\n",
    "\n",
    "if macro_us.shape[0] > 0:\n",
    "    # Only run the report if macro_us has rows.\n",
    "    print_series_report(macro_us[\"DGS10\"], \"US: 10Y Treasury Yield (DGS10)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd28f88",
   "metadata": {},
   "source": [
    "### <a id='#5.1.5.'>5.1.5. Multiple objects for return </a>\n",
    "\n",
    "**Exercise:** create a function that returns **two objects**:\n",
    "1. a cleaned return series (drop NaNs)\n",
    "2. a scalar volatility estimate (standard deviation)\n",
    "\n",
    "Use it on `DGS10` percent changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "9ebfa4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "2020-01-03   -4.255319\n",
      "2020-01-06    0.555556\n",
      "2020-01-07    1.104972\n",
      "2020-01-08    2.185792\n",
      "2020-01-09   -1.069519\n",
      "Name: DGS10, dtype: float64\n",
      "Vol: 3.356625083239736\n"
     ]
    }
   ],
   "source": [
    "def returns_and_volatility(x: pd.Series) -> tuple[pd.Series, float]:\n",
    "    # This function returns two things:\n",
    "    # 1) a cleaned return series (numeric + drop NaNs)\n",
    "    # 2) a single volatility number (sample standard deviation)\n",
    "\n",
    "    if x is None or not isinstance(x, pd.Series) or x.empty:\n",
    "        # If the input is missing, not a Series, or empty,\n",
    "        # return an empty Series and NaN for volatility.\n",
    "        return pd.Series(dtype=float), float(\"nan\")\n",
    "\n",
    "    clean = pd.to_numeric(x, errors=\"coerce\").dropna()\n",
    "    # Convert the series to numeric; invalid values become NaN.\n",
    "    # Then drop NaNs so we keep only valid returns.\n",
    "\n",
    "    vol = float(clean.std(ddof=1)) if clean.shape[0] >= 2 else float(\"nan\")\n",
    "    # Compute sample standard deviation (ddof=1).\n",
    "    # We require at least 2 observations; otherwise std is not defined.\n",
    "\n",
    "    return clean, vol\n",
    "    # Return both outputs as a tuple: (clean_series, volatility_number)\n",
    "\n",
    "dgs10_clean_ret, dgs10_vol = returns_and_volatility(dgs10_ret)\n",
    "# Apply the function to the DGS10 percent-change returns:\n",
    "# - dgs10_clean_ret gets the cleaned Series\n",
    "# - dgs10_vol gets the volatility scalar\n",
    "\n",
    "print(dgs10_clean_ret.head())\n",
    "# Show the first 5 cleaned returns.\n",
    "\n",
    "print(\"Vol:\", dgs10_vol)\n",
    "# Print the volatility estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf228a",
   "metadata": {},
   "source": [
    "### <a id='#5.1.5.'>5.1.5. If condition with return </a>\n",
    "\n",
    "**Exercise:** write a validation function.\n",
    "\n",
    "Requirements:\n",
    "- Input: DataFrame with `date` and `value`\n",
    "- If there are fewer than `min_n` rows, return `None`\n",
    "- Otherwise, return the DataFrame\n",
    "\n",
    "Test it with `copper_std` (monthly series can be shorter).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "67a84378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copper validated: True\n"
     ]
    }
   ],
   "source": [
    "def validate_min_rows(df: pd.DataFrame, min_n: int = 24):\n",
    "    # This function checks whether a DataFrame has \"enough\" rows.\n",
    "    # If it does not, it returns None; otherwise it returns the DataFrame itself.\n",
    "\n",
    "    if isinstance(df, pd.DataFrame) and df.shape[0] >= min_n:\n",
    "        # First condition: df must be a pandas DataFrame.\n",
    "        # Second condition: it must have at least min_n rows.\n",
    "        return df\n",
    "        # If both conditions are true, return the DataFrame (it passes validation).\n",
    "\n",
    "    return None\n",
    "    # If the conditions are not met, return None (it fails validation).\n",
    "\n",
    "copper_ok = validate_min_rows(copper_std, min_n=24)\n",
    "# Apply the validation to copper_std using a minimum of 24 rows.\n",
    "\n",
    "print(\"Copper validated:\", copper_ok is not None)\n",
    "# Print True if copper_ok is not None (meaning it passed), otherwise False.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30648403",
   "metadata": {},
   "source": [
    "### <a id='#5.1.6.'>5.1.6. Default values to parameters </a>\n",
    "\n",
    "**Exercise:** implement winsorization with default quantiles.\n",
    "\n",
    "Write a function:\n",
    "`winsorize(x, lower_q=0.01, upper_q=0.99)`\n",
    "\n",
    "Use it on `RIN_USD_mn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "efe8becf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "2020-02-03    68820.232306\n",
      "2020-02-04    68648.663705\n",
      "2020-02-05    68790.147192\n",
      "2020-02-06    68812.399398\n",
      "2020-02-07    68976.740110\n",
      "Name: RIN_USD_mn, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def winsorize(x: pd.Series, lower_q: float = 0.01, upper_q: float = 0.99) -> pd.Series:\n",
    "    # This function \"winsorizes\" a series by clipping extreme values.\n",
    "    # Any value below the lower quantile becomes the lower-quantile value,\n",
    "    # and any value above the upper quantile becomes the upper-quantile value.\n",
    "\n",
    "    if x is None or not isinstance(x, pd.Series) or x.empty:\n",
    "        # If the input is missing, not a Series, or empty, return an empty float Series.\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    s = pd.to_numeric(x, errors=\"coerce\")\n",
    "    # Convert to numeric; values that cannot be converted become NaN.\n",
    "\n",
    "    lo = s.quantile(lower_q)\n",
    "    # Compute the lower quantile cutoff (default 1%).\n",
    "\n",
    "    hi = s.quantile(upper_q)\n",
    "    # Compute the upper quantile cutoff (default 99%).\n",
    "\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "    # Clip values to stay within [lo, hi] and return the clipped series.\n",
    "\n",
    "rin_w = winsorize(rin.set_index(\"date\")[\"RIN_USD_mn\"])\n",
    "# Take the RIN_USD_mn series indexed by date and apply winsorization.\n",
    "\n",
    "print(rin_w.dropna().head())\n",
    "# Print the first few non-missing winsorized values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f46b9",
   "metadata": {},
   "source": [
    "### <a id='#5.1.7.'>5.1.7. Type hints for parameters and return types </a>\n",
    "\n",
    "**Exercise:** add type hints and implement a merge helper.\n",
    "\n",
    "Write:\n",
    "`merge_on_date(left: pd.DataFrame, right: pd.DataFrame, how: str = 'inner') -> pd.DataFrame`\n",
    "\n",
    "Then merge `rin` with `macro_us[['date','DGS10']]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "7b62c753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date    RIN_USD_mn  DGS10\n",
      "0 2020-02-03  68820.232306   1.54\n",
      "1 2020-02-04  68648.663705   1.61\n",
      "2 2020-02-05  68790.147192   1.66\n",
      "3 2020-02-06  68812.399398   1.65\n",
      "4 2020-02-07  68976.740110   1.59\n"
     ]
    }
   ],
   "source": [
    "def merge_on_date(left: pd.DataFrame, right: pd.DataFrame, how: str = \"inner\") -> pd.DataFrame:\n",
    "    # This function merges two DataFrames using a shared 'date' column.\n",
    "    # It also makes sure both 'date' columns are proper datetimes before merging.\n",
    "\n",
    "    if not isinstance(left, pd.DataFrame) or not isinstance(right, pd.DataFrame):\n",
    "        # If either input is not a DataFrame, return an empty DataFrame.\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    l = left.copy()\n",
    "    r = right.copy()\n",
    "    # Copy both inputs so the originals are not modified.\n",
    "\n",
    "    if \"date\" not in l.columns or \"date\" not in r.columns:\n",
    "        # Both DataFrames must have a 'date' column to merge on.\n",
    "        raise ValueError(\"Both DataFrames must contain a 'date' column.\")\n",
    "\n",
    "    l[\"date\"] = pd.to_datetime(l[\"date\"], errors=\"coerce\")\n",
    "    r[\"date\"] = pd.to_datetime(r[\"date\"], errors=\"coerce\")\n",
    "    # Convert dates to datetime; invalid dates become missing (NaT).\n",
    "\n",
    "    l = l.dropna(subset=[\"date\"])\n",
    "    r = r.dropna(subset=[\"date\"])\n",
    "    # Drop rows where date is missing, because they cannot be merged correctly.\n",
    "\n",
    "    out = l.merge(r, on=\"date\", how=how)\n",
    "    # Merge using the chosen method (default 'inner' keeps only matching dates).\n",
    "\n",
    "    return out.sort_values(\"date\").reset_index(drop=True)\n",
    "    # Sort by date and reset the index for a clean final DataFrame.\n",
    "\n",
    "rin_dgs10 = merge_on_date(rin, macro_us[[\"date\", \"DGS10\"]], how=\"inner\")\n",
    "# Merge Peru RIN with US DGS10 using only dates that appear in both series.\n",
    "\n",
    "print(rin_dgs10.head())\n",
    "# Print the first rows of the merged result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fc7a5",
   "metadata": {},
   "source": [
    "### <a id='#5.1.8.'>5.1.8. Local variables vs Global variables </a>\n",
    "\n",
    "**Exercise:** demonstrate the difference clearly.\n",
    "\n",
    "1. Create a global variable `BASE_CCY = 'USD'`.\n",
    "2. Write `format_label(series_name: str) -> str` that uses the global variable.\n",
    "3. Inside the function, create a local variable `suffix = '(global ccy)'`.\n",
    "4. Return a label like: `'{series_name} - USD (global ccy)'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "4c240076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIN - USD (global ccy)\n"
     ]
    }
   ],
   "source": [
    "BASE_CCY = \"USD\"\n",
    "# Global variable: it lives outside the function and can be used inside it.\n",
    "\n",
    "def format_label(series_name: str) -> str:\n",
    "    # This function builds a label using the global currency BASE_CCY.\n",
    "\n",
    "    suffix = \"(global ccy)\"\n",
    "    # Local variable: it exists only inside the function.\n",
    "\n",
    "    return f\"{series_name} - {BASE_CCY} {suffix}\"\n",
    "    # Create the final label string and return it.\n",
    "\n",
    "print(format_label(\"RIN\"))\n",
    "# This should print: \"RIN - USD (global ccy)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c516f",
   "metadata": {},
   "source": [
    "### <a id='#5.1.9.'>5.1.9. `*args` </a>\n",
    "\n",
    "**Exercise:** implement a function that accepts multiple series codes.\n",
    "\n",
    "Write:\n",
    "`fetch_bcrp_many(*codes: str, start: str, end: str) -> pd.DataFrame`\n",
    "\n",
    "Requirements:\n",
    "- Use the provided `bcrp_get` inside.\n",
    "- Return a DataFrame with `date` and one column per code.\n",
    "- Call it with two codes: `PD04650MD` and `PD04649MD`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "6e680979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date     PD04650MD     PD04649MD\n",
      "0 2020-02-03  42815.537788  68820.232306\n",
      "1 2020-02-04  42769.734020  68648.663705\n",
      "2 2020-02-05  42763.293456  68790.147192\n",
      "3 2020-02-06  42767.919586  68812.399398\n",
      "4 2020-02-07  42776.804806  68976.740110\n"
     ]
    }
   ],
   "source": [
    "def fetch_bcrp_many(*codes: str, start: str, end: str) -> pd.DataFrame:\n",
    "    # Accepts any number of BCRP series codes via *codes.\n",
    "    # It downloads them using bcrp_get and returns one DataFrame with:\n",
    "    #   date + one column per code.\n",
    "\n",
    "    if len(codes) == 0:\n",
    "        # If no codes were provided, return an empty standardized structure.\n",
    "        return pd.DataFrame(columns=[\"date\"])\n",
    "\n",
    "    df = bcrp_get(list(codes), start=start, end=end)\n",
    "    # Use the provided bcrp_get to fetch all series in one call.\n",
    "\n",
    "    if df is None or not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        # If nothing came back, still return a DataFrame with the expected columns.\n",
    "        return pd.DataFrame(columns=[\"date\"] + list(codes))\n",
    "\n",
    "    if \"date\" in df.columns:\n",
    "        # Make sure date is datetime, then sort and clean.\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "    # Final DataFrame: date + one column per code.\n",
    "\n",
    "two_series = fetch_bcrp_many(\"PD04650MD\", \"PD04649MD\", start=START_DAILY, end=END_DAILY)\n",
    "# Correct call: both are daily codes, so both columns should align on daily dates.\n",
    "\n",
    "print(two_series.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab9921",
   "metadata": {},
   "source": [
    "### <a id='#5.1.10.'>5.1.10. `**kwargs` </a>\n",
    "\n",
    "**Exercise:** write a wrapper around `DataFrame.describe`.\n",
    "\n",
    "Write:\n",
    "`describe_df(df: pd.DataFrame, **kwargs) -> pd.DataFrame`\n",
    "\n",
    "Then call it on `macro_us[['DGS10']]` with custom percentiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "af23a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             DGS10\n",
      "count  1491.000000\n",
      "mean      2.947304\n",
      "std       1.408279\n",
      "min       0.520000\n",
      "5%        0.680000\n",
      "50%       3.550000\n",
      "95%       4.550000\n",
      "max       4.980000\n"
     ]
    }
   ],
   "source": [
    "def describe_df(df: pd.DataFrame, **kwargs) -> pd.DataFrame:\n",
    "    # This function is a small wrapper around DataFrame.describe().\n",
    "    # It passes any extra keyword arguments (**kwargs) directly to describe().\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        # If df is not a DataFrame or it has no rows, return an empty DataFrame.\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    tmp = df.copy()\n",
    "    # Work on a copy so the original DataFrame is unchanged.\n",
    "\n",
    "    for c in tmp.columns:\n",
    "        # Loop through each column name.\n",
    "        tmp[c] = pd.to_numeric(tmp[c], errors=\"ignore\")\n",
    "        # Try to convert the column to numeric.\n",
    "        # If conversion fails, keep the original values (errors=\"ignore\").\n",
    "\n",
    "    return tmp.describe(**kwargs)\n",
    "    # Call describe with whatever options the user passed in (**kwargs)\n",
    "    # and return the summary table.\n",
    "\n",
    "desc_yields = describe_df(macro_us[[\"DGS10\"]], percentiles=[0.05, 0.5, 0.95])\n",
    "# Apply the wrapper to the DGS10 column only, requesting custom percentiles.\n",
    "\n",
    "print(desc_yields)\n",
    "# Print the resulting descriptive statistics table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb37077",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "#### Importing a Dictionary\n",
    "\n",
    "We use a **real** nested dictionary from the **U.S. Treasury Fiscal Data API** (saved as `data/treasury_debt_to_penny_raw.pkl`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "ed152d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, ['data', 'meta', 'links'])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dict = load_pickle(Path('data/treasury_debt_to_penny_raw.pkl'))\n",
    "type(raw_dict), list(raw_dict.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d6549f",
   "metadata": {},
   "source": [
    "### Part A — Extract to lists\n",
    "\n",
    "**Exercise:** using a `for` loop, extract these into Python lists:\n",
    "- `record_date`\n",
    "- `tot_pub_debt_out_amt`\n",
    "- `debt_held_public_amt`\n",
    "- `intragov_hold_amt`\n",
    "\n",
    "Then build a DataFrame with those columns and convert numeric columns to floats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "0583aa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  record_date  tot_pub_debt_out_amt  debt_held_public_amt  intragov_hold_amt\n",
      "0  2024-01-02          3.399013e+13          2.696633e+13       7.023794e+12\n",
      "1  2024-01-03          3.399422e+13          2.696795e+13       7.026266e+12\n",
      "2  2024-01-04          3.400627e+13          2.697693e+13       7.029342e+12\n",
      "3  2024-01-05          3.400969e+13          2.697608e+13       7.033606e+12\n",
      "4  2024-01-08          3.401220e+13          2.697394e+13       7.038259e+12\n"
     ]
    }
   ],
   "source": [
    "data = raw_dict.get(\"data\", []) if isinstance(raw_dict, dict) else []\n",
    "# Take the \"data\" field from the dictionary.\n",
    "# If raw_dict is not a dict, use an empty list to avoid errors.\n",
    "\n",
    "record_dates = []\n",
    "tot_debt = []\n",
    "public_debt = []\n",
    "intragov = []\n",
    "# Create empty Python lists where we will store values row by row.\n",
    "\n",
    "for row in data:\n",
    "    # Loop over each observation (each element should be a dict).\n",
    "    record_dates.append(row.get(\"record_date\"))\n",
    "    # Extract the date string and store it.\n",
    "    tot_debt.append(row.get(\"tot_pub_debt_out_amt\"))\n",
    "    # Extract total public debt outstanding.\n",
    "    public_debt.append(row.get(\"debt_held_public_amt\"))\n",
    "    # Extract debt held by the public.\n",
    "    intragov.append(row.get(\"intragov_hold_amt\"))\n",
    "    # Extract intragovernmental holdings.\n",
    "\n",
    "debt_df = pd.DataFrame({\n",
    "    \"record_date\": pd.to_datetime(record_dates, errors=\"coerce\"),\n",
    "    # Convert record_dates into datetime; invalid dates become NaT.\n",
    "    \"tot_pub_debt_out_amt\": pd.to_numeric(pd.Series(tot_debt), errors=\"coerce\"),\n",
    "    # Convert total debt values to numeric; invalid become NaN.\n",
    "    \"debt_held_public_amt\": pd.to_numeric(pd.Series(public_debt), errors=\"coerce\"),\n",
    "    # Convert public debt values to numeric; invalid become NaN.\n",
    "    \"intragov_hold_amt\": pd.to_numeric(pd.Series(intragov), errors=\"coerce\"),\n",
    "    # Convert intragov values to numeric; invalid become NaN.\n",
    "}).dropna(subset=[\"record_date\"]).sort_values(\"record_date\").reset_index(drop=True)\n",
    "# Drop rows with missing dates, sort by date, and reset the index.\n",
    "\n",
    "print(debt_df.head())\n",
    "# Show the first rows to verify the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eddf1cd",
   "metadata": {},
   "source": [
    "### Part B — Turn it into a function\n",
    "\n",
    "**Exercise:** define:\n",
    "`treasury_debt_dict_to_df(obj: dict) -> pd.DataFrame`\n",
    "\n",
    "Return a clean DataFrame sorted by date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "344af1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  record_date  tot_pub_debt_out_amt  debt_held_public_amt  intragov_hold_amt\n",
      "0  2024-01-02          3.399013e+13          2.696633e+13       7.023794e+12\n",
      "1  2024-01-03          3.399422e+13          2.696795e+13       7.026266e+12\n",
      "2  2024-01-04          3.400627e+13          2.697693e+13       7.029342e+12\n",
      "3  2024-01-05          3.400969e+13          2.697608e+13       7.033606e+12\n",
      "4  2024-01-08          3.401220e+13          2.697394e+13       7.038259e+12\n"
     ]
    }
   ],
   "source": [
    "data = raw_dict.get(\"data\", []) if isinstance(raw_dict, dict) else []\n",
    "# Take the \"data\" field from the dictionary.\n",
    "# If raw_dict is not a dict, use an empty list to avoid errors.\n",
    "\n",
    "record_dates = []\n",
    "tot_debt = []\n",
    "public_debt = []\n",
    "intragov = []\n",
    "# Create empty Python lists where we will store values row by row.\n",
    "\n",
    "for row in data:\n",
    "    # Loop over each observation (each element should be a dict).\n",
    "    record_dates.append(row.get(\"record_date\"))\n",
    "    # Extract the date string and store it.\n",
    "    tot_debt.append(row.get(\"tot_pub_debt_out_amt\"))\n",
    "    # Extract total public debt outstanding.\n",
    "    public_debt.append(row.get(\"debt_held_public_amt\"))\n",
    "    # Extract debt held by the public.\n",
    "    intragov.append(row.get(\"intragov_hold_amt\"))\n",
    "    # Extract intragovernmental holdings.\n",
    "\n",
    "debt_df = pd.DataFrame({\n",
    "    \"record_date\": pd.to_datetime(record_dates, errors=\"coerce\"),\n",
    "    # Convert record_dates into datetime; invalid dates become NaT.\n",
    "    \"tot_pub_debt_out_amt\": pd.to_numeric(pd.Series(tot_debt), errors=\"coerce\"),\n",
    "    # Convert total debt values to numeric; invalid become NaN.\n",
    "    \"debt_held_public_amt\": pd.to_numeric(pd.Series(public_debt), errors=\"coerce\"),\n",
    "    # Convert public debt values to numeric; invalid become NaN.\n",
    "    \"intragov_hold_amt\": pd.to_numeric(pd.Series(intragov), errors=\"coerce\"),\n",
    "    # Convert intragov values to numeric; invalid become NaN.\n",
    "}).dropna(subset=[\"record_date\"]).sort_values(\"record_date\").reset_index(drop=True)\n",
    "# Drop rows with missing dates, sort by date, and reset the index.\n",
    "\n",
    "print(debt_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532427b",
   "metadata": {},
   "source": [
    "### Part C — Row iteration\n",
    "\n",
    "**Exercise:**\n",
    "1. Create `raw_table = pd.DataFrame(raw_dict['data'])`.\n",
    "2. Iterate over rows (e.g. `.iterrows()`), and build `debt_small` with only:\n",
    "   `record_date` and `tot_pub_debt_out_amt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "4ee81c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  record_date  tot_pub_debt_out_amt\n",
      "0  2024-01-02          3.399013e+13\n",
      "1  2024-01-03          3.399422e+13\n",
      "2  2024-01-04          3.400627e+13\n",
      "3  2024-01-05          3.400969e+13\n",
      "4  2024-01-08          3.401220e+13\n"
     ]
    }
   ],
   "source": [
    "data = raw_dict.get(\"data\", []) if isinstance(raw_dict, dict) else []\n",
    "# Pull the list of rows from raw_dict[\"data\"].\n",
    "# If raw_dict is not a dict, fall back to an empty list.\n",
    "\n",
    "raw_table = pd.DataFrame(data)\n",
    "# Build a full DataFrame from the raw list of dictionaries.\n",
    "\n",
    "rows = []\n",
    "# We'll collect the small subset of fields here (as a list of dictionaries).\n",
    "\n",
    "if not raw_table.empty:\n",
    "    # Only iterate if there is something to iterate over.\n",
    "\n",
    "    for _, row in raw_table.iterrows():\n",
    "        # iterrows() gives (index, row) where row behaves like a Series.\n",
    "\n",
    "        rows.append({\n",
    "            \"record_date\": row.get(\"record_date\"),\n",
    "            # Keep only the date.\n",
    "            \"tot_pub_debt_out_amt\": row.get(\"tot_pub_debt_out_amt\"),\n",
    "            # Keep only total public debt.\n",
    "        })\n",
    "        # Append one small dictionary per row.\n",
    "\n",
    "debt_small = pd.DataFrame(rows)\n",
    "# Convert the list of dictionaries into a DataFrame.\n",
    "\n",
    "if not debt_small.empty:\n",
    "    # Only clean/convert if the DataFrame has rows.\n",
    "\n",
    "    debt_small[\"record_date\"] = pd.to_datetime(debt_small[\"record_date\"], errors=\"coerce\")\n",
    "    # Convert date strings to datetime; invalid values become NaT.\n",
    "\n",
    "    debt_small[\"tot_pub_debt_out_amt\"] = pd.to_numeric(debt_small[\"tot_pub_debt_out_amt\"], errors=\"coerce\")\n",
    "    # Convert the debt column to numeric; invalid values become NaN.\n",
    "\n",
    "    debt_small = debt_small.dropna(subset=[\"record_date\"]).sort_values(\"record_date\").reset_index(drop=True)\n",
    "    # Drop missing dates, sort by date, and reset index to keep it clean.\n",
    "\n",
    "print(debt_small.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0542721a",
   "metadata": {},
   "source": [
    "### <a id='#5.2.'>5.2. Class </a>\n",
    "\n",
    "Practice class basics using the same datasets.\n",
    "###  <a id='#5.2.2.'> 5.2.2. Defining a class </a>\n",
    "\n",
    "**Exercise:** define a class `MacroSeries`.\n",
    "\n",
    "Attributes:\n",
    "- `name` (str)\n",
    "- `source` (str)\n",
    "- `data` (pd.DataFrame with columns: date, value)\n",
    "\n",
    "Methods:\n",
    "- `latest_value()` → float\n",
    "- `n_obs()` → int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "8e40d21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIN latest value: 90898.18358352\n",
      "RIN n_obs: 877\n",
      "DGS10 latest value: 4.16\n",
      "DGS10 n_obs: 1491\n"
     ]
    }
   ],
   "source": [
    "class MacroSeries:\n",
    "    # A small class representing one time series with metadata.\n",
    "\n",
    "    def __init__(self, name: str, source: str, data: pd.DataFrame):\n",
    "        # Store basic metadata.\n",
    "        self.name = name\n",
    "        self.source = source\n",
    "\n",
    "        # Validate input type.\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise TypeError(\"data must be a pandas DataFrame.\")\n",
    "\n",
    "        # Validate required columns.\n",
    "        if \"date\" not in data.columns or \"value\" not in data.columns:\n",
    "            raise ValueError(\"data must contain columns ['date', 'value'].\")\n",
    "\n",
    "        df = data.copy()\n",
    "        # Copy so we do not modify the original DataFrame.\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        # Convert date to datetime; invalid values become NaT.\n",
    "\n",
    "        df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "        # Convert value to numeric; invalid values become NaN.\n",
    "\n",
    "        df = df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "        # Drop missing dates, sort by date, reset index.\n",
    "\n",
    "        self.data = df\n",
    "        # Store cleaned data in the object.\n",
    "\n",
    "    def latest_value(self) -> float:\n",
    "        # Return the latest non-missing value (by time order).\n",
    "        if self.data.empty:\n",
    "            return float(\"nan\")\n",
    "\n",
    "        s = self.data[\"value\"].dropna()\n",
    "        # Drop missing values so the last element is a real number.\n",
    "\n",
    "        if s.empty:\n",
    "            return float(\"nan\")\n",
    "\n",
    "        return float(s.iloc[-1])\n",
    "        # Last observed value.\n",
    "\n",
    "    def n_obs(self) -> int:\n",
    "        # Return the number of non-missing observations.\n",
    "        if self.data.empty:\n",
    "            return 0\n",
    "\n",
    "        return int(self.data[\"value\"].notna().sum())\n",
    "        # Count values that are not NaN.\n",
    "    def label(self) -> str:\n",
    "        return f\"{self.name} [{self.source}]\"\n",
    "    def pct_change(self, periods: int = 1) -> pd.Series:\n",
    "        if self.data.empty:\n",
    "            return pd.Series(dtype=float)\n",
    "\n",
    "        s = self.data.set_index(\"date\")[\"value\"]\n",
    "        out = s.pct_change(periods=periods) * 100.0\n",
    "        out.name = f\"{self.name}_pct_change\"\n",
    "        return out\n",
    "rin_obj = MacroSeries(\"Peru RIN (USD mn)\", \"BCRPData\", rin_std)\n",
    "# Create an object for the RIN series (already standardized to ['date','value']).\n",
    "\n",
    "dgs10_df = macro_us[[\"date\", \"DGS10\"]].rename(columns={\"DGS10\": \"value\"})\n",
    "# Build a ['date','value'] DataFrame for DGS10 so it matches the class requirement.\n",
    "\n",
    "dgs10_obj = MacroSeries(\"DGS10\", \"FRED\", dgs10_df)\n",
    "# Create an object for the DGS10 series.\n",
    "\n",
    "print(\"RIN latest value:\", rin_obj.latest_value())\n",
    "# Print the latest available value in the RIN series.\n",
    "\n",
    "print(\"RIN n_obs:\", rin_obj.n_obs())\n",
    "# Print how many non-missing observations RIN has.\n",
    "\n",
    "print(\"DGS10 latest value:\", dgs10_obj.latest_value())\n",
    "# Print the latest available value in the DGS10 series.\n",
    "\n",
    "print(\"DGS10 n_obs:\", dgs10_obj.n_obs())\n",
    "# Print how many non-missing observations DGS10 has."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc79b29",
   "metadata": {},
   "source": [
    "### <a id='#5.2.3.'> 5.2.3. Attributes </a>\n",
    "\n",
    "**Exercise:** access and modify attributes.\n",
    "\n",
    "1. Print `rin_obj.name` and `rin_obj.source`.\n",
    "2. Update `rin_obj.name` to `Peru RIN (USD mn)`.\n",
    "3. Print again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "1948c68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peru RIN (USD mn) BCRPData\n",
      "Peru RIN (USD mn) BCRPData\n"
     ]
    }
   ],
   "source": [
    "print(rin_obj.name, rin_obj.source)\n",
    "rin_obj.name = 'Peru RIN (USD mn)'\n",
    "print(rin_obj.name, rin_obj.source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ba843",
   "metadata": {},
   "source": [
    "### <a id='#5.2.5.'> 5.2.5. Method </a>\n",
    "\n",
    "**Exercise:** implement a method:\n",
    "`pct_change(self, periods: int = 1) -> pd.Series`\n",
    "that returns percent changes of the `value` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "c1049802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "2020-02-04   -0.249300\n",
      "2020-02-05    0.206098\n",
      "2020-02-06    0.032348\n",
      "2020-02-07    0.238824\n",
      "2020-02-10    0.018453\n",
      "Name: Peru RIN (USD mn)_pct_change, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Solution: pct_change() inside MacroSeries\n",
    "out = rin_obj.pct_change(periods=1)\n",
    "print(out.dropna().head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2a6d2",
   "metadata": {},
   "source": [
    "###  <a id='#5.2.6.'> 5.2.6. __init__() </a>\n",
    "\n",
    "**Assignment:** validate inputs inside `__init__`.\n",
    "\n",
    "Modify `__init__` so that if `data` does not have columns `date` and `value`, it raises `ValueError`.\n",
    "Then intentionally try to create a bad object and confirm it raises an error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "32da6b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected error: ValueError(\"data must contain columns ['date', 'value'].\")\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bad = MacroSeries(\"BAD\", \"TEST\", pd.DataFrame({\"x\":[1,2], \"y\":[3,4]}))\n",
    "    print(\"If you see this, you did not validate columns yet.\")\n",
    "except Exception as e:\n",
    "    print(\"Expected error:\", repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0093df",
   "metadata": {},
   "source": [
    "### <a id='#5.2.7.'> 5.2.7. Self</a>\n",
    "\n",
    "**Exercise:** use `self` to build a label.\n",
    "\n",
    "Add `label(self) -> str` returning `'{name} [{source}]'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "6b876778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peru RIN (USD mn) [BCRPData]\n"
     ]
    }
   ],
   "source": [
    "print(rin_obj.label())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73d9843",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "**Mini-project (Exercise):** create `MacroDashboard` to manage multiple series.\n",
    "\n",
    "Class requirements:\n",
    "- attribute `series_list` (list)\n",
    "- method `add(self, s: MacroSeries) -> None`\n",
    "- method `to_wide(self) -> pd.DataFrame` that merges all series on date (wide format)\n",
    "\n",
    "Test it by adding `rin_obj` and `dgs10_obj`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "ff140523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  Peru RIN (USD mn)  DGS10\n",
      "0 2020-01-01                NaN    NaN\n",
      "1 2020-01-02                NaN   1.88\n",
      "2 2020-01-03                NaN   1.80\n",
      "3 2020-01-06                NaN   1.81\n",
      "4 2020-01-07                NaN   1.83\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MacroDashboard:\n",
    "    def __init__(self):\n",
    "        # We keep all MacroSeries objects in a simple Python list.\n",
    "        self.series_list = []\n",
    "\n",
    "    def add(self, s: MacroSeries) -> None:\n",
    "        # Add one MacroSeries object to the dashboard.\n",
    "        if not isinstance(s, MacroSeries):\n",
    "            raise TypeError(\"s must be a MacroSeries instance.\")\n",
    "        self.series_list.append(s)\n",
    "\n",
    "    def to_wide(self) -> pd.DataFrame:\n",
    "        # Merge all stored series on date into a single wide DataFrame.\n",
    "        if len(self.series_list) == 0:\n",
    "            return pd.DataFrame(columns=[\"date\"])\n",
    "\n",
    "        wide = None\n",
    "        for s in self.series_list:\n",
    "            # Take only date/value and rename value to the series name.\n",
    "            df = s.data[[\"date\", \"value\"]].copy()\n",
    "            df = df.rename(columns={\"value\": s.name})\n",
    "\n",
    "            if wide is None:\n",
    "                # Start with the first series.\n",
    "                wide = df\n",
    "            else:\n",
    "                # Outer merge keeps the union of all dates across series.\n",
    "                wide = wide.merge(df, on=\"date\", how=\"outer\")\n",
    "\n",
    "        # Clean final formatting.\n",
    "        wide[\"date\"] = pd.to_datetime(wide[\"date\"], errors=\"coerce\")\n",
    "        wide = wide.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "        return wide\n",
    "\n",
    "\n",
    "dash = MacroDashboard()\n",
    "dash.add(rin_obj)\n",
    "dash.add(dgs10_obj)\n",
    "\n",
    "wide_df = dash.to_wide()\n",
    "print(wide_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113736a",
   "metadata": {},
   "source": [
    "# <a id='#5.3.'>5.3. References </a>\n",
    "\n",
    "- BCRPData API Guide: https://estadisticas.bcrp.gob.pe/estadisticas/series/ayuda/api\n",
    "- FRED: https://fred.stlouisfed.org/\n",
    "- U.S. Treasury Fiscal Data API: https://fiscaldata.treasury.gov/api-documentation/\n",
    "- Type hints (typing): https://docs.python.org/3/library/typing.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d698b",
   "metadata": {},
   "source": [
    "## Setup data\n",
    "We will use:\n",
    "- **Peru (BCRP)**: daily USD/PEN buy & sell → build a mid-rate\n",
    "- **US (FRED)**: unemployment rate (UNRATE) and CPI (CPIAUCSL)\n",
    "- **US (Treasury)**: Debt to the Penny dataset\n",
    "\n",
    "If a download fails, the notebook continues with empty DataFrames (so you can still practice OOP patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "d00c939d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        date  PEN_USD_buy  PEN_USD_sell\n",
       " 0 2018-02-01     3.211429      3.212571\n",
       " 1 2018-02-02     3.217143      3.218143\n",
       " 2 2018-02-05     3.224714      3.226000\n",
       " 3 2018-02-06     3.248857      3.250571\n",
       " 4 2018-02-07     3.247000      3.248571,\n",
       "         date  UNRATE  CPIAUCSL\n",
       " 0 2018-01-01     4.0   248.859\n",
       " 1 2018-02-01     4.1   249.529\n",
       " 2 2018-03-01     4.0   249.577\n",
       " 3 2018-04-01     4.0   250.227\n",
       " 4 2018-05-01     3.8   250.792,\n",
       "   record_date  tot_pub_debt_out_amt  debt_held_public_amt  intragov_hold_amt\n",
       " 0  2022-01-03          2.956191e+13          2.314436e+13       6.417552e+12\n",
       " 1  2022-01-04          2.968480e+13          2.321183e+13       6.472963e+12\n",
       " 2  2022-01-05          2.968871e+13          2.321414e+13       6.474572e+12\n",
       " 3  2022-01-06          2.971127e+13          2.322997e+13       6.481301e+12\n",
       " 4  2022-01-07          2.971221e+13          2.322965e+13       6.482560e+12)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START = \"2018-01-01\"\n",
    "END = \"2025-12-18\"\n",
    "\n",
    "# BCRP: daily exchange rate (buy/sell)\n",
    "try:\n",
    "    fx = bcrp_get([\"PD04637PD\",\"PD04638PD\"], start=START, end=END).rename(\n",
    "        columns={\"PD04637PD\":\"PEN_USD_buy\", \"PD04638PD\":\"PEN_USD_sell\"}\n",
    "    )\n",
    "except Exception:\n",
    "    fx = pd.DataFrame(columns=[\"date\",\"PEN_USD_buy\",\"PEN_USD_sell\"])\n",
    "\n",
    "# FRED: two series (monthly/annual frequency depends on series)\n",
    "try:\n",
    "    fred = fred_get([\"UNRATE\", \"CPIAUCSL\"], start=START)\n",
    "except Exception:\n",
    "    fred = pd.DataFrame(columns=[\"date\",\"UNRATE\",\"CPIAUCSL\"])\n",
    "\n",
    "# Treasury: debt to the penny (daily)\n",
    "try:\n",
    "    debt_raw = treasury_get_debt_to_penny(start_date=\"2022-01-01\")  # dict\n",
    "    debt = pd.DataFrame(debt_raw.get(\"data\", []))                   # DataFrame\n",
    "\n",
    "    if not debt.empty:\n",
    "        debt[\"record_date\"] = pd.to_datetime(debt[\"record_date\"], errors=\"coerce\")\n",
    "        for c in [\"tot_pub_debt_out_amt\", \"debt_held_public_amt\", \"intragov_hold_amt\"]:\n",
    "            if c in debt.columns:\n",
    "                debt[c] = pd.to_numeric(debt[c], errors=\"coerce\")\n",
    "\n",
    "        debt = debt.dropna(subset=[\"record_date\"]).sort_values(\"record_date\").reset_index(drop=True)\n",
    "\n",
    "except Exception:\n",
    "    debt = pd.DataFrame(columns=[\n",
    "        \"record_date\", \"tot_pub_debt_out_amt\", \"debt_held_public_amt\", \"intragov_hold_amt\"\n",
    "    ])\n",
    "\n",
    "\n",
    "safe_head(fx), safe_head(fred), safe_head(debt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf66bd",
   "metadata": {},
   "source": [
    "## <a id='6.1.'>6.1. Inheritance</a>\n",
    "\n",
    "Inheritance lets you define a base class and reuse/extend behavior in subclasses.\n",
    "### <a id='6.1.1.'> 6.1.1. Single Inheritance </a>\n",
    "\n",
    "We define a base `MacroSource` and then subclass it for BCRP and FRED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "87248d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacroSource:\n",
    "    \"\"\"Base class for macro/financial data sources.\"\"\"\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "\n",
    "    def fetch(self) -> pd.DataFrame:\n",
    "        raise NotImplementedError(\"Subclasses must implement fetch().\")\n",
    "class BCRPSource(MacroSource):\n",
    "    \"\"\"Fetch one or more BCRP series into a DataFrame.\"\"\"\n",
    "    def __init__(self, series_codes, start: str, end: str):\n",
    "        super().__init__(name=\"BCRPData\")\n",
    "        self.series_codes = series_codes\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "    def fetch(self) -> pd.DataFrame:\n",
    "        return bcrp_get(self.series_codes, start=self.start, end=self.end)\n",
    "\n",
    "class FREDSource(MacroSource):\n",
    "    \"\"\"Fetch one or more FRED series into a DataFrame.\"\"\"\n",
    "    def __init__(self, series_ids, start: str):\n",
    "        super().__init__(name=\"FRED\")\n",
    "        self.series_ids = series_ids\n",
    "        self.start = start\n",
    "\n",
    "    def fetch(self) -> pd.DataFrame:\n",
    "        return fred_get(self.series_ids, start=self.start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbacd73",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "1. Create `BCRPSource([\"PD04637PD\",\"PD04638PD\"], start=START, end=END)` and fetch it.\n",
    "2. Create a `PEN_USD_mid` column as the average of buy/sell.\n",
    "3. Create `FREDSource([\"UNRATE\",\"CPIAUCSL\"], start=START)` and fetch it.\n",
    "4. Keep only the last 24 rows of the FRED data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "bb4d457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        date  PEN_USD_buy  PEN_USD_sell  PEN_USD_mid\n",
       " 0 2018-02-01     3.211429      3.212571     3.212000\n",
       " 1 2018-02-02     3.217143      3.218143     3.217643\n",
       " 2 2018-02-05     3.224714      3.226000     3.225357\n",
       " 3 2018-02-06     3.248857      3.250571     3.249714\n",
       " 4 2018-02-07     3.247000      3.248571     3.247786,\n",
       "         date  UNRATE  CPIAUCSL\n",
       " 0 2023-12-01     3.8   308.735\n",
       " 1 2024-01-01     3.7   309.794\n",
       " 2 2024-02-01     3.9   311.022\n",
       " 3 2024-03-01     3.9   312.107\n",
       " 4 2024-04-01     3.9   313.016)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the BCRP source object (it knows which codes and date window to use).\n",
    "bcrp_src = BCRPSource([\"PD04637PD\", \"PD04638PD\"], start=START, end=END)\n",
    "\n",
    "# Fetch the FX data from BCRP (returns a DataFrame with 'date' and the two series codes).\n",
    "fx_df = bcrp_src.fetch()\n",
    "\n",
    "# Rename the raw code columns to clearer names (buy/sell).\n",
    "if isinstance(fx_df, pd.DataFrame) and (not fx_df.empty):\n",
    "    fx_df = fx_df.rename(columns={\n",
    "        \"PD04637PD\": \"PEN_USD_buy\",\n",
    "        \"PD04638PD\": \"PEN_USD_sell\",\n",
    "    })\n",
    "\n",
    "    # Ensure we are working with clean types and ordered by time.\n",
    "    if \"date\" in fx_df.columns:\n",
    "        fx_df[\"date\"] = pd.to_datetime(fx_df[\"date\"], errors=\"coerce\")\n",
    "    for c in [\"PEN_USD_buy\", \"PEN_USD_sell\"]:\n",
    "        if c in fx_df.columns:\n",
    "            fx_df[c] = pd.to_numeric(fx_df[c], errors=\"coerce\")\n",
    "\n",
    "    fx_df = fx_df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # Create the mid-rate as the simple average of buy and sell.\n",
    "    if (\"PEN_USD_buy\" in fx_df.columns) and (\"PEN_USD_sell\" in fx_df.columns):\n",
    "        fx_df[\"PEN_USD_mid\"] = (fx_df[\"PEN_USD_buy\"] + fx_df[\"PEN_USD_sell\"]) / 2.0\n",
    "\n",
    "# Create the FRED source object (it knows which series IDs and start date to use).\n",
    "fred_src = FREDSource([\"UNRATE\", \"CPIAUCSL\"], start=START)\n",
    "\n",
    "# Fetch the FRED data (returns a DataFrame with 'date' and the series columns).\n",
    "fred_df = fred_src.fetch()\n",
    "\n",
    "# Keep only the last 24 rows (most recent observations).\n",
    "fred_last24 = pd.DataFrame()\n",
    "if isinstance(fred_df, pd.DataFrame) and (not fred_df.empty):\n",
    "    if \"date\" in fred_df.columns:\n",
    "        fred_df = fred_df.copy()\n",
    "        fred_df[\"date\"] = pd.to_datetime(fred_df[\"date\"], errors=\"coerce\")\n",
    "        fred_df = fred_df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "    fred_last24 = fred_df.tail(24).reset_index(drop=True)\n",
    "\n",
    "# Optional self-check\n",
    "safe_head(fx_df), safe_head(fred_last24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2716b0",
   "metadata": {},
   "source": [
    "### <a id='6.1.2.'> 6.1.2. Multiple Inheritance </a>\n",
    "\n",
    "Use mixins to add caching metadata and validation behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "e42b16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheMixin:\n",
    "    \"\"\"Mixin: creates a stable cache key for an instance.\"\"\"\n",
    "    def cache_key(self) -> str:\n",
    "        return _hash_key(self.__class__.__name__, getattr(self, \"name\", \"\"), str(getattr(self, \"series_codes\", \"\")), str(getattr(self, \"series_ids\", \"\")))\n",
    "\n",
    "class ValidateDateMixin:\n",
    "    \"\"\"Mixin: validates a DataFrame has a 'date' column and it is datetime-like.\"\"\"\n",
    "    def validate_date(self, df: pd.DataFrame) -> None:\n",
    "        if \"date\" not in df.columns:\n",
    "            raise ValueError(\"Missing required column: 'date'\")\n",
    "        # try convertibility\n",
    "        if not np.issubdtype(pd.to_datetime(df[\"date\"], errors=\"coerce\").dtype, np.datetime64):\n",
    "            raise ValueError(\"'date' column cannot be parsed as datetime.\")\n",
    "class CachedValidatedBCRP(CacheMixin, ValidateDateMixin, BCRPSource):\n",
    "    \"\"\"BCRPSource + cache key + date validation.\"\"\"\n",
    "    def fetch(self) -> pd.DataFrame:\n",
    "        df = super().fetch()\n",
    "        if isinstance(df, pd.DataFrame) and df.shape[0] > 0:\n",
    "            self.validate_date(df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d318abc",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "1. Instantiate `CachedValidatedBCRP([\"PD04637PD\",\"PD04638PD\"], START, END)`.\n",
    "2. Print its `.cache_key()`.\n",
    "3. Call `.fetch()` and confirm `date` validation passes.\n",
    "4. Intentionally break validation by renaming the `date` column, then call `.validate_date()` inside a `try/except`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "e7126277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache key: a80d86ec12cca2e497e6adf2\n",
      "Expected validation error: ValueError(\"Missing required column: 'date'\")\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate the CachedValidatedBCRP object with the two FX codes and date window.\n",
    "cv = CachedValidatedBCRP([\"PD04637PD\", \"PD04638PD\"], start=START, end=END)\n",
    "\n",
    "# 2) Compute and store the cache key, then print it.\n",
    "ck = cv.cache_key()\n",
    "print(\"cache key:\", ck)\n",
    "\n",
    "# 3) Fetch the data. Inside fetch(), validate_date(df) is called if df is not empty.\n",
    "#    If validation fails, it will raise an error.\n",
    "cv_df = cv.fetch()\n",
    "\n",
    "# Optional quick look\n",
    "safe_head(cv_df)\n",
    "\n",
    "# 4) Intentionally break the validation:\n",
    "#    Rename 'date' -> 'date_broken', then call validate_date() in a try/except.\n",
    "broken = cv_df.rename(columns={\"date\": \"date_broken\"}).copy()\n",
    "\n",
    "try:\n",
    "    cv.validate_date(broken)   # This should raise ValueError (missing 'date')\n",
    "    print(\"If you see this, validation did NOT fail (unexpected).\")\n",
    "except Exception as e:\n",
    "    print(\"Expected validation error:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cedf08a",
   "metadata": {},
   "source": [
    "# Deeper Topics about Class\n",
    "\n",
    "## <a id='6.2.'>6.2. Private Variables </a>\n",
    "\n",
    "Python uses conventions and name mangling rather than strict privacy.\n",
    "#### Example\n",
    "\n",
    "We build a class that stores a dataset and keeps a private cache of computed results (e.g., latest values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "756ffc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacroAnalyzer:\n",
    "    def __init__(self, label: str, df: pd.DataFrame):\n",
    "        self.label = label\n",
    "        self.df = df.copy()\n",
    "\n",
    "        self._prepared = False\n",
    "        self.__cache = {}\n",
    "\n",
    "        # If there is a date column, clean and sort once so \"latest\" is truly latest.\n",
    "        if \"date\" in self.df.columns:\n",
    "            self.df[\"date\"] = pd.to_datetime(self.df[\"date\"], errors=\"coerce\")\n",
    "            self.df = self.df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    def prepare_fx_mid(self) -> None:\n",
    "        \"\"\"Prepare a mid exchange rate if the expected columns exist.\"\"\"\n",
    "        if {\"PEN_USD_buy\", \"PEN_USD_sell\"}.issubset(self.df.columns):\n",
    "            self.df[\"PEN_USD_mid\"] = (self.df[\"PEN_USD_buy\"] + self.df[\"PEN_USD_sell\"]) / 2.0\n",
    "\n",
    "            # Data changed → clear cache so \"latest\" recomputes correctly.\n",
    "            self.__cache = {}\n",
    "\n",
    "        self._prepared = True\n",
    "\n",
    "    def latest(self, col: str) -> float | None:\n",
    "        \"\"\"Return the latest non-missing value of a column (cached).\"\"\"\n",
    "        key = f\"latest:{col}\"\n",
    "\n",
    "        if key not in self.__cache:\n",
    "            if (col not in self.df.columns) or self.df.empty:\n",
    "                self.__cache[key] = None\n",
    "            else:\n",
    "                s = pd.to_numeric(self.df[col], errors=\"coerce\").dropna()\n",
    "                self.__cache[key] = float(s.iloc[-1]) if not s.empty else None\n",
    "\n",
    "        return self.__cache[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e24052b",
   "metadata": {},
   "source": [
    "### <a id='6.2.1.'>6.2.1. Name Mangling </a>\n",
    "\n",
    "`__cache` becomes `_MacroAnalyzer__cache` internally.\n",
    "#### Exercise\n",
    "\n",
    "1. Create `MacroAnalyzer(\"FX\", fx)` using the `fx` DataFrame from setup.\n",
    "2. Call `.prepare_fx_mid()`.\n",
    "3. Call `.latest(\"PEN_USD_mid\")`.\n",
    "4. Print `_prepared`.\n",
    "5. Try to access `an.__cache` directly (catch the AttributeError).\n",
    "6. Access it using the mangled name and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "4734067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_prepared: True\n",
      "Direct access error (expected): AttributeError(\"'MacroAnalyzer' object has no attribute '__cache'\")\n",
      "Mangled cache: {'latest:PEN_USD_mid': 3.361607142857145}\n",
      "mid_latest: 3.361607142857145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Create the analyzer object using the FX DataFrame.\n",
    "an = MacroAnalyzer(\"FX\", fx)\n",
    "\n",
    "# 2) Prepare the mid-rate column (if buy/sell columns exist).\n",
    "an.prepare_fx_mid()\n",
    "\n",
    "# 3) Get the latest mid-rate value (uses the private cache internally).\n",
    "mid_latest = an.latest(\"PEN_USD_mid\")\n",
    "\n",
    "# 4) Print the \"_prepared\" flag (single underscore = \"private by convention\").\n",
    "print(\"_prepared:\", an._prepared)\n",
    "\n",
    "# 5) Try to access the double-underscore cache directly (should fail due to name mangling).\n",
    "try:\n",
    "    print(an.__cache)\n",
    "except Exception as e:\n",
    "    print(\"Direct access error (expected):\", repr(e))\n",
    "\n",
    "# 6) Access the cache using the mangled attribute name and print it.\n",
    "print(\"Mangled cache:\", an._MacroAnalyzer__cache)\n",
    "\n",
    "print(\"mid_latest:\", mid_latest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc75f58",
   "metadata": {},
   "source": [
    "### <a id='6.2.2.'> 6.2.2. \\_Single Leading Underscores </a>\n",
    "\n",
    "A single underscore means internal use by convention.\n",
    "#### Exercise\n",
    "\n",
    "add an internal method `_scale_to_index(base=100)` to `MacroAnalyzer`.\n",
    "\n",
    "- If `col` exists (choose a column), compute an index:\n",
    "  - index_t = 100 * value_t / value_first_non_missing\n",
    "- Store it in a new column, e.g., `UNRATE_index` or `PEN_USD_mid_index`.\n",
    "\n",
    "Write only the method inside the class, then run it on a FRED column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "d3aaedfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scale_to_index(self, col: str, base: float = 100.0) -> None:\n",
    "    \"\"\"\n",
    "    Scale one existing column into an index.\n",
    "\n",
    "    Rule:\n",
    "      index_t = base * value_t / first_non_missing_value\n",
    "\n",
    "    This method:\n",
    "    - does nothing if the DataFrame is empty or the column does not exist\n",
    "    - creates a new column named f\"{col}_index\"\n",
    "    - clears the internal cache because the DataFrame changes\n",
    "    \"\"\"\n",
    "\n",
    "    # If self.df is not a DataFrame or has no rows, there is nothing to do.\n",
    "    if (not isinstance(self.df, pd.DataFrame)) or self.df.empty:\n",
    "        return\n",
    "\n",
    "    # If the requested column is not in the DataFrame, exit silently.\n",
    "    if col not in self.df.columns:\n",
    "        return\n",
    "\n",
    "    # Convert the column to numeric; non-numeric values become NaN.\n",
    "    s = pd.to_numeric(self.df[col], errors=\"coerce\")\n",
    "\n",
    "    # Find the first non-missing value (used as the base level for the index).\n",
    "    first_valid = s.dropna()\n",
    "    if first_valid.empty:\n",
    "        return\n",
    "\n",
    "    # The index uses the first non-missing value in the denominator.\n",
    "    first_value = float(first_valid.iloc[0])\n",
    "\n",
    "    # Avoid division by zero.\n",
    "    if first_value == 0:\n",
    "        return\n",
    "\n",
    "    # Create the new index column name.\n",
    "    new_col = f\"{col}_index\"\n",
    "\n",
    "    # Compute the index: base * (current value / first value).\n",
    "    self.df[new_col] = float(base) * (s / first_value)\n",
    "\n",
    "    # The DataFrame changed, so clear the internal cache (prevents stale results).\n",
    "    if hasattr(self, \"_MacroAnalyzer__cache\"):\n",
    "        self._MacroAnalyzer__cache = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8292f21b",
   "metadata": {},
   "source": [
    "### <a id='6.2.3.'> 6.2.3. \\_\\_Double Leading Underscores </a>\n",
    "\n",
    "Double underscore triggers name mangling to reduce accidental collisions in subclasses.\n",
    "#### Exercise\n",
    "\n",
    "Demonstrate why double-underscore name mangling helps avoid accidental collisions in subclasses.\n",
    "\n",
    "1. Create a subclass `MacroAnalyzerChild(MacroAnalyzer)` that defines its own `__cache` attribute in `__init__`.\n",
    "2. Instantiate the child with a small DataFrame (e.g., `fx.head()`).\n",
    "3. Show that the parent cache and child cache are stored under different mangled names.\n",
    "4. Print the keys of `child.__dict__` and explain (in 2–3 sentences) what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "e2aadc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent mangled cache name exists: True\n",
      "Parent cache: {}\n",
      "Child mangled cache name exists: True\n",
      "Child cache: {'child_cache': True}\n",
      "child.__dict__ keys: ['label', 'df', '_prepared', '_MacroAnalyzer__cache', '_MacroAnalyzerChild__cache']\n"
     ]
    }
   ],
   "source": [
    "class MacroAnalyzerChild(MacroAnalyzer):\n",
    "    def __init__(self, label: str, df: pd.DataFrame):\n",
    "        # Initialize the parent part (this creates MacroAnalyzer's own __cache).\n",
    "        super().__init__(label, df)\n",
    "\n",
    "        # Define a separate __cache in the child class (this will be mangled differently).\n",
    "        self.__cache = {\"child_cache\": True}\n",
    "\n",
    "\n",
    "# 2) Instantiate the child with a small DataFrame\n",
    "child = MacroAnalyzerChild(\"FX-child\", fx.head())\n",
    "\n",
    "# 3) Show that parent cache and child cache live under different mangled names\n",
    "# Parent (MacroAnalyzer) private cache:\n",
    "print(\"Parent mangled cache name exists:\", \"_MacroAnalyzer__cache\" in child.__dict__)\n",
    "print(\"Parent cache:\", child._MacroAnalyzer__cache)\n",
    "\n",
    "# Child (MacroAnalyzerChild) private cache:\n",
    "print(\"Child mangled cache name exists:\", \"_MacroAnalyzerChild__cache\" in child.__dict__)\n",
    "print(\"Child cache:\", child._MacroAnalyzerChild__cache)\n",
    "\n",
    "# 4) Print keys of __dict__ (shows the mangled attribute names)\n",
    "print(\"child.__dict__ keys:\", list(child.__dict__.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f789b9",
   "metadata": {},
   "source": [
    "### <a id='6.3.'>6.3. Fixed attributes </a>\n",
    "\n",
    "Restrict attributes using `__slots__` and expose read-only properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "92fc8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreasuryDebt:\n",
    "    __slots__ = (\"_df\", \"_start_date\")\n",
    "\n",
    "    def __init__(self, start_date: str = \"2022-01-01\"):\n",
    "        self._start_date = str(start_date)\n",
    "\n",
    "        raw = treasury_get_debt_to_penny(start_date=self._start_date)\n",
    "\n",
    "        data = raw.get(\"data\", []) if isinstance(raw, dict) else []\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        if not df.empty:\n",
    "            df[\"record_date\"] = pd.to_datetime(df[\"record_date\"], errors=\"coerce\")\n",
    "            for c in [\"tot_pub_debt_out_amt\", \"debt_held_public_amt\", \"intragov_hold_amt\"]:\n",
    "                if c in df.columns:\n",
    "                    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "            df = df.dropna(subset=[\"record_date\"]).sort_values(\"record_date\").reset_index(drop=True)\n",
    "\n",
    "        self._df = df\n",
    "\n",
    "    @property\n",
    "    def start_date(self) -> str:\n",
    "        return self._start_date\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        return self._df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123d7105",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "1. Create `TreasuryDebt(\"2023-01-01\")`.\n",
    "2. Confirm you can read `.df`.\n",
    "3. Try to set `obj.df = ...` and catch the exception.\n",
    "4. Try to add `obj.new_attr = 1` and catch the exception (because of `__slots__`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "e4f633c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected error when setting td.df: AttributeError(\"can't set attribute 'df'\")\n",
      "Expected error when adding td.new_attr: AttributeError(\"'TreasuryDebt' object has no attribute 'new_attr'\")\n"
     ]
    }
   ],
   "source": [
    "# 1) Create the object (stores start_date and builds the internal DataFrame).\n",
    "td = TreasuryDebt(\"2023-01-01\")\n",
    "\n",
    "# 2) Confirm we can read .df (property getter).\n",
    "safe_head(td.df)\n",
    "\n",
    "# 3) Try to overwrite td.df (should fail: df is a read-only @property).\n",
    "try:\n",
    "    td.df = pd.DataFrame()\n",
    "    print(\"If you see this, df was writable (unexpected).\")\n",
    "except Exception as e:\n",
    "    print(\"Expected error when setting td.df:\", repr(e))\n",
    "\n",
    "# 4) Try to add a new attribute (should fail because __slots__ blocks new attrs).\n",
    "try:\n",
    "    td.new_attr = 1\n",
    "    print(\"If you see this, __slots__ did not block new_attr (unexpected).\")\n",
    "except Exception as e:\n",
    "    print(\"Expected error when adding td.new_attr:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eefe3d7",
   "metadata": {},
   "source": [
    "### <a id='6.4.'>6.4. Costumize Exceptions with Python </a>\n",
    "\n",
    "Create your own exception types to make errors clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "dd88434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinanceDataError(Exception):\n",
    "    \"\"\"Base exception for this notebook.\"\"\"\n",
    "\n",
    "class DataDownloadError(FinanceDataError):\n",
    "    pass\n",
    "\n",
    "class DataValidationError(FinanceDataError):\n",
    "    pass\n",
    "\n",
    "def require_nonempty(df: pd.DataFrame, label: str) -> None:\n",
    "    if not isinstance(df, pd.DataFrame) or df.shape[0] == 0:\n",
    "        raise DataDownloadError(f\"{label}: no rows returned (check internet / endpoint / parameters).\")\n",
    "\n",
    "def require_column(df: pd.DataFrame, col: str, label: str) -> None:\n",
    "    if col not in df.columns:\n",
    "        raise DataValidationError(f\"{label}: required column '{col}' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf96c1d",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "1. Use `require_nonempty(fx, \"BCRP FX\")` and handle `DataDownloadError`.\n",
    "2. Use `require_column(fx, \"PEN_USD_buy\", \"BCRP FX\")` and handle `DataValidationError`.\n",
    "3. Intentionally trigger an error by checking a missing column (e.g. `\"NOT_A_COL\"`).\n",
    "4. Print a short message in each except block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "60b2e7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCRP FX: ok (non-empty).\n",
      "BCRP FX: column PEN_USD_buy exists.\n",
      "Expected missing-column error: BCRP FX: required column 'NOT_A_COL' not found.\n"
     ]
    }
   ],
   "source": [
    "# 1) Check that fx is non-empty (download worked)\n",
    "try:\n",
    "    require_nonempty(fx, \"BCRP FX\")\n",
    "    print(\"BCRP FX: ok (non-empty).\")\n",
    "except DataDownloadError as e:\n",
    "    print(\"Download problem:\", str(e))\n",
    "\n",
    "# 2) Check a required column name (note: your FX columns are PENUSD_buy / PENUSD_sell)\n",
    "try:\n",
    "    require_column(fx, \"PEN_USD_buy\", \"BCRP FX\")\n",
    "    print(\"BCRP FX: column PEN_USD_buy exists.\")\n",
    "except DataValidationError as e:\n",
    "    print(\"Validation problem:\", str(e))\n",
    "\n",
    "# 3) Intentionally trigger an error with a missing column\n",
    "try:\n",
    "    require_column(fx, \"NOT_A_COL\", \"BCRP FX\")\n",
    "    print(\"If you see this, NOT_A_COL existed (unexpected).\")\n",
    "except DataValidationError as e:\n",
    "    print(\"Expected missing-column error:\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94b485",
   "metadata": {},
   "source": [
    "## Excersise\n",
    "\n",
    "#### Importing a Dictionary\n",
    "\n",
    "Convert real API output into Python dictionaries and then into pandas objects.\n",
    "##### We can do it lists\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "build a dictionary from the Treasury debt dataset.\n",
    "\n",
    "1. Take the last 15 non-missing rows of `debt`.\n",
    "2. Create two Python lists: `dates` and `total_debt`.\n",
    "3. Build a dictionary: `debt_dict = {date: total_debt}`.\n",
    "4. Convert to a Series named `US_total_debt` and sort by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "2ea92d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2025-11-28    3.839621e+13\n",
       "2025-12-01    3.842567e+13\n",
       "2025-12-02    3.842376e+13\n",
       "2025-12-03    3.840353e+13\n",
       "2025-12-04    3.838937e+13\n",
       "Name: US_total_debt, dtype: float64"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) We will start from the Treasury DataFrame `debt`.\n",
    "#    The goal is to build a Python dictionary {date: total_debt} using the LAST 15 valid rows.\n",
    "\n",
    "tmp = pd.DataFrame()\n",
    "# Create an empty DataFrame placeholder (so the code does not crash if `debt` is missing).\n",
    "\n",
    "if isinstance(debt, pd.DataFrame) and (not debt.empty):\n",
    "    tmp = debt.copy()\n",
    "# If `debt` is a real non-empty DataFrame, copy it into `tmp` (so we do not modify the original).\n",
    "\n",
    "# 2) Check that the required columns exist before continuing.\n",
    "if (not tmp.empty) and (\"record_date\" in tmp.columns) and (\"tot_pub_debt_out_amt\" in tmp.columns):\n",
    "\n",
    "    tmp[\"record_date\"] = pd.to_datetime(tmp[\"record_date\"], errors=\"coerce\")\n",
    "    # Convert record_date into datetime. Bad values become NaT.\n",
    "\n",
    "    tmp[\"tot_pub_debt_out_amt\"] = pd.to_numeric(tmp[\"tot_pub_debt_out_amt\"], errors=\"coerce\")\n",
    "    # Convert the debt amounts into numeric. Bad values become NaN.\n",
    "\n",
    "    tmp = tmp.dropna(subset=[\"record_date\", \"tot_pub_debt_out_amt\"]).sort_values(\"record_date\")\n",
    "    # Keep only rows where BOTH date and total debt exist, then sort by date.\n",
    "\n",
    "    tmp15 = tmp.tail(15)\n",
    "    # Take the LAST 15 valid rows (after sorting by date, these are the most recent 15).\n",
    "\n",
    "    # 3) Create two Python lists using a loop (as the exercise asks).\n",
    "    dates = []\n",
    "    total_debt = []\n",
    "\n",
    "    for _, row in tmp15.iterrows():\n",
    "        # For each row, append the date and the total debt to their lists.\n",
    "        dates.append(row[\"record_date\"])\n",
    "        total_debt.append(float(row[\"tot_pub_debt_out_amt\"]))\n",
    "\n",
    "    # 4) Build the dictionary: date -> total debt value.\n",
    "    debt_dict = {d: v for d, v in zip(dates, total_debt)}\n",
    "\n",
    "    # 5) Convert the dictionary into a pandas Series, name it, and sort by date.\n",
    "    us_total_debt = pd.Series(debt_dict, dtype=float, name=\"US_total_debt\").sort_index()\n",
    "\n",
    "else:\n",
    "    # If columns are missing or the DataFrame is empty, keep outputs defined but empty.\n",
    "    dates = []\n",
    "    total_debt = []\n",
    "    debt_dict = {}\n",
    "    us_total_debt = pd.Series(debt_dict, dtype=float, name=\"US_total_debt\")\n",
    "\n",
    "us_total_debt.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c19863",
   "metadata": {},
   "source": [
    "##### We can do it using iteration over rows of a DataFrame\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "create a dict using `iterrows()`.\n",
    "\n",
    "1. Take the last 10 rows of `fx` (after you create `PEN_USD_mid`).\n",
    "2. Iterate over rows and create `fx_dict` mapping `date -> PEN_USD_mid`.\n",
    "3. Convert to a DataFrame with columns `date`, `PEN_USD_mid`.\n",
    "4. Ensure `date` is datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "898c1a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>PEN_USD_mid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>3.362321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-18</td>\n",
       "      <td>3.367964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-19</td>\n",
       "      <td>3.376429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-20</td>\n",
       "      <td>3.379857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-21</td>\n",
       "      <td>3.388214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  PEN_USD_mid\n",
       "0 2025-11-17     3.362321\n",
       "1 2025-11-18     3.367964\n",
       "2 2025-11-19     3.376429\n",
       "3 2025-11-20     3.379857\n",
       "4 2025-11-21     3.388214"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) First, make sure `fx` is a DataFrame and that we have a mid-rate column.\n",
    "#    Mid-rate = average of buy and sell.\n",
    "\n",
    "fx_dict = {}\n",
    "fx_mid_df = pd.DataFrame(columns=[\"date\", \"PEN_USD_mid\"])\n",
    "\n",
    "if isinstance(fx, pd.DataFrame) and (not fx.empty):\n",
    "\n",
    "    fx2 = fx.copy()\n",
    "    # Work on a copy so we do not change the original `fx`.\n",
    "\n",
    "    # If PEN_USD_mid is not there yet, create it using buy/sell.\n",
    "    if \"PEN_USD_mid\" not in fx2.columns:\n",
    "        if (\"PEN_USD_buy\" in fx2.columns) and (\"PEN_USD_sell\" in fx2.columns):\n",
    "            fx2[\"PEN_USD_mid\"] = (pd.to_numeric(fx2[\"PEN_USD_buy\"], errors=\"coerce\") +\n",
    "                                 pd.to_numeric(fx2[\"PEN_USD_sell\"], errors=\"coerce\")) / 2.0\n",
    "\n",
    "    # 2) Take the last 10 rows (by date) and iterate with iterrows().\n",
    "    if (\"date\" in fx2.columns) and (\"PEN_USD_mid\" in fx2.columns):\n",
    "\n",
    "        fx2[\"date\"] = pd.to_datetime(fx2[\"date\"], errors=\"coerce\")\n",
    "        # Ensure date is datetime.\n",
    "\n",
    "        fx2[\"PEN_USD_mid\"] = pd.to_numeric(fx2[\"PEN_USD_mid\"], errors=\"coerce\")\n",
    "        # Ensure the mid-rate is numeric.\n",
    "\n",
    "        fx2 = fx2.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "        # Drop missing dates and sort so \"tail(10)\" means the most recent 10 dates.\n",
    "\n",
    "        last10 = fx2.tail(10)\n",
    "\n",
    "        # Build fx_dict: date -> PEN_USD_mid\n",
    "        for _, row in last10.iterrows():\n",
    "            d = row[\"date\"]\n",
    "            v = row[\"PEN_USD_mid\"]\n",
    "            if pd.notna(d) and pd.notna(v):\n",
    "                fx_dict[d] = float(v)\n",
    "\n",
    "        # 3) Convert the dict into a DataFrame with the required columns.\n",
    "        fx_mid_df = pd.DataFrame(\n",
    "            {\"date\": list(fx_dict.keys()), \"PEN_USD_mid\": list(fx_dict.values())}\n",
    "        ).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "        # 4) Ensure date is datetime (again, for safety).\n",
    "        fx_mid_df[\"date\"] = pd.to_datetime(fx_mid_df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "fx_mid_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c2b73",
   "metadata": {},
   "source": [
    "### <a id='6.5.'>6.5. References </a>\n",
    "\n",
    "- Python inheritance: https://docs.python.org/3/tutorial/classes.html#inheritance\n",
    "- __slots__: https://docs.python.org/3/reference/datamodel.html#slots\n",
    "- BCRP API help: https://estadisticas.bcrp.gob.pe/estadisticas/series/ayuda/api\n",
    "- FRED CSV endpoint: https://fred.stlouisfed.org/graph/fredgraph.csv\n",
    "- Treasury Fiscal Data API: https://fiscaldata.treasury.gov/api-documentation/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ra_task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
