{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Assignment\n",
    "\n",
    "It is totally prohibited to use any kind of loop. You can use stackoverflow. If you copy codes from previous answers, explain each step. No explanation is `0 points`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tuple\n",
    "\n",
    "\n",
    "\n",
    "1. Print the first item of the second item of `tuple1` object. **Hint: Use indexing**<br><br>\n",
    "2. Print the last item of the `tuple1` object.**Hint: Use indexing**<br><br>\n",
    "3. Change the value \"orange\" for \"pink grapefruit\". Comment the output. **Hint: Is it possible?** <br><br>\n",
    "4. Print the length of `tuple1`. **Hint: Length function**<br><br>\n",
    "5. Sum all the elements of tuple2 and describe your steps and explain each of them. **Hint: Use `map` function.**<br><br>\n",
    "6. Generate a new tuple object named as `tuple3` with the half values of `tuple2`. This tuple should be similar as `tuple2`, tuple of lists.  **Hint: Use `map` funciont.**<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple1 = (\"Orange\", [10, 20, 30], (5, 15, 25))\n",
    "tuple2 = ([7, 8], [9, 1], [10, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#Sol 1.1 Renzo Update\n",
    "# 1) Print the first item of the second item of tuple1 (indexing)\n",
    "# This line defines the tuple named tuple1, containing a string, a list, and a nested tuple.\n",
    "tuple1 = (\"Orange\", [10, 20, 30], (5, 15, 25))\n",
    "# This line selects the second element of tuple1 (index 1), which is the list, and then selects its first element (index 0).\n",
    "print(tuple1[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 15, 25)\n"
     ]
    }
   ],
   "source": [
    "# 2) Print the last item of the tuple1 object (indexing)\n",
    "# This line accesses the last element of tuple1 using negative indexing (-1), which refers to the final item.\n",
    "print(tuple1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pink grapefruit', [10, 20, 30], (5, 15, 25))\n"
     ]
    }
   ],
   "source": [
    "# 3) Change \"Orange\" to \"pink grapefruit\" and comment whether it is possible\n",
    "# This commented-out line illustrates an invalid operation, because tuples are immutable and do not support item reassignment.\n",
    "# tuple1[0] = \"pink grapefruit\"  # TypeError: tuples do not support item assignment.\n",
    "\n",
    "# This line constructs a new tuple by placing the replacement string first (as a 1-item tuple) and concatenating the remaining slice of tuple1.\n",
    "tuple1_new = (\"pink grapefruit\",) + tuple1[1:]\n",
    "\n",
    "# This line prints the newly created tuple, which reflects the desired change without modifying the original tuple1.\n",
    "print(tuple1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# 4) Print the length of tuple1 (length function)\n",
    "# This line computes the number of top-level elements contained in tuple1 using the built-in len() function.\n",
    "print(len(tuple1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# 5) Sum all elements of tuple2 using map (no loops)\n",
    "# This line defines the tuple named tuple2, which contains three lists of integers.\n",
    "tuple2 = ([7, 8], [9, 1], [10, 7])\n",
    "\n",
    "# This line applies sum() to each list inside tuple2 using map, and then sums those intermediate results into one total.\n",
    "total = sum(map(sum, tuple2))\n",
    "\n",
    "# This line prints the final scalar total obtained from summing all numbers across the nested lists.\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([3.5, 4.0], [4.5, 0.5], [5.0, 3.5])\n"
     ]
    }
   ],
   "source": [
    "# 6) Create tuple3 with half the values of tuple2, preserving the same structure, using map\n",
    "# This line constructs tuple3 by mapping over each inner list, halving each element via a nested map, converting each mapped result into a list, and finally converting the outer result into a tuple.\n",
    "tuple3 = tuple(map(lambda lst: list(map(lambda x: x / 2, lst)), tuple2))\n",
    "\n",
    "# This line prints tuple3 to display the transformed structure (a tuple of lists) with each original value divided by two.\n",
    "print(tuple3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Lists\n",
    "1. Show the indices of the `np.nan` values  in the `f_list` list. We want to see this output: `The indices 0, 1, 4, 7 have np.nan values.` **Hint: Use print function and [f-strings](https://realpython.com/python-f-strings/) to insert the indices values.**<br><br>\n",
    "\n",
    "2. Replicate 4 times the values of the list `p2_list`. We expect an ouput like this: `[ 2 , 3, 4, 5, 2 , 3, 4, 5, 2 , 3, 4, 5, 2 , 3, 4, 5]`.  **Hint: Use multiplication function in `lists`and see the output.**<br><br>\n",
    "3. Print the length of `f_list`. **Hint: Length function**<br><br>\n",
    "4. Print `My teacher assistant is so boring.` using `text1` list. **Hint: Use the [`join` function](https://stackoverflow.com/questions/493819/why-is-it-string-joinlist-instead-of-list-joinstring)**<br><br>\n",
    "5. Print `My TA is so boring, but is very funny.` using `text1` list.**Hint: Use the `join` function, and `extend` method.**<br><br>\n",
    "6. Print <br>\n",
    "`The max value of values1 is 86 and is located in the 0 index. ` <br>\n",
    "`The min value of values1 is 0 and is located in the 7 index. ` <br> **Hint: Use the `f-string`, `min`, and `max` functions.**\n",
    "<br>\n",
    "7. Get two lists: `names` and `last_names` using `last_and_name` list. **Hint: Use `map` and `split`.**\n",
    "<br>\n",
    "8. Give only the last names of students who do not have email. Use the `emails` and `last_names` listt. **Hint: Use `map` and `split`.**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list = [np.nan , np.nan, \"Austria\", \"Germany\", np.nan, \"Pakistan\", \"np.nan\", np.nan ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = ['My', 'teacher', 'assistant', 'is', 'so', 'boring.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_list = [ 2 , 3, 4, 5 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "values1 = [ 86, 86, 85, 85, 85, 83, 23, 0, 84, 1 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two lists a\n",
    "last_and_name = [ \"CORNEJO SANCHEZ, CHRISTIAN SANTOS\", \"ORELLANA QUISPE, CRISTIAN NASSER\", \"MORALES CHOQUEHUANCA, ANGELICA KARINA\", \"GUIMARAY RIBEYRO, JOSE ROBERTO\", \"CAMACHO GAVIDIA, ABEL FERNANDO\", \"TINTAYA ORIHUELA, MEIR ALVARO\", \"CHAVEZ MARTINEZ, JOSELIN ALEXANDRA\", \"FIGUEROA MURO, LEONEL ARTURO\", \"GOMEZ CRIBILLERO, JOSE FELIPE\", \"PALOMINO SEGUÍN, AFRANIA\", \"LUZON CUEVA, BIANCA MARIETTE\", \"SUAÑA ZEGARRA, ADRIAN ANDRE\", \"SOTO POMACHAGUA, DORKAS YOMIRA JHERMY\", \"FIORENTINO MARTINEZ, LADY ALY\", \"LAMA MAVILA, HECTOR ANDRE\", \"MEZA HINOJO, GUSTAVO\", \"LOZADA MURILLO, PERSEO MARCELO\", \"ZAMBRANO JIMENEZ, MIGUEL ALONZO\", \"JACOBS LUQUE, NICOLAS\", \"VIDAL VIDAL, ROCIO GABRIELA\", \"TORRES ANICAMA, JANE CAMILA\", \"LOPEZ ESTRADA, MARIA ELISA\", \"BOYCO ORAMS, ALEJANDRO\", \"DIAZ BERROSPI, KARLINE ROSMELI\", \"RIEGA ESCALANTE, STEPHY ROSARIO\", \"LEVANO TORRES, VALERIA CECILIA\", \"ESQUIVES BRAVO, SEBASTIAN RENATO\", \"PEREZ GONZALES, JUAN CARLOS\", \"OTERO MAGUIÑA, MARIANA\", \"CLAVO CAMPOS, ANDREA BRIZETH\", \"AGUILAR GARCIA, ERICK JOSUE\", \"CALDAS VELASQUEZ, JOSUE DANIEL\", \"SALAS NUÑEZ BORJA, FABIO MANUEL\", \"PIZARRO VILLANES, FERNANDA NICOLLE\", \"QUILLATUPA MORALES, ANGELA ADELINA\", \"HUANCAYA IDONE, CESAR DANTE\", \"CALVO PORTOCARRERO, GABRIELA ISABEL\", \"IBAÑEZ ABANTO, ANGEL MAURICIO\", \"MELÉNDEZ APONTE, JUAN DIEGO\", \"CRISTIAN SERRANO, ARONE\", \"HINOJOSA CAHUANA, PERCY ALBERTH\", \"ANGLAS GARCÍA, KEVIN ARTURO\", \"ALDAVE ACOSTA, CESAR ERNESTO\", \"NÚÑEZ HUAMÁN, CÉSAR AGUSTO\", \"OBREGON HUAMAN, DIANA EDITH\", \"SOTO PACHERRES, RODRIGO FRANCO\", \"INGARUCA RIVERA, GRETTEL ALEXANDRA\", \"ROJAS HUAMAN, ROSA ANGELA\", \"NEYRA SALAS, DANTE OMAR\", \"HUERTA ESPINOZA, YAJAIRA ALEXANDRA\", \"HUANCA MARTINEZ, JORGE ALBERTO\", \"FLORES CADILLO, ALEXIS\" ]\n",
    "emails = [\"cscornejo@pucp.edu.pe\", \"orellana.cn@pucp.edu.pe\", \"karina.morales@pucp.edu.pe\", \"a20083223@pucp.pe\", \"abel.camacho@pucp.pe\", \"mtintaya@pucp.edu.pe\", \"joselin.chavez@pucp.edu.pe\", \"a20105737@pucp.pe\", \"jfgomezc@pucp.pe\", \"afrania.palomino@pucp.pe\", \"luzon.bianca@pucp.pe\", \"adrian.suanaz@pucp.pe\", \"soto.y@pucp.edu.pe\", \"a20132766@pucp.pe\", \"andre.lama@pucp.edu.pe\", \"gustavo.meza@pucp.edu.pe\", \"pmlozada@pucp.edu.pe\", \"m.zambranoj@pucp.edu.pe\", \"nicolas.jacobs@pucp.edu.pe\", \"gvidal@pucp.edu.pe\", \"jane.torres@pucp.edu.pe\", \"m.lopez@pucp.edu.pe\", \"alejandro.boyco@pucp.edu.pe\", \"a20167070@pucp.edu.pe\", \"riega.stephy@pucp.edu.pe\", \"vlevanot@pucp.edu.pe\", \"sesquives@pucp.edu.pe\", \"perez.juanc@pucp.edu.pe\", \"mariana.otero@pucp.edu.pe\", \"aclavo@pucp.edu.pe\", \"a20182474@pucp.edu.pe\", \"josue.caldas@pucp.edu.pe\", \"fabio.salas@pucp.edu.pe\", \"fernanda.pizarro@pucp.edu.pe\", \"aquillatupa@pucp.pe\", \"\", \"\", \"\", \"\", \"\", \"f0873079@pucp.edu.pe\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"flores.alexis@pucp.edu.pe\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The indices 0, 1, 4, 7 have np.nan values.\n",
      "[2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5]\n",
      "8\n",
      "My teacher assistant is so boring.\n",
      "My TA is so boring. but is very funny.\n",
      "The max value of values1 is 86 and is located in the 0 index.\n",
      "The min value of values1 is 0 and is located in the 7 index.\n",
      "['HUANCAYA IDONE', 'CALVO PORTOCARRERO', 'IBAÑEZ ABANTO', 'MELÉNDEZ APONTE', 'CRISTIAN SERRANO', 'ANGLAS GARCÍA', 'ALDAVE ACOSTA', 'NÚÑEZ HUAMÁN', 'OBREGON HUAMAN', 'SOTO PACHERRES', 'INGARUCA RIVERA', 'ROJAS HUAMAN', 'NEYRA SALAS', 'HUERTA ESPINOZA', 'HUANCA MARTINEZ']\n"
     ]
    }
   ],
   "source": [
    "#Sol 1.2 Carmen Update:\n",
    "# 1\n",
    "# Get index-value pairs, filter only np.nan values, and extract indices\n",
    "nan_indices = list(map(lambda x: x[0], filter(lambda x: x[1] is np.nan, enumerate(f_list))))\n",
    "\n",
    "# Print indices using f-string\n",
    "print(f\"The indices {nan_indices[0]}, {nan_indices[1]}, {nan_indices[2]}, {nan_indices[3]} have np.nan values.\")\n",
    "\n",
    "#2\n",
    "# Multiply the list to repeat its elements\n",
    "replicated_list = p2_list * 4\n",
    "\n",
    "# Print the replicated list\n",
    "print(replicated_list)\n",
    "\n",
    "#3\n",
    "# Use len() to get the number of elements in the list\n",
    "print(len(f_list))\n",
    "\n",
    "#4\n",
    "# Join list elements into a single string separated by spaces\n",
    "sentence = \" \".join(text1)\n",
    "\n",
    "# Print the sentence\n",
    "print(sentence)\n",
    "\n",
    "#5\n",
    "# Create a copy of the original list\n",
    "text1_extended = text1.copy()\n",
    "\n",
    "# Replace 'teacher assistant' with 'TA'\n",
    "text1_extended[1:3] = [\"TA\"]\n",
    "\n",
    "# Add extra words to the list\n",
    "text1_extended.extend([\"but\", \"is\", \"very\", \"funny.\"])\n",
    "\n",
    "# Join list into a sentence\n",
    "sentence2 = \" \".join(text1_extended)\n",
    "\n",
    "# Print the sentence\n",
    "print(sentence2)\n",
    "\n",
    "#6\n",
    "# Get maximum value from the list\n",
    "max_value = max(values1)\n",
    "\n",
    "# Get minimum value from the list\n",
    "min_value = min(values1)\n",
    "\n",
    "# Print max value and its index\n",
    "print(f\"The max value of values1 is {max_value} and is located in the {values1.index(max_value)} index.\")\n",
    "\n",
    "# Print min value and its index\n",
    "print(f\"The min value of values1 is {min_value} and is located in the {values1.index(min_value)} index.\")\n",
    "\n",
    "\n",
    "#7\n",
    "# Split each full name into last names and first names\n",
    "split_names = list(map(lambda x: x.split(\", \"), last_and_name))\n",
    "\n",
    "# Extract last names\n",
    "last_names = list(map(lambda x: x[0], split_names))\n",
    "\n",
    "# Extract first names\n",
    "names = list(map(lambda x: x[1], split_names))\n",
    "\n",
    "#8\n",
    "# Pair last names with emails, filter empty emails, and extract last names\n",
    "no_email_last_names = list(\n",
    "    map(\n",
    "        lambda x: x[0],\n",
    "        filter(lambda x: x[1] == \"\", zip(last_names, emails))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print last names without email\n",
    "print(no_email_last_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Strings\n",
    "\n",
    "\n",
    "1. Drop the duplicated blank spaces in `str1`. Do not use the regular expresions. Do not use the code shown in class. Explain your steps.**Hint: Use `split` method and `join` function.**\n",
    "\n",
    "2. Get the number of letters in the string .**Hint: Use `len`function.**\n",
    "\n",
    "3. Get the number of blank spaces (all of them) in the string .**Hint: Use `len`function.**\n",
    "\n",
    "4. Get the position of `@` in each string in the emails list. **Hint: Use `map`function and `find` method.**\n",
    "\n",
    "5. Identifies if exists `.edu.`  in each string in the `emails` list. Get a list of Booleans. **Hint: Use `map`function and `find` method.**\n",
    "\n",
    "6. Get all the strings before the first dot `.`in each string in the `emails` list. Identifies how many of them has `@`. **Hint: Use `map`function and `find` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = 'I am                            too                                                        old'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = [\"cscornejo@pucp.edu.pe\", \"orellana.cn@pucp.edu.pe\", \"karina.morales@pucp.edu.pe\", \"a20083223@pucp.pe\", \"abel.camacho@pucp.pe\", \"mtintaya@pucp.edu.pe\", \"joselin.chavez@pucp.edu.pe\", \"a20105737@pucp.pe\", \"jfgomezc@pucp.pe\", \"afrania.palomino@pucp.pe\", \"luzon.bianca@pucp.pe\", \"adrian.suanaz@pucp.pe\", \"soto.y@pucp.edu.pe\", \"a20132766@pucp.pe\", \"andre.lama@pucp.edu.pe\", \"gustavo.meza@pucp.edu.pe\", \"pmlozada@pucp.edu.pe\", \"m.zambranoj@pucp.edu.pe\", \"nicolas.jacobs@pucp.edu.pe\", \"gvidal@pucp.edu.pe\", \"jane.torres@pucp.edu.pe\", \"m.lopez@pucp.edu.pe\", \"alejandro.boyco@pucp.edu.pe\", \"a20167070@pucp.edu.pe\", \"riega.stephy@pucp.edu.pe\", \"vlevanot@pucp.edu.pe\", \"sesquives@pucp.edu.pe\", \"perez.juanc@pucp.edu.pe\", \"mariana.otero@pucp.edu.pe\", \"aclavo@pucp.edu.pe\", \"a20182474@pucp.edu.pe\", \"josue.caldas@pucp.edu.pe\", \"fabio.salas@pucp.edu.pe\", \"fernanda.pizarro@pucp.edu.pe\", \"aquillatupa@pucp.pe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am too old'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sol: 1.3 Kevin update\n",
    "#1\n",
    "# split() separates the string into words ignoring multiple spaces\n",
    "# join() rebuilds the string using only one space between words\n",
    "clean_str1 = \" \".join(str1.split())\n",
    "\n",
    "clean_str1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2\n",
    "# Remove all blank spaces\n",
    "# len() counts only the letters\n",
    "num_letters = len(clean_str1.replace(\" \", \"\"))\n",
    "\n",
    "num_letters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3\n",
    "# Total length of the original string\n",
    "# Minus length after removing spaces\n",
    "# The difference is the number of blank spaces\n",
    "num_spaces = len(str1) - len(str1.replace(\" \", \"\"))\n",
    "\n",
    "num_spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 11,\n",
       " 14,\n",
       " 9,\n",
       " 12,\n",
       " 8,\n",
       " 14,\n",
       " 9,\n",
       " 8,\n",
       " 16,\n",
       " 12,\n",
       " 13,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 12,\n",
       " 8,\n",
       " 11,\n",
       " 14,\n",
       " 6,\n",
       " 11,\n",
       " 7,\n",
       " 15,\n",
       " 9,\n",
       " 12,\n",
       " 8,\n",
       " 9,\n",
       " 11,\n",
       " 13,\n",
       " 6,\n",
       " 9,\n",
       " 12,\n",
       " 11,\n",
       " 16,\n",
       " 11]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4\n",
    "# map() applies find(\"@\") to each email\n",
    "# find() returns the index of '@' or -1 if not found\n",
    "at_positions = list(map(lambda x: x.find(\"@\"), emails))\n",
    "\n",
    "at_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5\n",
    "# find(\".edu.\") returns -1 if the substring does not exist\n",
    "# != -1 converts the result into a Boolean value\n",
    "has_edu = list(map(lambda x: x.find(\".edu.\") != -1, emails))\n",
    "\n",
    "has_edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6\n",
    "# split(\".\", 1) splits only at the first dot\n",
    "# [0] keeps the part before the dot\n",
    "before_dot = list(map(lambda x: x.split(\".\", 1)[0], emails))\n",
    "\n",
    "before_dot\n",
    "\n",
    "# \"@\" in x returns True or False\n",
    "# sum() counts True as 1 and False as 0\n",
    "count_at_before_dot = sum(map(lambda x: \"@\" in x, before_dot))\n",
    "\n",
    "count_at_before_dot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Obtaining dependency information for yfinance from https://files.pythonhosted.org/packages/8d/c1/ac130a6a46b7c23624220c8fcae9de5f3be0c2c492452d31c45cbf50bc12/yfinance-1.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading yfinance-1.0-py2.py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (1.24.3)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (2.31.0)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Downloading multitasking-0.0.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (3.10.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (2023.3.post1)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Obtaining dependency information for frozendict>=2.3.4 from https://files.pythonhosted.org/packages/38/74/f94141b38a51a553efef7f510fc213894161ae49b88bffd037f8d2a7cb2f/frozendict-2.4.7-py3-none-any.whl.metadata\n",
      "  Downloading frozendict-2.4.7-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Obtaining dependency information for peewee>=3.16.2 from https://files.pythonhosted.org/packages/1a/41/19c65578ef9a54b3083253c68a607f099642747168fe00f3a2bceb7c3a34/peewee-3.19.0-py3-none-any.whl.metadata\n",
      "  Downloading peewee-3.19.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yfinance) (4.12.2)\n",
      "Collecting curl_cffi<0.14,>=0.7 (from yfinance)\n",
      "  Obtaining dependency information for curl_cffi<0.14,>=0.7 from https://files.pythonhosted.org/packages/6d/e4/15a253f9b4bf8d008c31e176c162d2704a7e0c5e24d35942f759df107b68/curl_cffi-0.13.0-cp39-abi3-win_amd64.whl.metadata\n",
      "  Downloading curl_cffi-0.13.0-cp39-abi3-win_amd64.whl.metadata (13 kB)\n",
      "Collecting protobuf>=3.19.0 (from yfinance)\n",
      "  Obtaining dependency information for protobuf>=3.19.0 from https://files.pythonhosted.org/packages/64/20/4d50191997e917ae13ad0a235c8b42d8c1ab9c3e6fd455ca16d416944355/protobuf-6.33.2-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-6.33.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting websockets>=13.0 (from yfinance)\n",
      "  Obtaining dependency information for websockets>=13.0 from https://files.pythonhosted.org/packages/98/93/e36c73f78400a65f5e236cd376713c34182e6663f6889cd45a4a04d8f203/websockets-15.0.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from curl_cffi<0.14,>=0.7->yfinance) (1.15.1)\n",
      "Collecting certifi>=2024.2.2 (from curl_cffi<0.14,>=0.7->yfinance)\n",
      "  Obtaining dependency information for certifi>=2024.2.2 from https://files.pythonhosted.org/packages/e6/ad/3cc14f097111b4de0040c83a525973216457bbeeb63739ef1ed275c1c021/certifi-2026.1.4-py3-none-any.whl.metadata\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (1.26.16)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.12.0->curl_cffi<0.14,>=0.7->yfinance) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n",
      "Downloading yfinance-1.0-py2.py3-none-any.whl (127 kB)\n",
      "   ---------------------------------------- 0.0/127.1 kB ? eta -:--:--\n",
      "   -------------------------------------- - 122.9/127.1 kB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 127.1/127.1 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading curl_cffi-0.13.0-cp39-abi3-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.6/1.6 MB 51.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 25.9 MB/s eta 0:00:00\n",
      "Downloading frozendict-2.4.7-py3-none-any.whl (16 kB)\n",
      "Downloading peewee-3.19.0-py3-none-any.whl (411 kB)\n",
      "   ---------------------------------------- 0.0/411.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 411.9/411.9 kB 8.5 MB/s eta 0:00:00\n",
      "Downloading protobuf-6.33.2-cp310-abi3-win_amd64.whl (436 kB)\n",
      "   ---------------------------------------- 0.0/436.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 436.9/436.9 kB 13.8 MB/s eta 0:00:00\n",
      "Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl (176 kB)\n",
      "   ---------------------------------------- 0.0/176.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 176.8/176.8 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.9/152.9 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: multitasking\n",
      "  Building wheel for multitasking (setup.py): started\n",
      "  Building wheel for multitasking (setup.py): finished with status 'done'\n",
      "  Created wheel for multitasking: filename=multitasking-0.0.12-py3-none-any.whl size=15617 sha256=19407bc71a1d94332ba51aa9cba62b7458d0e433d347a4e692e44b0aef9ca2fc\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\42\\d6\\84\\bf57a755f4569494cd00de4bb46ef064874823f4d19c82e960\n",
      "Successfully built multitasking\n",
      "Installing collected packages: peewee, multitasking, websockets, protobuf, frozendict, certifi, curl_cffi, yfinance\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.7.22\n",
      "    Uninstalling certifi-2023.7.22:\n",
      "      Successfully uninstalled certifi-2023.7.22\n",
      "Successfully installed certifi-2026.1.4 curl_cffi-0.13.0 frozendict-2.4.7 multitasking-0.0.12 peewee-3.19.0 protobuf-6.33.2 websockets-15.0.1 yfinance-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "START = \"2022-01-01\"\n",
    "END = \"2025-12-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        date ticker      close    volume\n",
       " 0 2022-01-03    EEM  44.624966  27572700\n",
       " 1 2022-01-04    EEM  44.470772  24579500\n",
       " 2 2022-01-05    EEM  43.745163  46425100\n",
       " 3 2022-01-06    EEM  43.944710  34288700\n",
       " 4 2022-01-07    EEM  44.343796  32640900,\n",
       " (4970, 4))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Yahoo Finance via yfinance: US tickers (real market yh_df) ---\n",
    "tickers = [\"SPY\", \"QQQ\", \"TLT\", \"GLD\", \"EEM\"]\n",
    "# SPY : S&P 500 index\n",
    "# QQQ : Nasdaq-100 index\n",
    "# TLT : U.S. Treasury bonds with 20+ year maturity\n",
    "# GLD : Physical gold prices\n",
    "# EEM : MSCI Emerging Markets index\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception as e:\n",
    "    yf = None\n",
    "    print(\"Could not import yfinance:\", type(e).__name__, str(e))\n",
    "\n",
    "if yf is not None:\n",
    "    try:\n",
    "        yh_df = yf.download(tickers, start=START, end=END, auto_adjust=True, progress=False)\n",
    "    except Exception as e:\n",
    "        yh_df = pd.DataFrame()\n",
    "        print(\"yfinance download failed:\", type(e).__name__, str(e))\n",
    "else:\n",
    "    yh_df = pd.DataFrame()\n",
    "\n",
    "# Convert to long format: date, ticker, close, volume\n",
    "if isinstance(yh_df, pd.DataFrame) and yh_df.shape[0] > 0:\n",
    "    if isinstance(yh_df.columns, pd.MultiIndex):\n",
    "        close = yh_df[\"Close\"].copy()\n",
    "        vol = yh_df[\"Volume\"].copy()\n",
    "    else:\n",
    "        close = yh_df[[\"Close\"]].rename(columns={\"Close\": tickers[0]})\n",
    "        vol = yh_df[[\"Volume\"]].rename(columns={\"Volume\": tickers[0]})\n",
    "\n",
    "    close.index.name = \"date\"\n",
    "    vol.index.name = \"date\"\n",
    "\n",
    "    us_close_long = close.reset_index().melt(id_vars=\"date\", var_name=\"ticker\", value_name=\"close\")\n",
    "    us_vol_long = vol.reset_index().melt(id_vars=\"date\", var_name=\"ticker\", value_name=\"volume\")\n",
    "    us_mkt = us_close_long.merge(us_vol_long, on=[\"date\",\"ticker\"], how=\"inner\").dropna(subset=[\"close\"])\n",
    "else:\n",
    "    us_mkt = pd.DataFrame(columns=[\"date\",\"ticker\",\"close\",\"volume\"])\n",
    "\n",
    "us_mkt.head(), us_mkt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_raw</th>\n",
       "      <th>PENUSD_buy</th>\n",
       "      <th>PENUSD_sell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03.Ene.22</td>\n",
       "      <td>3.98366666666667</td>\n",
       "      <td>3.98883333333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04.Ene.22</td>\n",
       "      <td>3.9595</td>\n",
       "      <td>3.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05.Ene.22</td>\n",
       "      <td>3.952</td>\n",
       "      <td>3.95633333333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06.Ene.22</td>\n",
       "      <td>3.96716666666667</td>\n",
       "      <td>3.96966666666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07.Ene.22</td>\n",
       "      <td>3.94516666666667</td>\n",
       "      <td>3.94816666666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>12.Dic.25</td>\n",
       "      <td>3.36685714285714</td>\n",
       "      <td>3.36885714285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>15.Dic.25</td>\n",
       "      <td>3.36871428571429</td>\n",
       "      <td>3.3705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>16.Dic.25</td>\n",
       "      <td>3.36985714285714</td>\n",
       "      <td>3.37142857142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>17.Dic.25</td>\n",
       "      <td>3.36771428571429</td>\n",
       "      <td>3.36914285714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>18.Dic.25</td>\n",
       "      <td>3.36514285714286</td>\n",
       "      <td>3.36664285714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>988 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_raw        PENUSD_buy       PENUSD_sell\n",
       "0    03.Ene.22  3.98366666666667  3.98883333333333\n",
       "1    04.Ene.22            3.9595             3.964\n",
       "2    05.Ene.22             3.952  3.95633333333333\n",
       "3    06.Ene.22  3.96716666666667  3.96966666666667\n",
       "4    07.Ene.22  3.94516666666667  3.94816666666667\n",
       "..         ...               ...               ...\n",
       "983  12.Dic.25  3.36685714285714  3.36885714285714\n",
       "984  15.Dic.25  3.36871428571429            3.3705\n",
       "985  16.Dic.25  3.36985714285714  3.37142857142857\n",
       "986  17.Dic.25  3.36771428571429  3.36914285714286\n",
       "987  18.Dic.25  3.36514285714286  3.36664285714286\n",
       "\n",
       "[988 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- BCRP: daily exchange rate PEN/USD buy & sell (official API) ---\n",
    "# Codes:\n",
    "# - PD04637PD: USD/PEN (buy)\n",
    "# - PD04638PD: USD/PEN (sell)\n",
    "\n",
    "import requests\n",
    "\n",
    "bcrp_url = f\"https://estadisticas.bcrp.gob.pe/estadisticas/series/api/PD04637PD-PD04638PD/json/{START}/{END}/esp\"\n",
    "try:\n",
    "    r = requests.get(bcrp_url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    bcrp_obj = r.json()\n",
    "except Exception as e:\n",
    "    bcrp_obj = {\"periods\": []}\n",
    "    print(\"BCRP request failed:\", type(e).__name__, str(e))\n",
    "\n",
    "periods = bcrp_obj.get(\"periods\", [])\n",
    "rows = []\n",
    "for p in periods:\n",
    "    name = p.get(\"name\")\n",
    "    vals = p.get(\"values\", [])\n",
    "    if isinstance(vals, str):\n",
    "        vals = [vals]\n",
    "    if name is None or not isinstance(vals, list) or len(vals) < 2:\n",
    "        continue\n",
    "    rows.append([name, vals[0], vals[1]])\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"date_raw\", \"PENUSD_buy\", \"PENUSD_sell\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 From NumPy array to Series\n",
    "\n",
    "Using `us_mkt`:\n",
    "\n",
    "1. Filter to `ticker == \"SPY\"`.\n",
    "2. Take `close` as a NumPy array.\n",
    "3. Create a Series indexed by `date` named `SPY_close_series`.\n",
    "4. Compute the mean, minimum, and maximum using Series methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 485.608585871921, Min: 341.1820983886719, Max: 687.1395263671875\n"
     ]
    }
   ],
   "source": [
    "# This line filters the market DataFrame to keep only observations where the ticker is 'SPY'.\n",
    "spy_df = us_mkt.loc[us_mkt[\"ticker\"].eq(\"SPY\")].copy()\n",
    "\n",
    "# This line converts the 'close' column from the filtered DataFrame into a NumPy array.\n",
    "spy_close_np = spy_df[\"close\"].to_numpy()\n",
    "\n",
    "# This line constructs a pandas Series using dates as the index and the NumPy array as values, naming it explicitly.\n",
    "SPY_close_series = pd.Series(spy_close_np, index=spy_df[\"date\"], name=\"SPY_close_series\")\n",
    "\n",
    "# This line computes the mean of the Series using pandas' Series method (which is NaN-aware).\n",
    "spy_mean = SPY_close_series.mean()\n",
    "\n",
    "# This line computes the minimum value of the Series using pandas' Series method.\n",
    "spy_min = SPY_close_series.min()\n",
    "\n",
    "# This line computes the maximum value of the Series using pandas' Series method.\n",
    "spy_max = SPY_close_series.max()\n",
    "\n",
    "# This line prints the summary statistics in a single, readable output.\n",
    "print(f\"Mean: {spy_mean}, Min: {spy_min}, Max: {spy_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 From Dictionary to Series\n",
    "\n",
    "Using `us_mkt`:\n",
    "\n",
    "1. Compute the **last available close** for each ticker in `tickers`.\n",
    "2. Store the result in a dictionary of the form `{ticker: last_close}`.\n",
    "3. Convert the dictionary to a Series and sort it in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SPY    669.421936\n",
       "QQQ    599.637390\n",
       "GLD    399.290009\n",
       "TLT     87.459633\n",
       "EEM     52.599998\n",
       "Name: last_close, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line sorts the DataFrame by date to ensure that 'last' refers to the most recent observation per ticker.\n",
    "us_sorted = us_mkt.sort_values(\"date\")\n",
    "\n",
    "# This line extracts the last available close per ticker by grouping and taking the final close in each group.\n",
    "last_close_per_ticker = us_sorted.groupby(\"ticker\")[\"close\"].last()\n",
    "\n",
    "# This line converts the resulting Series into a dictionary of the form {ticker: last_close}.\n",
    "last_close_dict = last_close_per_ticker.to_dict()\n",
    "\n",
    "# This line converts the dictionary back into a pandas Series for convenient sorting and analysis.\n",
    "last_close_series = pd.Series(last_close_dict, name=\"last_close\")\n",
    "\n",
    "# This line sorts the Series in descending order, as requested.\n",
    "last_close_series_sorted = last_close_series.sort_values(ascending=False)\n",
    "\n",
    "# This line displays the final sorted Series.\n",
    "last_close_series_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.4 Series vs NumPy\n",
    "\n",
    "**Goal:** show why pandas alignment matters.\n",
    "\n",
    "1. Create two Series indexed by date:\n",
    "   - Mid-rate from `df`\n",
    "   - SPY close from `us_mkt`\n",
    "2. Combine them into a single DataFrame (pandas aligns on dates).\n",
    "3. Separately, build two NumPy arrays by truncating them to the same length.\n",
    "4. In Markdown, explain why alignment is safer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9344\\2809656145.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_dates = pd.to_datetime(df[\"date_raw\"], errors=\"coerce\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PENUSD_mid</th>\n",
       "      <th>SPY_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-02-01</th>\n",
       "      <td>3.874500</td>\n",
       "      <td>428.454193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-02</th>\n",
       "      <td>3.854500</td>\n",
       "      <td>432.616211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-03</th>\n",
       "      <td>3.859667</td>\n",
       "      <td>422.447601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-04</th>\n",
       "      <td>3.865417</td>\n",
       "      <td>424.434021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-07</th>\n",
       "      <td>3.842167</td>\n",
       "      <td>423.071930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            PENUSD_mid   SPY_close\n",
       "2022-02-01    3.874500  428.454193\n",
       "2022-02-02    3.854500  432.616211\n",
       "2022-02-03    3.859667  422.447601\n",
       "2022-02-04    3.865417  424.434021\n",
       "2022-02-07    3.842167  423.071930"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line converts the raw BCRP date strings into pandas datetime to enable robust index alignment.\n",
    "df_dates = pd.to_datetime(df[\"date_raw\"], errors=\"coerce\")\n",
    "\n",
    "# This line converts the buy column to numeric, coercing any non-numeric entries into NaN.\n",
    "buy = pd.to_numeric(df[\"PENUSD_buy\"], errors=\"coerce\")\n",
    "\n",
    "# This line converts the sell column to numeric, coercing any non-numeric entries into NaN.\n",
    "sell = pd.to_numeric(df[\"PENUSD_sell\"], errors=\"coerce\")\n",
    "\n",
    "# This line computes the mid exchange rate as the arithmetic mean of buy and sell rates.\n",
    "mid = (buy + sell) / 2\n",
    "\n",
    "# This line constructs a mid-rate Series indexed by date, which is required for safe calendar-based alignment.\n",
    "mid_rate_series = pd.Series(mid.to_numpy(), index=df_dates, name=\"PENUSD_mid\")\n",
    "\n",
    "# This line removes missing dates (NaT) from the index to avoid invalid alignment keys.\n",
    "mid_rate_series = mid_rate_series.loc[mid_rate_series.index.notna()]\n",
    "\n",
    "# This line collapses duplicate dates by keeping the last observation per date, ensuring a unique index.\n",
    "mid_rate_series = mid_rate_series.groupby(level=0).last()\n",
    "\n",
    "# This line filters the US market dataset to keep only SPY observations.\n",
    "spy_df2 = us_mkt.loc[us_mkt[\"ticker\"].eq(\"SPY\")].copy()\n",
    "\n",
    "# This line constructs a SPY close Series indexed by date, enabling index-based alignment.\n",
    "spy_close_series = pd.Series(spy_df2[\"close\"].to_numpy(), index=spy_df2[\"date\"], name=\"SPY_close\")\n",
    "\n",
    "# This line removes missing dates (NaT) from the SPY index to avoid invalid alignment keys.\n",
    "spy_close_series = spy_close_series.loc[spy_close_series.index.notna()]\n",
    "\n",
    "# This line collapses duplicate SPY dates by keeping the last observation per date, ensuring a unique index.\n",
    "spy_close_series = spy_close_series.groupby(level=0).last()\n",
    "\n",
    "# This line concatenates both Series into a single DataFrame, aligning observations strictly by their date indices.\n",
    "aligned_df = pd.concat([mid_rate_series, spy_close_series], axis=1)\n",
    "\n",
    "# This line drops rows where at least one of the two aligned variables is missing.\n",
    "aligned_df_clean = aligned_df.dropna()\n",
    "\n",
    "# This line extracts the aligned mid-rate values as a NumPy array after ensuring correct calendar alignment.\n",
    "mid_np_aligned = aligned_df_clean[\"PENUSD_mid\"].to_numpy()\n",
    "\n",
    "# This line extracts the aligned SPY close values as a NumPy array after ensuring correct calendar alignment.\n",
    "spy_np_aligned = aligned_df_clean[\"SPY_close\"].to_numpy()\n",
    "\n",
    "# This line displays the first few aligned observations for verification.\n",
    "aligned_df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why pandas alignment is safer than NumPy truncation\n",
    "\n",
    "Pandas alignment is safer because it matches observations based on their index values, in this case calendar dates, rather than relying on positional correspondence. When two Series are combined using their date indices, pandas ensures that each observation refers to the same point in time before any comparison or transformation is performed.\n",
    "\n",
    "By contrast, truncating NumPy arrays to a common length only guarantees that both arrays have the same shape, but it does not ensure that element *i* in one array corresponds to the same date as element *i* in the other. This positional matching can silently introduce temporal mismatches, particularly when datasets contain missing dates, duplicated observations, or different sampling frequencies.\n",
    "\n",
    "Therefore, index-based alignment in pandas provides a more robust and conceptually correct approach for financial and time-series analysis, as it preserves the economic meaning of each observation and prevents misleading statistical results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.6 Dealing with Nulls\n",
    "\n",
    "Using `us_mkt`:\n",
    "\n",
    "1. Copy `us_mkt` to `us_mkt_nan`.\n",
    "2. Set 1% of `close` values to NaN (fixed random seed).\n",
    "3. Create:\n",
    "   - `us_drop`: drop NaN values\n",
    "   - `us_fill`: fill NaNs with ticker-specific median close\n",
    "4. Compare dataset shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (4970, 4) | With NaNs: (4970, 4) | Dropped: (4920, 4) | Filled: (4970, 4)\n"
     ]
    }
   ],
   "source": [
    "# This line creates a defensive copy so the original us_mkt remains unchanged.\n",
    "us_mkt_nan = us_mkt.copy()\n",
    "\n",
    "# This line samples 1% of rows in a reproducible manner using a fixed random seed.\n",
    "nan_idx = us_mkt_nan.sample(frac=0.01, random_state=42).index\n",
    "\n",
    "# This line sets the 'close' values at the sampled indices to NaN, introducing controlled missingness.\n",
    "us_mkt_nan.loc[nan_idx, \"close\"] = np.nan\n",
    "\n",
    "# This line drops rows with missing close values, producing the 'drop' version.\n",
    "us_drop = us_mkt_nan.dropna(subset=[\"close\"])\n",
    "\n",
    "# This line computes the ticker-specific median close for each row via transform, preserving original shape for filling.\n",
    "ticker_medians = us_mkt_nan.groupby(\"ticker\")[\"close\"].transform(\"median\")\n",
    "\n",
    "# This line fills missing close values with the corresponding ticker-specific median.\n",
    "us_fill = us_mkt_nan.assign(close=us_mkt_nan[\"close\"].fillna(ticker_medians))\n",
    "\n",
    "# This line prints the shapes to compare how dropping versus filling affects the dataset size.\n",
    "print(\"Original:\", us_mkt.shape, \"| With NaNs:\", us_mkt_nan.shape, \"| Dropped:\", us_drop.shape, \"| Filled:\", us_fill.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.7 Duplicates\n",
    "\n",
    "1. Create `dup_df` by stacking the last 5 rows of `us_mkt` twice.\n",
    "2. Detect duplicates using `.duplicated()`.\n",
    "3. Remove them using `.drop_duplicates()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates flagged: 5\n",
      "Before: (10, 4) | After: (5, 4)\n"
     ]
    }
   ],
   "source": [
    "# This line extracts the last five rows of us_mkt, which will be duplicated deliberately.\n",
    "tail5 = us_mkt.tail(5)\n",
    "\n",
    "# This line stacks the same five rows twice to create intentional duplicates.\n",
    "dup_df = pd.concat([tail5, tail5], ignore_index=True)\n",
    "\n",
    "# This line identifies duplicates using pandas' duplicated method, marking repeated rows as True.\n",
    "dup_mask = dup_df.duplicated()\n",
    "\n",
    "# This line prints how many rows are flagged as duplicates.\n",
    "print(\"Number of duplicates flagged:\", dup_mask.sum())\n",
    "\n",
    "# This line removes duplicated rows, keeping the first occurrence by default.\n",
    "dedup_df = dup_df.drop_duplicates()\n",
    "\n",
    "# This line prints the shapes before and after removal to confirm the effect.\n",
    "print(\"Before:\", dup_df.shape, \"| After:\", dedup_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.8 Groupby\n",
    "\n",
    "Using `us_mkt`:\n",
    "\n",
    "1. Group by `ticker` and compute:\n",
    "   - mean close\n",
    "   - median close\n",
    "   - max volume\n",
    "2. Rename columns clearly.\n",
    "3. Sort by mean close in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_close</th>\n",
       "      <th>median_close</th>\n",
       "      <th>max_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>485.608586</td>\n",
       "      <td>460.740021</td>\n",
       "      <td>256611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QQQ</th>\n",
       "      <td>411.276965</td>\n",
       "      <td>400.214554</td>\n",
       "      <td>198685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLD</th>\n",
       "      <td>220.130422</td>\n",
       "      <td>187.864998</td>\n",
       "      <td>62025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TLT</th>\n",
       "      <td>91.395621</td>\n",
       "      <td>88.549702</td>\n",
       "      <td>131353500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EEM</th>\n",
       "      <td>40.462350</td>\n",
       "      <td>39.107407</td>\n",
       "      <td>134225700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean_close  median_close  max_volume\n",
       "ticker                                      \n",
       "SPY     485.608586    460.740021   256611400\n",
       "QQQ     411.276965    400.214554   198685800\n",
       "GLD     220.130422    187.864998    62025000\n",
       "TLT      91.395621     88.549702   131353500\n",
       "EEM      40.462350     39.107407   134225700"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line groups the data by ticker and computes the requested aggregate statistics for close and volume.\n",
    "gb_stats = us_mkt.groupby(\"ticker\").agg(\n",
    "    mean_close=(\"close\", \"mean\"),\n",
    "    median_close=(\"close\", \"median\"),\n",
    "    max_volume=(\"volume\", \"max\")\n",
    ")\n",
    "\n",
    "# This line sorts the resulting summary table by mean_close in descending order, as specified.\n",
    "gb_stats_sorted = gb_stats.sort_values(\"mean_close\", ascending=False)\n",
    "\n",
    "# This line displays the final grouped summary table.\n",
    "gb_stats_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.9 Reshape\n",
    "\n",
    "1. Create a 1-row wide DataFrame with last closes per ticker.\n",
    "2. Convert it to long format using `melt()` into columns: `ticker`, `last_close`.\n",
    "3. Pivot `us_mkt` into a wide table with:\n",
    "   - index = `date`\n",
    "   - columns = `ticker`\n",
    "   - values = `close`\n",
    "   (keep the first 50 dates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticker</th>\n",
       "      <th>EEM</th>\n",
       "      <th>GLD</th>\n",
       "      <th>QQQ</th>\n",
       "      <th>SPY</th>\n",
       "      <th>TLT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>last_close</th>\n",
       "      <td>52.599998</td>\n",
       "      <td>399.290009</td>\n",
       "      <td>599.63739</td>\n",
       "      <td>669.421936</td>\n",
       "      <td>87.459633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker            EEM         GLD        QQQ         SPY        TLT\n",
       "last_close  52.599998  399.290009  599.63739  669.421936  87.459633"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line ensures the dataset is sorted by date so 'last' reflects the most recent close per ticker.\n",
    "us_sorted2 = us_mkt.sort_values(\"date\")\n",
    "\n",
    "# This line creates a one-row wide DataFrame whose columns are tickers and values are last closes.\n",
    "last_close_wide = us_sorted2.groupby(\"ticker\")[\"close\"].last().to_frame().T\n",
    "\n",
    "# This line sets an informative index label for the single row (purely cosmetic, but academically clearer).\n",
    "last_close_wide.index = [\"last_close\"]\n",
    "\n",
    "# This line converts the 1-row wide format into long format with columns 'ticker' and 'last_close'.\n",
    "last_close_long = last_close_wide.reset_index(drop=True).melt(var_name=\"ticker\", value_name=\"last_close\")\n",
    "\n",
    "# This line extracts the first 50 unique dates (chronologically) to satisfy the “keep first 50 dates” requirement.\n",
    "first_50_dates = us_sorted2[\"date\"].drop_duplicates().head(50)\n",
    "\n",
    "# This line filters the dataset to keep only observations whose date is among the first 50 unique dates.\n",
    "us_first50 = us_sorted2.loc[us_sorted2[\"date\"].isin(first_50_dates)]\n",
    "\n",
    "# This line pivots the filtered data into a wide table with dates as rows, tickers as columns, and close as values.\n",
    "close_wide_50 = us_first50.pivot(index=\"date\", columns=\"ticker\", values=\"close\").sort_index()\n",
    "\n",
    "# This line displays the 1-row wide table of last closes.\n",
    "last_close_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>last_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EEM</td>\n",
       "      <td>52.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GLD</td>\n",
       "      <td>399.290009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QQQ</td>\n",
       "      <td>599.637390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPY</td>\n",
       "      <td>669.421936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TLT</td>\n",
       "      <td>87.459633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker  last_close\n",
       "0    EEM   52.599998\n",
       "1    GLD  399.290009\n",
       "2    QQQ  599.637390\n",
       "3    SPY  669.421936\n",
       "4    TLT   87.459633"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line displays the long-format version obtained via melt.\n",
    "last_close_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticker</th>\n",
       "      <th>EEM</th>\n",
       "      <th>GLD</th>\n",
       "      <th>QQQ</th>\n",
       "      <th>SPY</th>\n",
       "      <th>TLT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>44.624966</td>\n",
       "      <td>168.330002</td>\n",
       "      <td>391.679474</td>\n",
       "      <td>451.875122</td>\n",
       "      <td>125.295341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>44.470772</td>\n",
       "      <td>169.570007</td>\n",
       "      <td>386.599182</td>\n",
       "      <td>451.723816</td>\n",
       "      <td>124.774353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>43.745163</td>\n",
       "      <td>169.059998</td>\n",
       "      <td>374.722382</td>\n",
       "      <td>443.049744</td>\n",
       "      <td>124.097122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>43.944710</td>\n",
       "      <td>166.990005</td>\n",
       "      <td>374.459106</td>\n",
       "      <td>442.633575</td>\n",
       "      <td>124.418381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>44.343796</td>\n",
       "      <td>167.750000</td>\n",
       "      <td>370.402679</td>\n",
       "      <td>440.883575</td>\n",
       "      <td>123.524017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-10</th>\n",
       "      <td>44.343796</td>\n",
       "      <td>168.259995</td>\n",
       "      <td>370.646484</td>\n",
       "      <td>440.334991</td>\n",
       "      <td>123.827965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-11</th>\n",
       "      <td>45.368721</td>\n",
       "      <td>170.289993</td>\n",
       "      <td>376.214325</td>\n",
       "      <td>444.345673</td>\n",
       "      <td>124.652809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-12</th>\n",
       "      <td>46.121532</td>\n",
       "      <td>170.740005</td>\n",
       "      <td>377.706329</td>\n",
       "      <td>445.546967</td>\n",
       "      <td>124.175270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-13</th>\n",
       "      <td>45.468487</td>\n",
       "      <td>170.160004</td>\n",
       "      <td>368.257507</td>\n",
       "      <td>439.407898</td>\n",
       "      <td>125.278023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-14</th>\n",
       "      <td>45.450348</td>\n",
       "      <td>169.669998</td>\n",
       "      <td>370.549042</td>\n",
       "      <td>439.587677</td>\n",
       "      <td>123.385101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-18</th>\n",
       "      <td>44.643112</td>\n",
       "      <td>169.389999</td>\n",
       "      <td>361.324524</td>\n",
       "      <td>431.802765</td>\n",
       "      <td>121.648514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-19</th>\n",
       "      <td>44.833584</td>\n",
       "      <td>172.080002</td>\n",
       "      <td>357.355835</td>\n",
       "      <td>427.319031</td>\n",
       "      <td>122.490776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-20</th>\n",
       "      <td>45.096619</td>\n",
       "      <td>171.649994</td>\n",
       "      <td>352.714355</td>\n",
       "      <td>422.589478</td>\n",
       "      <td>123.246162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-21</th>\n",
       "      <td>44.443558</td>\n",
       "      <td>171.089996</td>\n",
       "      <td>342.934052</td>\n",
       "      <td>414.293762</td>\n",
       "      <td>124.713577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-24</th>\n",
       "      <td>43.817734</td>\n",
       "      <td>172.029999</td>\n",
       "      <td>344.503937</td>\n",
       "      <td>416.053192</td>\n",
       "      <td>123.688988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-25</th>\n",
       "      <td>43.736092</td>\n",
       "      <td>172.580002</td>\n",
       "      <td>336.517914</td>\n",
       "      <td>410.973633</td>\n",
       "      <td>123.489288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-26</th>\n",
       "      <td>43.182812</td>\n",
       "      <td>169.789993</td>\n",
       "      <td>335.991333</td>\n",
       "      <td>409.942596</td>\n",
       "      <td>121.995804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-27</th>\n",
       "      <td>42.683964</td>\n",
       "      <td>167.600006</td>\n",
       "      <td>332.607666</td>\n",
       "      <td>407.918304</td>\n",
       "      <td>124.236008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-28</th>\n",
       "      <td>42.892570</td>\n",
       "      <td>167.100006</td>\n",
       "      <td>343.041351</td>\n",
       "      <td>418.049103</td>\n",
       "      <td>124.279465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-31</th>\n",
       "      <td>44.298443</td>\n",
       "      <td>168.089996</td>\n",
       "      <td>354.011200</td>\n",
       "      <td>425.578613</td>\n",
       "      <td>123.636833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-01</th>\n",
       "      <td>44.570541</td>\n",
       "      <td>168.229996</td>\n",
       "      <td>356.419739</td>\n",
       "      <td>428.454193</td>\n",
       "      <td>123.226517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-02</th>\n",
       "      <td>44.497982</td>\n",
       "      <td>168.839996</td>\n",
       "      <td>359.315765</td>\n",
       "      <td>432.616211</td>\n",
       "      <td>123.643929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-03</th>\n",
       "      <td>44.026337</td>\n",
       "      <td>168.600006</td>\n",
       "      <td>344.747711</td>\n",
       "      <td>422.447601</td>\n",
       "      <td>122.678680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-04</th>\n",
       "      <td>44.135178</td>\n",
       "      <td>168.860001</td>\n",
       "      <td>349.096741</td>\n",
       "      <td>424.434021</td>\n",
       "      <td>120.870110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-07</th>\n",
       "      <td>44.044479</td>\n",
       "      <td>170.110001</td>\n",
       "      <td>346.288391</td>\n",
       "      <td>423.071930</td>\n",
       "      <td>120.948402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-08</th>\n",
       "      <td>44.470772</td>\n",
       "      <td>170.630005</td>\n",
       "      <td>350.179108</td>\n",
       "      <td>426.552917</td>\n",
       "      <td>120.139725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-09</th>\n",
       "      <td>45.169170</td>\n",
       "      <td>171.210007</td>\n",
       "      <td>357.599579</td>\n",
       "      <td>432.795990</td>\n",
       "      <td>120.365837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-10</th>\n",
       "      <td>44.869862</td>\n",
       "      <td>170.559998</td>\n",
       "      <td>349.506287</td>\n",
       "      <td>425.020538</td>\n",
       "      <td>118.452904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-11</th>\n",
       "      <td>44.189602</td>\n",
       "      <td>173.809998</td>\n",
       "      <td>338.419373</td>\n",
       "      <td>416.639648</td>\n",
       "      <td>120.209328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-14</th>\n",
       "      <td>43.881218</td>\n",
       "      <td>174.740005</td>\n",
       "      <td>338.838623</td>\n",
       "      <td>415.277527</td>\n",
       "      <td>118.713791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-15</th>\n",
       "      <td>44.833584</td>\n",
       "      <td>173.080002</td>\n",
       "      <td>347.263489</td>\n",
       "      <td>421.974609</td>\n",
       "      <td>117.366020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-16</th>\n",
       "      <td>45.160099</td>\n",
       "      <td>174.860001</td>\n",
       "      <td>347.175720</td>\n",
       "      <td>422.447601</td>\n",
       "      <td>118.061646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-17</th>\n",
       "      <td>44.634037</td>\n",
       "      <td>177.250000</td>\n",
       "      <td>336.849396</td>\n",
       "      <td>413.423553</td>\n",
       "      <td>118.939857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-18</th>\n",
       "      <td>44.189602</td>\n",
       "      <td>177.119995</td>\n",
       "      <td>333.007507</td>\n",
       "      <td>410.746613</td>\n",
       "      <td>120.191940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-22</th>\n",
       "      <td>43.563759</td>\n",
       "      <td>177.490005</td>\n",
       "      <td>329.662903</td>\n",
       "      <td>406.338654</td>\n",
       "      <td>120.504974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-23</th>\n",
       "      <td>43.055836</td>\n",
       "      <td>178.289993</td>\n",
       "      <td>321.218445</td>\n",
       "      <td>399.130676</td>\n",
       "      <td>118.844193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-24</th>\n",
       "      <td>42.166969</td>\n",
       "      <td>177.139999</td>\n",
       "      <td>332.012848</td>\n",
       "      <td>405.137299</td>\n",
       "      <td>118.922440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-25</th>\n",
       "      <td>42.946995</td>\n",
       "      <td>176.550003</td>\n",
       "      <td>337.161469</td>\n",
       "      <td>414.076202</td>\n",
       "      <td>119.009438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-28</th>\n",
       "      <td>42.384647</td>\n",
       "      <td>178.380005</td>\n",
       "      <td>338.165833</td>\n",
       "      <td>413.016815</td>\n",
       "      <td>121.617912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-01</th>\n",
       "      <td>41.822296</td>\n",
       "      <td>181.619995</td>\n",
       "      <td>332.988007</td>\n",
       "      <td>406.726440</td>\n",
       "      <td>123.024055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-02</th>\n",
       "      <td>41.894863</td>\n",
       "      <td>179.729996</td>\n",
       "      <td>338.575317</td>\n",
       "      <td>414.208649</td>\n",
       "      <td>118.818756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-03</th>\n",
       "      <td>41.305305</td>\n",
       "      <td>180.800003</td>\n",
       "      <td>333.738861</td>\n",
       "      <td>412.146484</td>\n",
       "      <td>120.028946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-04</th>\n",
       "      <td>40.470852</td>\n",
       "      <td>183.679993</td>\n",
       "      <td>328.902283</td>\n",
       "      <td>408.797974</td>\n",
       "      <td>122.101143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-07</th>\n",
       "      <td>38.956150</td>\n",
       "      <td>186.410004</td>\n",
       "      <td>316.772003</td>\n",
       "      <td>396.746979</td>\n",
       "      <td>121.169502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-08</th>\n",
       "      <td>39.083126</td>\n",
       "      <td>191.509995</td>\n",
       "      <td>315.299652</td>\n",
       "      <td>393.738983</td>\n",
       "      <td>119.950600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-09</th>\n",
       "      <td>40.189678</td>\n",
       "      <td>185.820007</td>\n",
       "      <td>326.649841</td>\n",
       "      <td>404.295441</td>\n",
       "      <td>118.775246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-10</th>\n",
       "      <td>39.436859</td>\n",
       "      <td>186.399994</td>\n",
       "      <td>323.022369</td>\n",
       "      <td>402.469788</td>\n",
       "      <td>117.068726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-11</th>\n",
       "      <td>38.611477</td>\n",
       "      <td>185.089996</td>\n",
       "      <td>316.323486</td>\n",
       "      <td>397.352417</td>\n",
       "      <td>117.460510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-14</th>\n",
       "      <td>37.677258</td>\n",
       "      <td>182.300003</td>\n",
       "      <td>310.248627</td>\n",
       "      <td>394.448425</td>\n",
       "      <td>114.717903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-15</th>\n",
       "      <td>37.731678</td>\n",
       "      <td>178.889999</td>\n",
       "      <td>319.980042</td>\n",
       "      <td>403.122467</td>\n",
       "      <td>114.517715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker            EEM         GLD         QQQ         SPY         TLT\n",
       "date                                                                 \n",
       "2022-01-03  44.624966  168.330002  391.679474  451.875122  125.295341\n",
       "2022-01-04  44.470772  169.570007  386.599182  451.723816  124.774353\n",
       "2022-01-05  43.745163  169.059998  374.722382  443.049744  124.097122\n",
       "2022-01-06  43.944710  166.990005  374.459106  442.633575  124.418381\n",
       "2022-01-07  44.343796  167.750000  370.402679  440.883575  123.524017\n",
       "2022-01-10  44.343796  168.259995  370.646484  440.334991  123.827965\n",
       "2022-01-11  45.368721  170.289993  376.214325  444.345673  124.652809\n",
       "2022-01-12  46.121532  170.740005  377.706329  445.546967  124.175270\n",
       "2022-01-13  45.468487  170.160004  368.257507  439.407898  125.278023\n",
       "2022-01-14  45.450348  169.669998  370.549042  439.587677  123.385101\n",
       "2022-01-18  44.643112  169.389999  361.324524  431.802765  121.648514\n",
       "2022-01-19  44.833584  172.080002  357.355835  427.319031  122.490776\n",
       "2022-01-20  45.096619  171.649994  352.714355  422.589478  123.246162\n",
       "2022-01-21  44.443558  171.089996  342.934052  414.293762  124.713577\n",
       "2022-01-24  43.817734  172.029999  344.503937  416.053192  123.688988\n",
       "2022-01-25  43.736092  172.580002  336.517914  410.973633  123.489288\n",
       "2022-01-26  43.182812  169.789993  335.991333  409.942596  121.995804\n",
       "2022-01-27  42.683964  167.600006  332.607666  407.918304  124.236008\n",
       "2022-01-28  42.892570  167.100006  343.041351  418.049103  124.279465\n",
       "2022-01-31  44.298443  168.089996  354.011200  425.578613  123.636833\n",
       "2022-02-01  44.570541  168.229996  356.419739  428.454193  123.226517\n",
       "2022-02-02  44.497982  168.839996  359.315765  432.616211  123.643929\n",
       "2022-02-03  44.026337  168.600006  344.747711  422.447601  122.678680\n",
       "2022-02-04  44.135178  168.860001  349.096741  424.434021  120.870110\n",
       "2022-02-07  44.044479  170.110001  346.288391  423.071930  120.948402\n",
       "2022-02-08  44.470772  170.630005  350.179108  426.552917  120.139725\n",
       "2022-02-09  45.169170  171.210007  357.599579  432.795990  120.365837\n",
       "2022-02-10  44.869862  170.559998  349.506287  425.020538  118.452904\n",
       "2022-02-11  44.189602  173.809998  338.419373  416.639648  120.209328\n",
       "2022-02-14  43.881218  174.740005  338.838623  415.277527  118.713791\n",
       "2022-02-15  44.833584  173.080002  347.263489  421.974609  117.366020\n",
       "2022-02-16  45.160099  174.860001  347.175720  422.447601  118.061646\n",
       "2022-02-17  44.634037  177.250000  336.849396  413.423553  118.939857\n",
       "2022-02-18  44.189602  177.119995  333.007507  410.746613  120.191940\n",
       "2022-02-22  43.563759  177.490005  329.662903  406.338654  120.504974\n",
       "2022-02-23  43.055836  178.289993  321.218445  399.130676  118.844193\n",
       "2022-02-24  42.166969  177.139999  332.012848  405.137299  118.922440\n",
       "2022-02-25  42.946995  176.550003  337.161469  414.076202  119.009438\n",
       "2022-02-28  42.384647  178.380005  338.165833  413.016815  121.617912\n",
       "2022-03-01  41.822296  181.619995  332.988007  406.726440  123.024055\n",
       "2022-03-02  41.894863  179.729996  338.575317  414.208649  118.818756\n",
       "2022-03-03  41.305305  180.800003  333.738861  412.146484  120.028946\n",
       "2022-03-04  40.470852  183.679993  328.902283  408.797974  122.101143\n",
       "2022-03-07  38.956150  186.410004  316.772003  396.746979  121.169502\n",
       "2022-03-08  39.083126  191.509995  315.299652  393.738983  119.950600\n",
       "2022-03-09  40.189678  185.820007  326.649841  404.295441  118.775246\n",
       "2022-03-10  39.436859  186.399994  323.022369  402.469788  117.068726\n",
       "2022-03-11  38.611477  185.089996  316.323486  397.352417  117.460510\n",
       "2022-03-14  37.677258  182.300003  310.248627  394.448425  114.717903\n",
       "2022-03-15  37.731678  178.889999  319.980042  403.122467  114.517715"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line displays the wide pivot table for the first 50 dates.\n",
    "close_wide_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
